<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Conversations</title>
        <link>https://alexey.radul.name</link>
        <description><![CDATA[Recent posts from Conversations]]></description>
        <atom:link href="https://alexey.radul.name/feed.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Sat, 30 Mar 2024 00:00:00 UT</lastBuildDate>
        <item>
    <title>On the Town Budget</title>
    <link>https://alexey.radul.name/ideas/2024/on-the-town-budget/index.html</link>
    <description><![CDATA[<p>This Tuesday Belmont, MA, the town where I live, makes a contentious
decision: To vote for or against an increase in property taxes. It’s
a contested election: I’ve received three flyers in favor and four
against, to say nothing of comments online and by folks I meet around
the town. I want to work out a good way to think about this kind of
choice.</p>
<h2 id="the-mechanics">The Mechanics</h2>
<p>Belmont is in Massachusetts, and thus, due to previous state-level
history, is subject to <a href="https://www.mass.gov/info-details/proposition-2-12-and-tax-rate-process">Proposition 2 and a
half</a>,
which limits the rate at which the town government can increase
property taxes year over year without the consent of the voters.
The limit is 2.5%, <em>not adjusted for inflation</em>.
Belmont’s expenses have been growing faster than this limit for many
years, increasingly constraining the budget, so some change is
necessary. The specific point at issue is whether to override the
limit, and permit a one-time increase in the tax rate, amounting to
$8.4 million in annual revenue for the town. This new rate then
becomes the new base from which Proposition 2 and a half counts future
increases.</p>
<h2 id="the-contention">The Contention</h2>
<p>Reading the messaging of the two campaigns, for the override and
against, they are talking past each other. The “No” campaign stresses
the cost, and implies in its messaging that Belmont’s spending is out
of control, that the town government is incompetent or corrupt, and
that passing the override is just rewarding wasteful spending with
more money.</p>
<p>Conversely, the “Yes” campaign stresses the things the override money will buy,
or the losses its absence is projected to inflict: Laying off
schoolteachers, possibly closing a whole elementary school, cuts to
school extracurriculars, fewer firefighters, pushing the cost of trash
pickup directly onto residents, and loss of access to the regional
library network due to insufficient library funding. Implicit in this
messaging: the town government consists of dedicated civil servants
spending the town’s limited resources as best they can, presenting the
voters with the stark consequences of their action or inaction.</p>
<p>Yet neither campaign seems to be trying to address the essential
disagreement: is the town government a trustworthy steward of public
funds or is it not?</p>
<h2 id="is-belmont-government-a-good-value">Is Belmont Government a Good Value?</h2>
<p>The question of the town’s efficiency isn’t obvious. The
budget for Fiscal Year 2022 (from
<a href="https://www.belmont-ma.gov/sites/g/files/vyhlif12826/f/uploads/final_fy2024_budget_rec._5-17-23_.pdf">here</a>)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>
was $139 million dollars of revenue and $134.5 million dollars of
spending, which are staggering numbers. Do you know what you can
buy for $134.5 million dollars? Me neither.</p>
<p>As of the 2020 US Census, Belmont’s population was 27,295, with an
estimated (by <a href="https://worldpopulationreview.com/us-cities/belmont-ma-population">some random
website</a>)
annual growth rate of 1.53%. That gives us more human numbers: In
2022, Belmont spent ($134.5 million) / (27,295 * 1.0153 * 1.0153) =
$4,780 per person on town services. But what do those lucky enough to
live in Belmont get for that money?</p>
<p>Looking deeper into the budget, we see the biggest subcategory is
$56.4 million = $2004 per person for education, dominated by Belmont’s
public schools. Belmont’s public schools are <a href="https://www.nytimes.com/interactive/2016/04/29/upshot/money-race-and-success-how-your-school-district-compares.html">off the
charts</a>
for educational attainment, serving <a href="https://profiles.doe.mass.edu/profiles/student.aspx?orgcode=00260000&amp;orgtypecode=5&amp;&amp;fycode=2022">4,356 enrolled
students</a>
in 2022, for a cost per student of $12,948. But is that a high budget
or a low? Well, private school tuitions run $20,000 per year or more,
so that’s one datum; and we can compare Belmont to nearby towns:
<a href="https://www.arlingtonma.gov/departments/town-manager/town-manager-s-annual-budget-financial-report/fy2024">Arlington</a>,
<a href="https://www.lexingtonma.gov/1560/2024Budget">Lexington</a>,
<a href="https://portal.laserfiche.com/Portal/DocView.aspx?id=17234&amp;repo=r-5ece5628">Watertown</a>, and
<a href="https://www.city.waltham.ma.us/auditors-department/files/fy-2024-council-approved-budget">Waltham</a>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<table>
<colgroup>
<col style="width: 13%" />
<col style="width: 9%" />
<col style="width: 12%" />
<col style="width: 31%" />
<col style="width: 17%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr>
<th>Town</th>
<th>Pop.</th>
<th>Students</th>
<th>School budget (millions)</th>
<th>Per resident</th>
<th>Per student</th>
</tr>
</thead>
<tbody>
<tr>
<td>Arlington</td>
<td>46,837</td>
<td>5,866</td>
<td>$ 80.1</td>
<td>$1710</td>
<td>$13,654</td>
</tr>
<tr>
<td>Belmont</td>
<td>28,137</td>
<td>4,356</td>
<td>$ 56.4</td>
<td>$2004</td>
<td>$12,948</td>
</tr>
<tr>
<td>Lexington</td>
<td>35,370</td>
<td>6,790</td>
<td>$123.8</td>
<td>$3500</td>
<td>$18,233</td>
</tr>
<tr>
<td>Waltham</td>
<td>64,217</td>
<td>5,496</td>
<td>$ 93.4</td>
<td>$1454</td>
<td>$16,994</td>
</tr>
<tr>
<td>Watertown</td>
<td>35,018</td>
<td>2,571</td>
<td>$ 52.8</td>
<td>$1508</td>
<td>$20,537</td>
</tr>
</tbody>
</table>
<p>Two things jump out at me from this data:</p>
<ul>
<li>First, per student, Belmont’s public schools are very efficient.
Belmont spends the least of the peer towns per student, and yet
achieves superb results, second only to Lexington, which has the
highest-achieving public schools in the entire <em>country</em>.</li>
<li>But second, per resident, Belmont’s public schools are actually the
second most expensive, again behind only Lexington.</li>
</ul>
<p>So on one hand, public school funding in Belmont is a good
value—relatively little money produces good outcomes. On the other
hand, Belmont is very enrollment-heavy—far more public school
students per resident than nearby towns (again, except Lexington). So
it seems like people with school-age kids are moving to Belmont (and
Lexington), and I could see how that would frustrate long-time
residents who are facing rising property taxes even though their own
kids have graduated long ago.</p>
<p>What about other town services? Unfortunately, it’s harder to do a
clear comparison, because the different towns break their budgets down
differently. That said, my gestalt sense from looking over
neighboring towns’ budgets is that Belmont is not an outlier—I don’t
see a clear case that Belmont government wastes money, at least not
compared to how things are in Belmont’s neighbor towns. And if we add
the supposition that the school district is broadly similar in
efficiency to the rest of the town government, then I think it’s safe
to conclude that yes, Belmont town services are a good value.</p>
<p>At least, I haven’t seen any evidence to the contrary presented by the
“No” campaign.</p>
<h2 id="should-we-buy">Should we Buy?</h2>
<p>Even if something is a good value, that doesn’t mean everyone has to
buy it. I feel like the substance of the “Yes” campaign’s messaging
is that, indeed, we should. I actually think that this question is
not very amenable to a general analysis, but rather, this is what
elections and referenda are for: ask everyone who lives in Belmont
whether they prefer to live in a town that funds its schools and
public services adequately or tries very hard to reduce costs, and
whether they are willing to pay for that funding through property
taxes, or would prefer to keep the money for themselves.</p>
<p>The exact extent and severity of service cuts is a point where I think
the “Yes” campaign is being a bit alarmist. They discuss a particular
plan brought forward by the town’s select committee; but that is just
a plan, that will play out over multiple years, and things change.
The town government will do its best with the resources they have;
Belmont may be able to obtain additional state aid; Belmont voters may
pass a different override before the most severe cuts materialize;
Belmont may be rescued by a magnanimous benefactor.</p>
<p>Nonetheless, barring a stroke of good fortune, some cuts somewhere
will be real. And, as far as I can tell, Belmont is spending its tax
money effectively—it doesn’t seem like residents could buy the same
things for themselves by keeping the funds away from the town
government. So the choice really is between more services or less.
And, of course, a town with good town services is a town where people
want to live, which attracts buyers and raises the home values of the
very people who are considering an increase in their property taxes.</p>
<p>Which brings me to the next point that has shown in up in this debate
in Belmont.</p>
<h2 id="can-we-pay">Can we Pay?</h2>
<p>One theme the “No” campaign has harped on is the bind that Belmont’s
fiscal situation places on seniors and residents on a fixed income.
The prototypical exemplar has lived in Belmont their whole life,
doesn’t have a large pool of savings outside the value of their home,
and is being pushed out by a double whammy of a rising property tax
rate, joined with a rising assessed value for their home, which leads
to a property tax bill that skyrockets year after year. On top of
financial strain due to our recent Covid-induced bout of general
inflation.</p>
<p>The cruel capitalist says “Your house is too expensive for you to
afford? Sell, move somewhere cheaper, and pocket the difference!”
But that’s not really a nice message to send to people who have chosen
to retire in Belmont, where they presumably have long-term
connections, a home, a sense of community. Facing a choice like that,
such a person would reasonably be angry, and would reasonably want to
vote against any further town taxes that are driving them to this
bind. And, no longer having to work, they have the time to form
political opinions, and to express them by voting.</p>
<p>But wait a minute. How are we in a situation where people want to
vote against increasing the value of an asset that they own? What?
So this objection isn’t really about having enough wealth at all, it’s
about liquidity. Our prototypical senior (or senoir couple) is facing
$10,000 per year in property taxes on a $1 million house they own
outright. They’re not <em>poor</em>, precisely, but if they don’t have the money to
pay the tax bill, they could be in real trouble, because the bulk of
their wealth is tied up in that house. And getting that wealth out
means either selling the house, and being forced through a disruptive
move whose result is to leave their friends and community, or going
through the hassle and humiliation of taking out a home equity loan or
a second mortgage. Just to pay property taxes on a house that’s
<em>already theirs</em>.</p>
<p>This is where a little socio-financial engineering might come to the
rescue. Imagine yourself in this position, and imagine that you’ve
read and believed what I’ve said so far. So you’d really like your
public schools to have a band (and maybe you have grandkids or
grand-nieces or -nephews that might even play in it). And you’d
definitely like the town’s fire department to be well staffed and well
equipped. But you just can’t afford your rising taxes. Even though
you know, somewhere in the back of your mind, that paying for these
things now raises the value of your house, and it’s money that will
either come back to you if you do decide to sell during your lifetime,
or at least will come back to your children when they sell after
you’ve passed on.</p>
<p>But what if this just wasn’t a problem? What if paying your property
taxes now were just optional? You get a tax bill, it says “Here’s
what the town government has assessed for this year,” and there’s a
check-box there labeled “Defer”. Simple as that—you just decide,
every year, how much you’d like to (or can afford to) chip in to the
town budget now, and the town gives you an automatic, at-cost loan for
the rest. Maybe something like 2–3% annual interest, whatever it costs the
town to finance the unpaid taxes. No fuss, no hassle, no stigma, no
late fees, no collections, this is just the way it’s done, everybody
eligible defers their property taxes if they feel like it.</p>
<p>Too good to be true? Well, the town will collect eventually—when
the house changes ownership. What if, every time a house is sold in
Belmont, part of the seller’s proceeds goes to pay off all previously
deferred property taxes. All the real-estate agents know about it,
it’s not something unusual or bothersome for the buyers, it’s just the
way things are done here. House sells for $960,000, real-estate agent
takes their fee, bank takes their mortgage close-out if there is a
mortgage, title transfer, attorneys’ fees, and the town takes their
deferred property taxes. No big deal.</p>
<p>And if the house isn’t sold but inherited, and the heirs decide to
keep it, they have some polite interval in which they have to settle
the deferred taxes. Maybe they just come due at the next tax date, or
maybe there’s some pay-down plan that spreads the liability over
several years, but in any case, the town gets its due without having
to wait another generation. Again, no fuss, no stigma, no approvals
processes, no humiliating means-testing—if the town has assessed the
land+building as worth $800,000, then surely they can float a loan of
$10,000 secured by that building without checking anything else.</p>
<p>If that was how things were done in Belmont, would the override vote
be quite so contentious, I wonder? Would anyone be quite so upset
about $704 dollars per year of additional taxes on a $1 million house,
if they could just defer the excess, and have their heirs pay it off
from the increased value of that same house when eventually settling
the estate?</p>
<h2 id="details">Details</h2>
<p>Obviously a property tax deferral scheme like this has a bunch of
details that would need to be worked out. First, who’s eligible?
Belmont has to collect their property taxes sooner or later, so we
can’t have them float intergenerationally forever. Perhaps the legal
age of eligibility for Social Security benefits makes a good cutoff?
If you’re under 67, you’re on the hook for your full property taxes
every year, sorry. But if you’re over, it’s defer at will.</p>
<p>Second, how much of the operating budget can Belmont afford to make
optional like this? The obvious way to handle it is to get a line of
credit from a bank: the town passes this tax regime as a bylaw, and
then, every year, the town just borrows the amount that people defer,
and pays down that debt as catch-up payments come in. Which they
inevitably must, since the real estate itself isn’t going anywhere.
Ideally, Belmont’s total debt on the credit line at any moment would
exactly equal Belmont’s total deferred property taxes owed.</p>
<p>Part of what will be needed for this scheme to work is for the town to
get that credit line on good terms, and then not try to make a profit
by charging deferring taxpayers excess interest (maybe just a little
more to cover the cost of administering the program, but that’s it).
The point is that this has to not feel usurious or exploitative to the
people eligible to take advantage of it—just the town passing on
their own costs, as transparent and fair as can be. How much of
Belmont’s total revenue might people defer? 10%? Can Belmont ink a
bank deal where the town gets to just decide to borrow around $10
million any given year, depending on the whims of its taxpayers? What
kind of interest will the bank need to charge Belmont for this? It’s
important for Belmont to know the interest rate in advance, so the
town can be transparent with taxpayers about how much they, in turn,
will have to incur in interest if they do choose to defer their taxes.</p>
<p>Third, the total deferred tax liability for a house does need to stay
well under that house’s available equity, so that Belmont (or, heaven
forfend, Belmont’s bank) is not put into the position of having to
seize the house. Partly that can be achieved by limiting the amount
of tax that can be deferred, if needed. In an extreme case, consider
someone living to 107, having deferred all property taxes starting
at 67. Belmont’s current tax rate is 1.057%. Assuming it stays that
way, and the house doesn’t change value, and Belmont charges 3%
interest per year, the deferred taxes will add up to, um, <em>(goes off and
writes a short program to compute the answer)</em> 79.7% of the value of that
person’s house by the time they pass away at age 107.</p>
<p>80% is a lot, but it’s not completely out of the question even as it
stands. If the house loses 30%-ish of its value near the end of this
hypothetical tax-payer’s life, then they could actually be underwater,
which wouldn’t be great. But we can fiddle with the rules for this
system to prevent this kind of scenario from being a problem. One
candidate hack would be to just limit the amount of property taxes
that can be deferred. Remember, the goal is to make seniors feel like
they can afford to let the town make long-term fiscal decisions, so
how about requiring them to pay only as much property tax as was due
the year they turned 67, and letting them defer the rest? Then your
property taxes due right away don’t suddenly drop to zero when you
turn 67, but just stay at the level they were before you became
eligible for the deferment program. That’s presumably not too hard to
plan for. And then Belmont doesn’t have to finance anywhere near so
large a volume of deferrals, especially not right away; and the risk
of a house going underwater due to long-standing deferrals is much
less.</p>
<p>The one other thing about not going underwater is to make sure that
deferred taxes due don’t overlap unfavorably with other loans secured
by the house. Part of the point of this exercise is that tax
deferrals have the force of law, and are therefore senior to other
leins that may be present on the property. For new loans that’s not
too big of a problem; Belmont just needs to advertise this program to
banks that do business with Belmont residents, and make sure that said
banks are aware of any existing tax deferrals and can adjust their
loan-to-value calculations accordingly. Future deferrals are a little
trickier: what should happen if someone takes out a second mortgage
when they’re 63, and then starts deferring their property taxes on the
same house when they turn 67? Could the total of mortgage and
deferred taxes put them underwater? Will banks refuse to lend to
Belmont residents, fearing the potential seniority of the town’s potential
future claim? I’m not enough of a financier to invent a solution for
this problem on the spot, but surely one exists. Maybe Belmont can
maintain a database of residents’ outstanding mortgages and only offer
property tax deferments to people with enough home equity that their
obligations to Belmont will not end up conflicting with their
obligations to their lender. I don’t know.</p>
<p>In any case, I hope Belmont’s leaders will consider an automatic
property tax deferral scheme like this, because I think it addresses a
real liquidity need for Belmont’s seniors. And perhaps it may help
those same seniors take a longer view toward investing in the town,
knowing that they won’t be on the hook for those costs until the
investment has had time to bear fruit.</p>
<h2 id="conclusion">Conclusion</h2>
<p>So, where are we? It seems that a decision like the Belmont override boils
down to three questions:</p>
<ul>
<li>Are Belmont property taxes a good value for what they buy?</li>
<li>Do we want more of that?</li>
<li>Can we pay for it?</li>
</ul>
<p>When it comes to Belmont schools, the answer to the first of these
questions is a clear “yes”; and given that record, and my brief
inspection of budgets for Belmont and neighboring towns, I am willing to
argue that the rest of the town budget should get the benefit of the
doubt until clear evidence is adduced to the contrary.</p>
<p>The second question I think is essentially personal, but based on the
answer to the first, the cuts put forward as consequences of failing
to pass the override should be taken seriously. Of course, the
particular plan discussed by the “Yes” campaign is just a plan, and
things change, and the town government will do its best with the
resources they have; but I think the basic tradeoff of taxes for
services is real.</p>
<p>And to the third question, I think the answer is also “yes”. But, to
make that more clearly true for everyone, Belmont may want to
implement a deferred taxation plan, so that residents can put their
home equity into investing in the town’s future instead of having to
come up with cash they may not have.</p>
<h2 id="notes">Notes</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Why 2022? Because that’s the latest year for which
actual revenues and expenditures have been compiled, as of the FY2024
budget.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>I’m not including Cambridge because, while a neighbor,
it’s pretty different in character from Belmont, and I didn’t figure
out how to get a comparable number from their budget documents.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>true</pubDate>
    <guid>https://alexey.radul.name/ideas/2024/on-the-town-budget/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Do Carbon Offsets Really Work?</title>
    <link>https://alexey.radul.name/ideas/2021/do-carbon-offsets-really-work/index.html</link>
    <description><![CDATA[<p>A friend recently pointed me to <a href="https://www.cesifo.org/en/publikationen/2021/working-paper/do-carbon-offsets-offset-carbon">an
article</a>
arguing that the Clean Development Mechanism may be counter-productive
if it mis-estimates how additional its supported projects are, and
ends up allowing more emissions elsewhere than it actually reduces.
Does this mean offsets are useless? As an individual, does buying and
retiring offsets still hold water as a climate charity? Let’s work
through a contrived example and see what happens.</p>
<h2 id="context">Context</h2>
<p>For context, the CDM is the world’s largest carbon-offset market. If
I understand correctly, the idea is this: There are relatively
easy-to-regulate carbon emitters, like fossil-fuel power plants. They
can be subjected to a cap-and-trade scheme, where the regulatory
authority demands that each one emit no more mega-tons of
CO2-equivalent than its allowance. To take advantage of markets to
maximize efficiency, the regulatory authority allocates a single total
allowance (cap) for the whole sector, and auctions the individual
allowances under the cap to whoever is willing to pay for them. Thus,
power plants that can reduce emissions the most cheaply can do that,
and those that can’t buy more of the allowances, achieving the
required emissions limit at the lowest cost. As an individual, you
can further push the world to reduce carbon emissions by buying some
of the allowances and retiring them without emitting anything,
effectively paying to lower the cap.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>So far so good. Where do offsets and the CDM come in? Here the idea
is that reducing emissions from even the easiest-to-clean power plants
may not be the most effective use of money. Other reduction projects
are harder to measure and regulate, though, so the regulatory
authority can’t do it reliably. Enter the CDM. They are supposed to
do a bunch of due diligence evaluating various emissions reduction
projects according to their criteria, notably including
“additionality”. If a project meets the criteria, the CDM funds the
project, and creates an offset, which is a legal certificate saying
“because we paid them, this project reduced emissions by X mega-tons
that otherwise wouldn’t have happened, and it’s all for real.” Then
the regulatory authority in our cap-and-trade power market accepts
this certificate as equivalent to a tradable X-mega-ton emissions
allowance. If the CDM’s projects are cheap enough, the regulated
emitters will buy these certificates, thus funding the CDM’s projects
instead of performing their own (presumably more expensive) emissions
mitigations, ultimately achieving the required emissions reductions
even more cost-effectively.</p>
<h2 id="problem">Problem</h2>
<p>Sounds great! But, as Calel et. al. point out, what if the CDM is
wrong, and its projects aren’t actually as additional as it thought?
Then it’s handing out money to projects that would have happened
anyway, and letting regulated emitters off the hook by selling them
offsets that don’t actually correspond to real reductions. How bad is
this?</p>
<p>Let’s work through some contrived numbers. Suppose we are working
with a regulated power market that, in the absence of the CDM, would
clear at $26/T for 30 GT of emissions allowances. Suppose the CDM
funds $10B worth of emissions reductions projects that the CDM thinks
reduce emissions by 1 GT. Following its rules, the CDM proceeds to
create 1 GT worth of offset certificates and (I assume) offer them
on the regulated market at $10/T.</p>
<p>The referenced paper is a study of wind farms in India, comparing
those the CDM funded with similar ones it did not. It comes to the
conclusion that at least 52% of the farms would have been built
anyway, so let’s go with that: we assume in our example that the CDM
is wrong by 2x, and in fact its $10B only produced 0.5 GT of really
additional emissions reductions. What happens now?</p>
<h3 id="scenario-a">Scenario A</h3>
<p>As a baseline, what if the CDM just doesn’t issue those
offsets in the first place? Then the regulated market is irrelevant,
and the CDM is charity that thinks it’s getting reductions at a rate
of $10/T, but it’s actually paying $20/T. Given the severity of the
climate disaster, that’s probably still worth it.</p>
<h3 id="scenario-b">Scenario B</h3>
<p>CDM makes its offsets and offers them on the market.
Let’s assume that the increased supply means the market now closes at
$25/T for 31 GT of allowances instead of $26/T for 30 GT. Then the
CDM:</p>
<ol type="1">
<li>made a profit of $15B;</li>
<li>increased total emissions by 0.5 GT;</li>
<li>reduced in-market costs faced by emitters by (26 * 30 - 25 * 31) = $5B;</li>
<li>reduced out-of-market compliance costs for
emitters by some amount between $25B and $26B (because at those market
clearing prices, that must be what that 1 GT of reductions would have
cost to implement by the regulated emitters); and</li>
<li>reduced the revenue of the regulatory authority by $30B.</li>
</ol>
<p>So in this scenario, the CDM is itself a polluter with revenue $50/T
and profit $30/T; which cheated the regulatory authority by increasing
the effective cap in the sector by 0.5 GT. The authority can fix it
post-hoc by lowering the cap by 0.5 GT, but the CDM’s existence is
still post-hoc “justified” if we buy the logic of cap-and-trade
markets, as it’s generating “more value” from the limited pool of
allowed emissions than the marginal other market participant.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>Even in this setting, buying offsets remains a reasonable climate
charity. The existence of the offset mechanism has silently increased
the real emissions cap, but as long as that cap remains binding,
buying allowances still forces marginal reductions (in this case at a
cost of $25/T).</p>
<h3 id="scenario-c">Scenario C</h3>
<p>CDM catches itself, and creates 0.5 GT of offsets instead
of 1 GT, which it offers at $20/T instead of $10/T.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Supply in the
emissions market again increases, but not as much as in Scenario B, so
maybe now the market clears at $25.5/T. Then the CDM:</p>
<ol type="1">
<li>made a profit of $2.75B;</li>
<li>was emissions-neutral;</li>
<li>reduced in-market costs faced by emitters by (26 * 30 - 25.5 * 30.5) = $2.25B;</li>
<li>reduced out-of-market compliance costs for emitters by some amount
between $12.75B and $13B; and</li>
<li>reduced the revenue of the regulatory authority by $15B.</li>
</ol>
<p>So now the CDM is doing what it was designed to do: achieving the
emissions targets set by the regulatory authority at lower overall cost
(and therefore probably higher political feasibility).</p>
<h2 id="conclusion">Conclusion</h2>
<p>Of course, all the above has lots of assumptions and moving parts, but
a few themes seem durable to me (assuming that one trusts
regulated cap-and-trade markets to work as advertised, and to be big
enough to absorb the offsets):</p>
<ol type="1">
<li><p>Carbon offset markets can indeed backfire, in the sense of
accidentally increasing effective emissions caps (but not by more
than the volume of the new offsets).</p></li>
<li><p>If the realized additionality of funded projects can be measured
after the fact, the backfire can be fixed by lowering caps to
compensate.</p></li>
<li><p>Charitable offset purchases still work, but they only work as
advertised because they reduce the effective cap in the regulated
market into which those offsets would otherwise be sold.</p></li>
<li><p>If the offsets are not tied to a trustworthy regulated market,
we’re in Scenario A. A buyer can still obtain 1 T of reductions
by buying offsets, but it becomes necessary to pay $20 for 2 T
worth of offset certificates to do so.</p></li>
</ol>
<p>I like point 2 above. Everything is easier to measure after the fact
than before, including how additional an emissions reduction project
really was. I wonder whether some energetic economist can work out a
good policy for cap-and-trade market authorities to follow when
accepting offsets, along the lines of “We’ll honor your offset
certificates today, but we’ll measure how good they were later and
compensate with automatic additional cap reductions.” Maybe with some
mechanism for pushing money around too, so that not-really-additional
offset projects don’t become too good of a swindle.</p>
<h2 id="notes">Notes</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Assuming, of course, that you trust the caps to
be set wisely and exogenously, based on scientific climate mitigation
goals rather than on observed prices for the allowances. In the
latter case, burning allowances just causes the authority to slow down
the pace at which the cap is lowered, defeating the purpose. That
problem is out of scope for this essay, though.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>What about the money? Who is really paying for
what here? Well, that depends on what our modeled entities do with
their money. One (charitable!) assumption is that the regulatory
authority and the CDM, being public institutions, return all their
profits to the public as a dividend or as reduced taxes; and the
emitters are forced by competition to pass their savings along to the
public as well, in the form of reduced electricity prices. Then the
public gains $15-16B in aggregate, which came from giving $25-26B less
in work to the power-plant-emissions-reduction industry and instead
giving $10B more to the CDM-funded projects. It also cost 0.5 GT more
emissions, coming out at $30-32/T. The gain of $15-16B is
uneven, though: $25-26B in savings are distributed in proportion to
consumption of electricity (including indirectly through the cost of
energy-intensive but competitively priced goods), while $15B of costs
are imposed in proportion to payment of taxes.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Why do I bother with the CDM’s offer price, if the
market clears above it anyway? Because the gap between offer and
clearing is the CDM’s signal to fund more or fewer projects. If we
assume the CDM has the organizational capacity to find and fund enough
projects to equalize its cost to the clearing price in the regulated
market, then pricing offsets higher to account for diligence errors
will reduce the number of projects the CDM can fund. This is
presumably good, because it lowers the magnitude of the mistake the
CDM is making under these assumptions.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>true</pubDate>
    <guid>https://alexey.radul.name/ideas/2021/do-carbon-offsets-really-work/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>On Organizing</title>
    <link>https://alexey.radul.name/ideas/2020/on-organizing/index.html</link>
    <description><![CDATA[<p>Over the years, I have developed a fairly strong theory of how to
organize, and what the end-goal of organizing is. Since people
occasionally ask me, I figured I would write it down. Of course,
articulating also helps me refine my own understanding.
Without further ado,</p>
<ol type="1">
<li>Organizing a space has two goals:</li>
</ol>
<ul>
<li>To make it easy to pick up and put everything away.</li>
<li>To make it easy to get whatever is needed.</li>
</ul>
<p>Note that just picking up and putting things somewhere is not
organizing: organizing is slower and more painful than picking up, but
we do it because it is a long-term investment in making picking up
easier. And because picking up without organizing makes finding
things later more difficult.</p>
<ol start="2" type="1">
<li>Organizing is about information first:</li>
</ol>
<ul>
<li>Making whoever picks up know where to put things away.</li>
<li>Making whoever needs a thing know where it is.</li>
<li>Making whoever may need a thing know which things are available.</li>
</ul>
<ol start="3" type="1">
<li>Therefore, <em>clarity</em> is paramount.</li>
</ol>
<ul>
<li>A place is only a “place where something goes” if the boundary
separating that place from other places is unmistakable. As you
already know, that’s what containers are for: piles separated by air require
far more space to make unmistakably distinct than boxes separated by
walls.</li>
<li>A thing only goes in a place if it can’t go anywhere else.
Definitions of what goes in a place must be precise, unambiguous,
and exclusive. Some examples from my own household organization:
“Interior computer components”; “Data
(not power) cables”; “Hooks, except drapery hooks and command
hooks”; “Drapery hooks”.</li>
<li>Such precise definitions cannot be remembered, so that’s what labels
are for.</li>
<li>When you hold a thing in your hand, reading the label should
unambiguously tell you whether that thing should be put away in that
place.</li>
<li>When you hold a thing in your mind, reading the label should
unambiguously tell you whether that thing is to be found in that
place.</li>
</ul>
<p>Corollary: vague adjectives like “small” are to be avoided on labels,
unless the distribution of objects of that type in the household is
very strongly bimodal, so that “small” is actually unambigous in
context.</p>
<ol start="4" type="1">
<li>Organizing is about <em>flow</em> second:</li>
</ol>
<ul>
<li>The place where a commonly picked-up thing goes should be easy to
reach and close to where that thing tends to get dropped.</li>
<li>The place where a commonly used thing is stored should be easy to
reach and close to where that thing tends to be needed.</li>
<li>Things that are commonly picked up together or commonly used
together should be stored together.</li>
</ul>
<p>This is why categories and containers should be the right size and
shape. Fishing for a golf ball at the bottom of a 64-quart tub of
“sports equipment” does not qualify as “easy to reach”. Conversely,
trying to puzzle-pack a toy airplane into an overflowing bin isn’t
“easy” either. Containers will have some extra space in them; that’s
ok. The space you get back by being able to put something on top of
or beside a container whose lid closes is worth more than the space
you lose by making that container big enough to always close its lid.</p>
<p>This, in turn, is why organizing is so personal. The right categories
and the right container sizes depend on what you have; and the right
places for the containers depend on how and how much you use what you
have.</p>
<p>This is also why organizing is so slow. It requires clarity about
every single object: what is it, what is it for, and how, when, and
where is it used. And when a space is shared, that clarity needs to
be shared between whoever is doing the organizing and everyone who
uses the space and the objects. And, of course, people’s needs and
habits change over time, and with them change their uses of objects.
Marie Kondo’s insight is that choosing not to own an object is often
less long-term work than continually understanding, supporting, and
communicating that object’s evolving purpose.</p>
<p>The rest is technique. A label printer;<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> keeping things
vertical; stackable containers; shelves vs drawers vs hanging rods vs
pegboards; neverending streams of organization products; tradeoffs
between storage space and aesthetics. Those are matters of practice
and style that will fall into place behind the dual needs of clarity
and flow.</p>
<h2 id="notes">Notes</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Best $100 I ever spent, to this day.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>true</pubDate>
    <guid>https://alexey.radul.name/ideas/2020/on-organizing/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Yak Shaving</title>
    <link>https://alexey.radul.name/ideas/2020/yak-shaving/index.html</link>
    <description><![CDATA[<p>Every once in a while someone hears me use the phrase “yak shaving” to
describe some activity and asks what I mean. Here is a superb example
from my recent life:</p>
<ol type="1">
<li>I want to lower the interest rate on my mortgage.</li>
<li>So I start an application to refinance.</li>
<li>During the application, I discover that my credit score is lower
than I expect.</li>
<li>So I pull my credit reports, and find an error.</li>
<li>In order to dispute the error, I must supply supporting documents,
including statements from the relevant credit card.</li>
<li>I go to the web site of the credit card processor to download the
statements.</li>
<li>The problem happened because of a change of credit card processors,
so the new one doesn’t have an online copy of the statement before
the change, which I want to include in my dispute packet.</li>
<li>The old processor doesn’t have the statement either, so I have to
scan the paper copy from my records.</li>
<li>I don’t have a scanner, so I photograph the pages one by one with my
phone.</li>
<li>They come out as separate jpegs, but I want to convert them to pdf
to re-collate them into documents with pages.</li>
<li>But when I run imagemagick to do that, it says “Not authorized”,
which turns out to be due to a security vulnerability in GhostScript
9.24.</li>
<li>The vulnerability has since been patched, but the over-careful
imagemagick configuration remains, so I must edit my imagemagick
config file to authorize converting jpegs to pdf.</li>
<li>The location of this configuration file is tattooed on the hide of a
Bedouin yak, and the only way to discover it is to guess the correct
yak and remove enough of its hair.</li>
<li>So here I am in the desert, razor in hand, diligently shaving yaks,
because I wanted to lower the interest rate on my mortgage.</li>
</ol>
<p>Doing this kind of task for this kind of reason is called “yak shaving”.</p>]]></description>
    <pubDate>true</pubDate>
    <guid>https://alexey.radul.name/ideas/2020/yak-shaving/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Computational Murk</title>
    <link>https://alexey.radul.name/ideas/2019/computational-murk/index.html</link>
    <description><![CDATA[<p>Why is playing chess fun? Why are people irrational? What is
randomness? Why does cryptography work? What do these questions have
to do with each other?</p>
<p>Let’s start with chess. Can you imagine a perfectly rational agent, a
<a href="https://en.wikipedia.org/wiki/Homo_economicus"><em>Homo Economicus</em></a>,
if you will, playing chess? To be perfectly
rational is to know all the consequences of everything one knows.
Knowing the rules of chess, our <em>Homo Economicus</em>—call her
Alyssa—would know who wins (or, more likely in the case of chess,
how to force a draw), would know the array of winning (or drawing)
moves from any given position, and would know that any other <em>Homo
Economicus</em>—Ben—knows the same. Chess, for Alyssa and Ben, would
be a mindless exercise in stepping through one of the perfect games,
where every move is either forced, or an arbitrary choice among a
handful of known options, all leading to the same inevitable,
preordained result. Alyssa would not play chess.</p>
<p>This caricature is, of course, absurd. Our civilization’s best minds,
using our civilization’s best computers, have not managed to play even
one game of chess anywhere near as well as Alyssa. But why, exactly,
not? Of course we know: there are too many possible variations from a
given position. The number of potential consequences of a chess
position grows exponentially in the number of moves ahead one tries to
look, and that exponential quickly blows past all computational abilities.
So what, then, does it mean to make a move in a chess game? It is a
decision. Not just any old decision, but a decision specifically
about how to think: moving is deciding to stop analyzing potential
alternatives. Placing the piece in its new position on the board is
communication: telling one’s opponent what one has decided.</p>
<blockquote class="pullquote-display">
<p>
Between the total clarity of
knowledge and the total opacity of ignorance lies the computational
murk.
</p>
</blockquote>
<p>So what does a game of chess between realistic players look like?
White spends some time thinking about moves and their consequences.
If they’re any good, Black is also spending that time thinking about
the same thing. But, since they’re a different person (or a computer
running with a differently-initialized random-number generator), and
since neither White nor Black has enough computation to think through
everything, they think at least somewhat different things. Then the
move comes, and now Black’s clock is ticking. Unless White is much
stronger than Black, Black can put more computrons into analyzing
White’s move after the decision than White could before—the whole
point of it being a decision was that White was spending at least some
computation on alternatives, and moved exactly when they decided not
to spend any more. That’s why it’s fun: if the players are close to
the same strength, Black can surprise White, by thinking about White’s
move harder (or differently).</p>
<p>So does White know the consequences of their potential moves or not?
The standard model of rationality demands the answer be “yes”, since
the position, the rules, and the players’ goals are all deterministic
common knowledge, with no information hidden from anyone anywhere.
Yet perfectly clear knowledge of the consequences is impossible,
because no one is that smart. Is the answer then just “no”? That
can’t be right either—thinking even a little about possible
responses makes one less in the dark about what the other player will
do than one is about a roulette wheel.</p>
<p>Now I have gotten to the point. Knowing or not knowing is not the
dichotomy it appears to be. Between the total clarity of knowledge
and the total opacity of ignorance lies the computational murk. There
are things one knows, in some operational sense. There are things one
could discover if one reasoned. But reasoning takes time and blood
sugar (or electricity), so one must always be deciding what to reason
about and how much. Others may decide differently, and thus come to
different conclusions from the same premises—conclusions one could
in principle understand, but only in return for more blood sugar. In
some areas this regress seems to have an end, but in chess it’s
obvious that knowledge is always bounded by the murk. Chess is a game
of knowing how (and when) to peer deeper into the mist.</p>
<p>Now that we have it, on what can the idea of computational murk shed,
so to speak, light?</p>
<p>I promised cryptography. There the murk is so deep as to be near
total darkness. The goal, when designing ciphers, is to make sure no
one without the encryption key can decrypt—except by guessing the
key. But the darkness even an ideal cipher generates is not quite
total: testing out a guess takes a finite amount of computation, and there
are only a finite number of possible keys, so no encrypted secret is
safe from a truly computationally unlimited thinker. It’s just
usually unhelpful to model cryptography that way, because the gain
from trying to guess a key is so small as not to be worth the trouble.
Nonetheless, choosing not to waste time guessing private keys is a
decision about how to think: different in parameters but not in kind
from the decision not to think more about a move in chess.</p>
<p>I promised randomness. Pseudo-random number generators are actually
better modeled as ignorance amplifiers. One modern example works like
this: Start with some seed, of, say, 32 bytes. Then encrypt the pair
<code>(seed, 1)</code> with a cipher that emits 64 bytes. Someone who doesn’t
know or guess the seed (and can’t break the cipher) can’t predict
those 64 bytes. If more bytes are needed, encrypt the pair <code>(seed, 2)</code>, then <code>(seed, 3)</code>, and so on. Voilá—little ignorance has become
much ignorance.</p>
<p>But are such pseudo-randomly generated bytes “really random”? That’s
the wrong question. Their future is hidden by computational murk, as
dark as a bitcoin wallet and no darker. The future of dice and decks
of cards and roulette wheels is also only as unpredictable as our
unwillingness to measure well enough and simulate kinematics and
turbulence fast enough. That’s what “randomness” is: the choice to
reason about a thing as though it were unknown (i.e.,
probabilistically) instead of trying to wade through the murk needed
to know it.</p>
<p>That choice is made more uniformly in some circumstances than in
others. The design objective of crypto is to make the murk so dark
that it’s safe to bet billions of dollars that no digital thief can
see through it. In my own professional life, these choices of what to
try and compute through and what to leave in the fog are called
“modeling assumptions”, and reasonable people regularly disagree on
how to make them in any given circumstance. It ends up being a
tradeoff between cost and accuracy. It gets amusing when the accuracy
in question determines another purely computational decision, and thus
can be brought back to the same units of cost.</p>
<p>One particularly interesting example of this kind of circularity lies
at the foundations of the technology called Bayesian optimization. Bayesian optimization is
a family of techniques for looking for the maximum of some function
(for example, the test accuracy of a neural network one is training,
as a function of, say, the number and widths of its layers). The
techniques themselves end up being pretty computationally costly, so
people only tend to use them when the function being optimized is so
expensive to compute that it’s really valuable to squeeze as much as
possible from every trial.</p>
<p>What bothered me for a long time about all this was the set of
modeling assumptions. One begins the math for Bayesian optimization
by saying that one has some unknown function <span class="math inline">\(f\)</span> with some known
input-output pairs <span class="math inline">\(y_i = f(x_i)\)</span>. Then one proceeds to put down some
sort of probabilistic prior on the space of possible functions
(typical choices in this setting just favor smoothness) and to do a
bunch of math to form a posterior probability distribution over <span class="math inline">\(f\)</span>
conditioned the known outputs. Having formed that probability
distribution one then chooses the next point at which to try
evaluating <span class="math inline">\(f\)</span> based on some criterion, such as maximum probability of
exceeding the previous optimum.</p>
<p>But, back up a second here. Why is it legitimate to model <span class="math inline">\(f\)</span> as
unknown, when we can just compute it? Pragmatically, of course, the
answer is that we only use Bayesian optimization when the price of all
this modeling rigamarole is less than the price of evaluating <span class="math inline">\(f\)</span> at
the wrong point. But philosophically, how can we simultaneously treat
<span class="math inline">\(f\)</span> as known enough to compute and unknown enough to model
probabilistically? How can the value of <span class="math inline">\(f\)</span> at a new point <span class="math inline">\(x&#39;\)</span> be
“unknown” and subject to modeling when the computer running <span class="math inline">\(f\)</span> is
right there and we can just ask it? I had much cognitive dissonance
working through the math of deriving perfectly rational conclusions
from such clearly incomplete—intentionally irrational?—modeling
assumptions.</p>
<p>Computational murk restores consonance. We neither “know” nor “don’t
know” <span class="math inline">\(f\)</span>. Rather, it’s murky. The source code means we can discover
<span class="math inline">\(f(x)\)</span> for any <span class="math inline">\(x\)</span>—but that costs time and, in the cloud computing
setting where these methods are common, money. To make the best use
of that time and money, we alternate between learning a new value of
<span class="math inline">\(f\)</span> at a carefully selected <span class="math inline">\(x\)</span>, and pretending we have no access to
<span class="math inline">\(f\)</span> while thinking about the next <span class="math inline">\(x\)</span> to select. My former dissonance
now becomes a clue to a possible research direction: Are there static
analyses that could read the source code of a suitable <span class="math inline">\(f\)</span>, and derive
some clues about its behavior that could then be folded into a more
informative prior for use in Bayesian optimization?</p>
<p>Which brings me to another instance of computational murk, namely
compilers. Especially just-in-time compilers. Much is made in the
compiler world of the distinction between “static” and “dynamic”
information, or “static” and “dynamic” program analyses. As a
practical matter, “static” stuff is available “before the program is
run”, whereas “dynamic” stuff is available only as it runs, and may
vary, for example, from one call of some subroutine to another. But
as a philosophical matter, one asks why it seems so stark. After all,
one compiles programs for computing <span class="math inline">\(\pi\)</span>; and when one does that, all
the information about everything that program will do is, in
principle, “available to the compiler”. But some of it (like the data
types of internal variables) is somehow more “static” than other (like
actual digits of <span class="math inline">\(\pi\)</span> that flow through those variables). The
distinction can also be seen as the murkiness of knowledge: yes, in
principle the compiler could know what numbers will appear in what
variables at the 100,000-th step in the computation; but computing that
is too costly, and the ultimate results are better if one thinks just
about data types first. The compiler’s goal is to look through the
murk partially, and construct a program that will look all the way
faster than the compiler itself would.</p>
<p>But enough about economists’ and computer scientists’ abstractions.
Perhaps by this point you can guess what I will say about what “being
irrational” means when used colloquially about normal people.
Alyssa’s perfect rationality is absurd, but there is nonetheless a
culturally accepted ideal of how (and how much) one is expected to
reason about a given situation in light of given information. To be
“irrational”, then, is to reach a different conclusion than this
idealized “culturally rational reasoner” is expected to reach in the
same circumstances. A necessarily fuzzy concept, because “cultural
rationality” is constructed at best out of the judgements of real
people, through their conversations with each other and in public
about the situation in question; and they disagree often enough. And
so we have many fuzzy words for various fuzzy boundaries around this
fuzzy concept: being confused, following emotional logic,
overthinking, making mistakes (honest or otherwise).</p>
<p>Let me end on what for me is a positive note. Computational murk is
real, so irrationality is inevitable. Individual humans are
irrational. Human organizations are irrational—they may have more
total computation than any one person, but they also have more they
need to think about (such as all their internal working
relationships), so murk limits and bedevils their effective knowledge
as well. Human governments are irrational. Emergent human
institutions (such as “the market”) are irrational. Perforce. There
isn’t enough computation in all the minds and all the computers in all
the world to think through everything that merits thinking through.
So there is room both to question convention and orthodoxy, and,
perhaps, to improve it, by choosing to think about something that the
culture effectively hasn’t thought about before.</p>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      R: "{\\mathbb{R}}",
      eps: "\\varepsilon"
    },
    equationNumbers: { autoNumber: "AMS" },
    noErrors: { disabled: true },
  }
});
</script>]]></description>
    <pubDate>Sat, 09 Mar 2019 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2019/computational-murk/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>The Finitary Finiversalist Church</title>
    <link>https://alexey.radul.name/ideas/2019/finitary-finiversalist-church/index.html</link>
    <description><![CDATA[<p>Infinity. What is it, really? Perhaps you remember, as I do, the
moment in your life when you realized one could (in principle) count
forever, and there would always be integers there for one to count.
Yet, how can our finite minds comprehend the truly infinite? Do
infinte things even exist?</p>
<p>My own thoughts on this subject have gone full circle. I accepted
the infinitude of the integers at a young enough age. Since I recall this
as an act of acceptance, I infer that I must previously have assumed
that only finite things exist. But accept I did, and proceeded,
over the course of my mathematical education, to grow more and more
steeped in the mysteries of the non-finite. One can measure
infinities by bijection; so the even numbers are no more and no less
infinite than all the integers, and than the positive numbers
divisible by 1000. Yet the real numbers are more infinite—they are
too many to be counted, for there is no 1-1 mapping between them and
the counting numbers.</p>
<p>It went on from there: One can measure infinities more finely by
order-preserving bijection. This leads to the
<a href="https://en.wikipedia.org/wiki/Ordinal_number">ordinals</a>, to
<a href="https://en.wikipedia.org/wiki/Transfinite_induction">transfinite induction</a>,
to the <a href="https://en.wikipedia.org/wiki/Continuum_hypothesis">continuum hypothesis</a>.
The <a href="https://en.wikipedia.org/wiki/Axiom_of_choice">axiom of choice</a>
rears its controversial head. I was in my
element, an initiate of eldritch powers delving (or, at least,
learning how to delve) into eternal secrets beyond the (finite)
universe. I accepted the Axiom for the power it gave, and looked down
upon the doubters and detractors as only a teenager can.</p>
<p>But the study of computing sowed the seeds of doubt, and the practice
of probability germinated them, and with the passage of years they
have begun to sprout. Real numbers are awkward things to try to
compute with. It may be a little trite to say that you can’t store
all of the digits of a real number in the memory of the computer, but
the limits on precision really are a consistent source of trouble in
practice.</p>
<p>Probabilities over real numbers are awkward too. It’s easy enough to
say “consider a real number drawn uniformly between 0 and 1”, but you
can’t actually draw one: the probability of any given <span class="math inline">\(x\)</span> under that
distribution is 0 (whether <span class="math inline">\(x\)</span> is between 0 and 1 or not!).
Mathematically, one is forced to resort to the usual song and dance
about probability densities: the limit as <span class="math inline">\(\eps\)</span> goes to zero of the
ratio of the probability in the interval to the size of the interval,
yada, yada. Computationally, finite precision makes this even worse:
the joke in probabilistic computing is that probability zero events
happen alarmingly often. To say nothing of trying represent those
probability densities, and of periodically being bitten by having
forgotten whether one was referencing
<a href="https://en.wikipedia.org/wiki/Lebesgue_measure">Lebesgue</a> or
<a href="https://en.wikipedia.org/wiki/Counting_measure">counting</a>
measure.</p>
<p>So what is it like to sit in the shade of the tree of doubt? The
epigraph from which I started this essay is the foliage between me and
the burning sun of irresolution. If mathematics is to be a study of
mental objects, one cannot simply ban infinities, because infinite
mental objects are too easy to construct (they even form a <a href="https://en.wikipedia.org/wiki/Lazy_evaluation#Working_with_infinite_data_structures">useful
programming
technique</a>).
But they must have <em>reproducible</em> properties: so the place to draw the
line is at objects that take an infinite amount of information to
describe. This is the article of faith that defines finitary
finiversalism: infinity only as the consequence of a finite description.</p>
<p>What, then, is admissible? The integers may be infinite, but they are
probably the most reproducible mental object in all of
mathematics—every kid learns the integers. The rationals are fine
too, as each is given as a ratio of two finite integers—nothing
non-reproducible about that. What of the reals? They are
conventionally defined as the completion (in the order topology) of
the rationals. That is, a real number is the limit of a sequence of
rational numbers. How are we to reproduce such a sequence? We can’t
just write it all down, because the whole point is that the sequences
with irrational limits are infinite. But we can of course specify
some sequences. For instance, “the <span class="math inline">\(k\)</span>th term is the perimeter of a
regular <span class="math inline">\(2^k\)</span>-gon of diameter two.” The limit is a perfectly good
irrational number, with very many very well-reproduced properties.</p>
<p>Which sequences of rationals are describable with a finite amount of
information? Well, certainly any given Turing machine is fine: it can
be encoded as a finite string in a finite alphabet. The only thing
infinite about (some) Turing machines is their behavior; and the
behavior can be reproduced by reproducing the machine.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> So any
sequence of rationals that we can program a (finite) computer to print
is reproducible, at least in principle. And the
<a href="https://en.wikipedia.org/wiki/Church-Turing_thesis">Church-Turing thesis</a>
tells us that’s it.</p>
<p>In fact, the Church-Turing thesis tells us that’s it for all of
mathmatics, not just the reals. Computable objects are the <em>only</em>
objects that can be described with a finite amount of information.
This view reassuringly eliminates various controversies in
mathematical foundations:</p>
<ul>
<li><p>Among sets of computable objects, the <a href="https://en.wikipedia.org/wiki/Well-ordering_theorem">well-ordering
theorem</a>
is obvious, because we can always order objects
lexicographically by (the description of) the smallest Turing
machines that compute them.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p></li>
<li><p>The conventionally controversial axiom of choice follows from
well-ordering, but of necessity it’s now an axiom of
finitely-specified choice.</p></li>
<li><p>The conventional alternative to choice, namely the <a href="https://en.wikipedia.org/wiki/Axiom_of_determinacy">axiom of
determinacy</a>,
is moot. It concerns perfect play in
(certain) infinite games, but since it’s not in general possible to
compute the outcome of such a game even with a given pair of
strategies, dicusssing existence of perfect strategies seems
excessive.</p></li>
</ul>
<p>What of the real numbers? There are only countably many Turing
machines, so the reals can’t really exist in a universe of only
computable objects.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> What we have instead are the
<a href="https://en.wikipedia.org/wiki/Computable_number">computable numbers</a>,
which is the field of limits of
computably-<a href="https://en.wikipedia.org/wiki/Cauchy_sequence">Cauchy</a>
computable sequences of rationals. The famous <a href="https://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument">Cantor diagonalization
proof</a>
that the classical reals are uncountable turns into a proof that there
is no <em>computable</em> bijection between the computable numbers and the
integers.</p>
<p>On the other hand, the unique characterization of the reals still
holds, and the computable numbers are indeed not complete. Not only
is it not possible in general to compute the limit of a computable
sequence of rationals, there are even specific <a href="https://en.wikipedia.org/wiki/Specker_sequence">computable sequences
whose limits are not computable
numbers</a>. (To get the
limit one also needs to compute the Cauchy condition, i.e., bound how
much farther the sequence may move.) Never mind that if one tries to
make computable numbers totally ordered, one finds that equality is
not computable.</p>
<p>So the analysis one gets out of computable numbers is different from
real analysis. Which analysis is better? Computable analysis is more
faithful to reproducible reality: you really can’t reliably compute
the limits of infinite sequences, and must instead ever watch for
accumulating errors. On the other hand, real analysis is simpler:
even formulating the computable analogue of any nontrivial statement
requires thinking about what is to be computed, in what generality,
and given what inputs. Real analysis is also better developed, having
predated attempts at computable analysis by centuries.</p>
<p>And so it is with infinity. It’s a simplification. Only behvaiors
(of, e.g., computing processes) are infinite, but it’s often
convenient to noun verbs and treat summaries of those behaviors as
objects in their own right. But sometimes one needs information that
was summarized away, and the predictive power of the summary fails.</p>
<p>And that’s the finitary finiversalist catechism:</p>
<ul>
<li><p><em>What exists?</em> Only what can be communicated about.</p></li>
<li><p><em>What can be communicated?</em> Only finite descriptions.</p></li>
<li><p><em>What is infinite?</em> Only behaviors of finite machines.</p></li>
<li><p><em>What are infinities?</em> Only summaries of infinite behavior.</p></li>
<li><p><em>Do infinite or infinitesimal things exist?</em> Only while they
summarize behavior faithfully.</p></li>
</ul>
<h2 id="notes">Notes</h2>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      R: "{\\mathbb{R}}",
      Pr: "{\\mathrm{Pr}}",
      E: "\\mathbb{E}",
      eps: "\\varepsilon"
    }
  }
});
</script>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>And its input, and the model of computation in which it’s
interpreted, but those are finite too.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This ordering is not, in general,
computable, because checking whether a given Turing machine is the
smallest that emits a given (infinite) object is not computable. But
while I don’t know offhand whether every computable set of computable
objects can be computably well-ordered, this is a technical rather
than philosophical question.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>One can, of course, study sets as formal entities in
their own right—“the unique
<a href="https://en.wikipedia.org/wiki/Least-upper-bound_property">Dedekind-complete</a>
totally-ordered field extension of the rationals” is itself a mental
object with reproducible properties, even if nearly all of the limit
points it consists of are not. But how useful of a mental object is
it?<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Sat, 26 Jan 2019 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2019/finitary-finiversalist-church/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Statistical Testing 3</title>
    <link>https://alexey.radul.name/ideas/2018/statistical-testing-3/index.html</link>
    <description><![CDATA[<p>Previously in this miniseries on testing samplers, I <a href="/ideas/2016/on-testing-probabilistic-programs/">laid out the
problem</a> that
statistical testing is inherently more fraught than conventional
software testing, and <a href="/ideas/2017/compositional-statistical-unit-testing/">set up a basic
framework</a> for
defining dependable error-calibrated statistical tests of
stochastic software. I have since learned a great
inequality, and can now lay out a
more complete set of practical basic tests.</p>
<p>To recap, the problem with testing samplers is that even a correct
sampler can emit arbitrarily weird-looking output by sheer chance, and
even a badly incorrect one can accidentally produce a good-looking run
during testing. There’s hope, though: the probability of weird
stuff falls as one draws more samples, and as one loosens the
threshold of what counts as “weird”. So the general strategy for
testing samplers is to draw so many samples, and to fail the test only
on results that are so outlandish, that the probability of a correct
code base triggering a failure in the test suite by chance becomes
negligible; say <span class="math inline">\(10^{-9}\)</span>.</p>
<p>So our statistical test suite passes. Does that mean we’re done? No,
because we also need to make sure that <em>incorrect</em> code has a
negligible probability of <em>not</em> triggering a test failure.
Unfortunately, this can’t be done perfectly: no matter how many
samples we draw, and no matter how tightly we set the criteria for each
test passing, it’s always possible to have a bug whose effects are so
mild or so rare that our test suite can’t reliably detect it. The best we
can do is <em>calibrate</em>: compute the “size” of the mistake we can
reliably detect, and draw enough samples to drive that size small
enough to give us confidence that our software is, in fact, correct.</p>
<p>So, how many samples is “enough”? The technical component of that
problem is the subject of this post. Let us
focus today on one-dimensional continuous testing. That’s not as
restrictive as it may at first seem, because much can be learned about
multivariate distributions by composing multiple tests on projections
(or other probe functions); and discrete spaces can, as a last
resort, be embedded into <span class="math inline">\(\R\)</span> by adding an ordering on the elements.
There are also facts and algorithms that can be used for discrete stuff,
but that will have to wait for another post.</p>
<h2 id="contents">Contents</h2>
<ul>
<li><a href="#the-inequality">The Inequality</a></li>
<li><a href="#the-tests">The Tests</a>
<ul>
<li><a href="#goodness-of-fit-vs-k-s-distance">Goodness of Fit</a></li>
<li><a href="#k-s-tolerant-goodness-of-fit-vs-k-s-distance">Tolerant Goodness of Fit</a></li>
<li><a href="#equality-in-distribution-vs-k-s-distance">Equality</a></li>
<li><a href="#k-s-tolerant-equality-in-distribution">Nearness</a></li>
<li><a href="#bounds-on-the-mean-and-other-statistics">Mean Bounds</a></li>
<li><a href="#sampler-density-agreement">Sampler-Density Agreement</a></li>
</ul></li>
<li><a href="#the-performance">The Performance</a></li>
<li><a href="#the-alternatives">The Alternatives</a></li>
<li><a href="#the-summary">The Summary</a></li>
<li><a href="#the-notes">The Notes</a></li>
</ul>
<h2 id="the-inequality">The Inequality</h2>
<p>Without further ado, suppose we have some sampler <span class="math inline">\(s\)</span> that emits
real-valued samples <span class="math inline">\(x_i \in \R\)</span> and we want to check its properties.
The workhorse for this setting is the <a href="https://en.wikipedia.org/wiki/Dvoretzky%E2%80%93Kiefer%E2%80%93Wolfowitz_inequality">Dvoretzky-Keifer-Wolfowitz
inequality</a>.
It was originally proven by the eponymous mathematicians in 1956 with
an unknown constant. It became unquestionably practically usable for statistical
testing in 1990, when <a href="https://projecteuclid.org/euclid.aop/1176990746">P. Massart determined</a> the constant to be 2.</p>
<p>The inequality gives a stochastic bound on the discrepancy between the
sampler’s true cumulative distribution function (CDF) <span class="math inline">\(F\)</span> and the
empirical cumulative distribution function <span class="math inline">\(F_n\)</span> obtained from drawing
<span class="math inline">\(n\)</span> independent samples:</p>
<p><span class="math display">\[ \Pr\left(\sup_{x \in \R}|F_n(x) - F(x)| &gt; \eps\right) \leq 2e^{-2n\eps^2}. \]</span></p>
<p>The bounded supremum of the left hand side is called the
<a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov-Smirnov
distance</a>
(K-S for short), in this case between the true and empirical CDFs.
The bound is an upper bound on the probability of this distance being large.
The important thing for our purposes is that this bound holds for
finite sample counts <span class="math inline">\(n\)</span> rather than just asymptotically. We will use
it to turn empirical samples into confidence envelopes on the true CDF
they were drawn from, for example like this:</p>
<div style="width:100%">
<p><img src="DKW_bounds.svg" width="200%"/></p>
</div>
<p>The blue staircase is an example empirical CDF, whose true CDF is, in
this case, the smooth curve in orange. The DKM(W) inequality lets us
form the confidence envelope in purple, and bounds the probability
that the true CDF escapes it.</p>
<h2 id="the-tests">The Tests</h2>
<p>Armed with the DKW(M) inequality, we can form an array of calibrated
statistical tests for various situations.</p>
<h3 id="goodness-of-fit-vs-k-s-distance">Goodness of Fit vs K-S Distance</h3>
<p>Suppose the target CDF <span class="math inline">\(F\)</span> is analytically and computationally
tractable (e.g., we are trying to test a sampler for something like
the <a href="https://en.wikipedia.org/wiki/Gamma_distribution">Gamma
distribution</a>).
Then two applications of DKW(M) give a calibrated test that the
program <span class="math inline">\(s\)</span> samples from <span class="math inline">\(F\)</span>.</p>
<p>The test has two parameters, <span class="math inline">\(n\)</span> and <span class="math inline">\(\eps\)</span>. The test procedure is to</p>
<ol type="1">
<li>Draw <span class="math inline">\(n\)</span> samples from <span class="math inline">\(s\)</span>;</li>
<li>Form the empirical CDF <span class="math inline">\(F_n\)</span>;</li>
<li>Compute the K-S distance
<span class="math inline">\(d = \sup_{x \in \R}|F_n(x) - F(x)|;\)</span> then</li>
<li>Pass if <span class="math inline">\(d \leq \eps\)</span> and fail otherwise.</li>
</ol>
<p>To analyze this as a calibrated test, we need an additional (purely
analytic) parameter <span class="math inline">\(\delta\)</span>, for the K-S distance from correctness.
We derive the test’s pass/fail properties as follows:</p>
<ul>
<li><p>The test should pass when <span class="math inline">\(s\)</span> samples from exactly <span class="math inline">\(F\)</span>.</p></li>
<li><p>Applying DKW(M) to <span class="math inline">\(F\)</span> and <span class="math inline">\(F_n\)</span> directly gives an upper bound on the
false-failure rate <span class="math inline">\(\alpha\)</span> of this test, giving
<span class="math display">\[\alpha \leq 2e^{-2n\eps^2}.\]</span></p></li>
<li><p>The test should fail when the true CDF <span class="math inline">\(G\)</span> of <span class="math inline">\(s\)</span> differs in K-S
distance from <span class="math inline">\(F\)</span> by at least <span class="math inline">\(\delta + \eps\)</span>.</p></li>
<li><p>For the test to pass, the empirical CDF <span class="math inline">\(F_n\)</span> has to fall within
<span class="math inline">\(\eps\)</span> of <span class="math inline">\(F\)</span>. For this to happen when the test should fail, the
empirical CDF has to fall least <span class="math inline">\(\delta\)</span> from its actual CDF
<span class="math inline">\(G\)</span>. Applying DKW(M) to <span class="math inline">\(G\)</span> and <span class="math inline">\(F_n\)</span> therefore bounds the false
pass rate <span class="math inline">\(\beta\)</span>:
<span class="math display">\[ \begin{align*}
\beta &amp; = \Pr\left(\sup_{x \in \R}|F_n(x) - F(x)| \leq \eps\right) \\
      &amp; \leq \Pr\left(\sup_{x \in \R}|F_n(x) - G(x)| &gt; \delta\right) \\
      &amp; \leq 2e^{-2n\delta^2}.
\end{align*} \]</span></p></li>
</ul>
<p>To decide how many samples to draw, then, it suffices to choose
acceptable error rates <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, and an acceptable
guarantee gap <span class="math inline">\(\eps + \delta\)</span>, and solve the above inequalities for
<span class="math inline">\(n\)</span> and <span class="math inline">\(\eps\)</span>.</p>
<h3 id="k-s-tolerant-goodness-of-fit-vs-k-s-distance">K-S Tolerant Goodness of Fit vs K-S Distance</h3>
<p>The previous procedure can also be viewed as a test for approximate
equality under K-S distance, namely a test that the true CDF of the
sampler <span class="math inline">\(s\)</span> does not differ in K-S distance from the target CDF <span class="math inline">\(F\)</span> by
more than a tolerance <span class="math inline">\(\zeta &lt; \eps\)</span>. This <span class="math inline">\(\zeta\)</span> is an analytic
parameter: its only effect is to change the definition of when the
test should pass to include all CDFs within <span class="math inline">\(\zeta\)</span> of <span class="math inline">\(F\)</span>, and change
the false failure rate bound to
<span class="math display">\[ \alpha \leq 2e^{-2n(\eps - \zeta)^2}. \]</span></p>
<p>Stands to reason: If we widen the definition of “should pass” to
include everything up to <span class="math inline">\(\zeta\)</span>, the chance of something that should
pass producing a result more than <span class="math inline">\(\eps\)</span> away increases.</p>
<h3 id="equality-in-distribution-vs-k-s-distance">Equality in Distribution vs K-S Distance</h3>
<p>We can use the same inquality to construct a calibrated test that two
samplers <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span> are sampling from the same (one-dimensional)
distribution.</p>
<p>The test again has two parameters, <span class="math inline">\(n\)</span> and <span class="math inline">\(\eps\)</span>. The test procedure
is, of course, very similar to the last one:</p>
<ol type="1">
<li>Draw <span class="math inline">\(n\)</span> samples from <span class="math inline">\(s_1\)</span> and <span class="math inline">\(n\)</span> samples from <span class="math inline">\(s_2\)</span>;</li>
<li>Form the empirical CDFs <span class="math inline">\(F_{1n}\)</span> and <span class="math inline">\(F_{2n}\)</span>;</li>
<li>Compute the K-S distance
<span class="math inline">\(d = \sup_{x \in \R}|F_{1n}(x) - F_{2n}(x)|;\)</span> then</li>
<li>Pass if <span class="math inline">\(d \leq \eps\)</span> and fail otherwise.</li>
</ol>
<p>The analysis differs only in that each bound forces us to use DKW(M)
twice. We again use the distance from correctness <span class="math inline">\(\delta\)</span>, and
obtain</p>
<ul>
<li><p>The test should pass when <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span> both sample from some unknown CDF <span class="math inline">\(G\)</span>.</p></li>
<li><p>Suppose this is so. Suppose the K-S distances of the obtained
empirical samples <span class="math inline">\(F_{1n}\)</span> and <span class="math inline">\(F_{2n}\)</span> from <span class="math inline">\(G\)</span> are <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span>,
respectively. Then for the test to fail, we must have <span class="math inline">\(d_1 + d_2
\geq d &gt; \eps\)</span>. Then, applying DKW(M) to <span class="math inline">\(F_{1n}\)</span> and <span class="math inline">\(G\)</span> and to
<span class="math inline">\(F_{2n}\)</span> and <span class="math inline">\(G\)</span>, we bound<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> the false failure rate <span class="math inline">\(\alpha\)</span> as
<span class="math display">\[ \begin{align*}
\alpha &amp; = \Pr\left(\sup_{x \in \R}|F_{1n}(x) - F_{2n}(x)| &gt; \eps\right) \\
&amp; \leq \Pr\left(\sup_{x \in \R}|F_{1n}(x) - G(x)| &gt; d_1\right) \cdot
       \Pr\left(\sup_{x \in \R}|F_{2n}(x) - G(x)| &gt; d_2\right) \\
&amp; \leq 2e^{-2nd_1^2} 2e^{-2nd_2^2} \\
&amp; \leq 4e^{-2n\eps^2}.
\end{align*} \]</span></p></li>
<li><p>The test should fail when <span class="math inline">\(s_1\)</span> samples from some unknown CDF <span class="math inline">\(G_1\)</span>,
<span class="math inline">\(s_2\)</span> samples from the equally unknown <span class="math inline">\(G_2\)</span>, and the K-S distance
between <span class="math inline">\(G_1\)</span> and <span class="math inline">\(G_2\)</span> is at least <span class="math inline">\(\eps + \delta\)</span>.</p></li>
<li><p>Suppose this is so, and suppose again that the obtained K-S
distances between the empirical samples and their respective CDFs
are <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span>. Then, for the test to pass by chance, we must
have <span class="math inline">\(\eps + \delta \leq d_1 + \eps + d_2\)</span>. By the same double
application of DKW(M), that gives us a false pass rate bound of
<span class="math display">\[ \beta \leq 4e^{-2n\delta^2}. \]</span></p></li>
</ul>
<p>Deciding how many samples to draw is again a matter of finding some
<span class="math inline">\(n\)</span> and <span class="math inline">\(\eps\)</span> that yield acceptable error rates <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>,
and an acceptable guarantee gap <span class="math inline">\(\eps + \delta\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<h3 id="k-s-tolerant-equality-in-distribution">K-S Tolerant Equality in Distribution</h3>
<p>We obtain the tolerant version exactly the same way as when the target
CDF <span class="math inline">\(F\)</span> is known. To wit, if we wish to declare that the test should pass when
<span class="math inline">\(G_1\)</span> and <span class="math inline">\(G_2\)</span> differ in K-S distance by at most <span class="math inline">\(\zeta &lt; \eps\)</span>,
we find that the same test procedure yields
<span class="math display">\[ \alpha \leq 4e^{-2n(\eps - \zeta)^2}. \]</span></p>
<h3 id="bounds-on-the-mean-and-other-statistics">Bounds on the Mean and Other Statistics</h3>
<p>So much for equality in distribution. But with more complex samplers,
we often don’t have useful distributional invariants to work from.
Fortunately, we do often analytically know what the mean of one or
another probe should be, and DKW(M) gives a way to test that, too.</p>
<p>The critical observation is that, if the support of <span class="math inline">\(s\)</span> has an upper
bound, then a confidence envelope on the true CDF also gives a
confidence bound on the mean. Indeed, if the true CDF is within an
<span class="math inline">\(\eps\)</span> envelope of the empirical CDF, then it only has <span class="math inline">\(\eps\)</span>
probability mass of freedom. The most it could do to push the empirical mean up
would be to put all that mass on the upper limit of the support, and
we can efficiently compute what mean that would lead to. Likewise for
lower bounds. The computation is very simple.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> In pseudo-code</p>
<pre><code>def maximum_mean(samples, eps, upper_bound):
  n = len(samples)
  samples_left = ... # discard eps * n smallest samples
  return mean(samples_left) + eps * upper_bound</code></pre>
<p>From this, we can derive calibrated tests on the mean of a sampler.
We can test whether the mean is some known value, or lies in some
known interval (vs lying at least <span class="math inline">\(\delta\)</span> away); we can also test
whether the means of two samplers are equal, or within <span class="math inline">\(\zeta\)</span> of each
other (vs being at least <span class="math inline">\(\zeta + \delta\)</span> away). This also scales
cleanly to arbitrary-dimensional distributions, because a vector mean
is determined by its projections.</p>
<p>The same trick also works for the median, and arbitrary percentiles.
Similar games also give probabilistic bounds on <a href="https://doi.org/10.1081%2Fsta-120006065">the
variance</a>, <a href="https://arxiv.org/abs/cs/0504091">the
entropy</a>, and <a href="https://doi.org/10.1162%2Fneco_a_00144">mutual
information</a>.</p>
<h3 id="sampler-density-agreement">Sampler-Density Agreement</h3>
<p>One specific application I find relevant in my capacity as maintainer
of a statistical library is checking whether a (multivariate) sampler
<span class="math inline">\(s\)</span> samples according to a given density function <span class="math inline">\(f\)</span>. In the univariate
case, we typically have the CDF, so we can just use the equality in
distribution test.</p>
<p>CDFs don’t really work for multivariate distributions, but there is still
something we can do. Suppose for the moment that the support of
the distribution is the compact set <span class="math inline">\(S \subset \R^n\)</span>, with known
volume <span class="math inline">\(|S|\)</span>. Suppose further that the density <span class="math inline">\(f\)</span> is bounded away from zero on <span class="math inline">\(S\)</span>,
i.e., we have a positive real number <span class="math inline">\(h\)</span> for which <span class="math inline">\(\forall x \in S, f(x) &gt; h\)</span>.
Then we have an interesting invariant we can use.</p>
<p>The trick is to estimate the volume of <span class="math inline">\(S\)</span> by using the testee <span class="math inline">\(s\)</span> as
a proposal for an importance sampler. If <span class="math inline">\(s\)</span> is really sampling
according to the density <span class="math inline">\(f\)</span>, the mean weight will be the volume
<span class="math inline">\(|S|\)</span>. To wit, consider the distribution on real numbers given by
<span class="math inline">\(1/f(x)\)</span> for <span class="math inline">\(x\)</span> drawn from <span class="math inline">\(s\)</span>. What is its mean? If <span class="math inline">\(s\)</span> is
sampling according to the probability density <span class="math inline">\(f\)</span>, we will have
<span class="math display">\[ \E_{x \sim s}\left[\frac{1}{f(x)}\right] = \int_S \frac{1}{f(x)} f(x) dx = \int_S 1 dx = |S|. \]</span>
The distribution on <span class="math inline">\(1/f(x)\)</span> is of course bounded below by <span class="math inline">\(0\)</span>, and our lower
bound <span class="math inline">\(h\)</span> on <span class="math inline">\(f\)</span> turns into an upper bound <span class="math inline">\(1/h\)</span> on <span class="math inline">\(1/f\)</span>, so we can proceed to
test this invariant using the mean confidence intervals we worked out
previously. It’s not a perfect check (e.g., <span class="math inline">\(s\)</span> is free to mis-weight
points within any equal-value levels of <span class="math inline">\(f\)</span>), but it’s better than
nothing.</p>
<p>Even if the support of the distribution we are interested in is not
compact, or if the density approaches <span class="math inline">\(0\)</span>, we can still salvage
something by testing a restriction of the distribution of interest.
The only non-mechanical part is choosing a restriction domain and
proving a lower bound on the density within that domain.</p>
<h2 id="the-performance">The Performance</h2>
<p>The measure of speed for our statistical tests is <span class="math inline">\(n\)</span>, the number of
samples we have to draw from <span class="math inline">\(s\)</span>. As you can see from the above
results, this will be logarithmic in the desired error rates, so there
isn’t much reason to tolerate flakiness. Unfortunately, the
performance is also quadratic in the test resolution <span class="math inline">\(1/\eps\)</span>. This
comes from the <span class="math inline">\(n\eps^2\)</span> term in the exponent of the DKW(M) bound. I
don’t know that there’s much that can be done about that, except to
keep the computational cost in mind when setting one’s test
resolutions. In particular, the Law of Large Numbers suggests that a
precision of <span class="math inline">\(O\left(\sqrt{n}\right)\)</span> is about what we should expect where sums
are involved.</p>
<h2 id="the-alternatives">The Alternatives</h2>
<p>There are other inequalities on which finite-sample statistical
testing can be based. In a <a href="/ideas/2017/compositional-statistical-unit-testing/">previous
post</a> I worked
out a test for binary probability distributions based on the CDF of
the binomial distribution. There is also a rich literature of other
so-called “property testing” algorithms, including a very nice survey
of methods applicable to discrete distributions in Gautam Kamath’s MIT
doctoral dissertation <a href="http://www.gautamkamath.com/phdthesis.pdf">“Modern Challenges in Distribution
Testing”</a> (September 2018).</p>
<p>The one point I would like to bring up from there is that different
methods are sensitive to different notions of distance in
distribution, and have different sample efficiency. For example,
consider testing whether a sampler <span class="math inline">\(s\)</span> samples from a given discrete
distribution on <span class="math inline">\(M\)</span> objects. One approach would be to choose an
ordering on those objects, embed them in the interval <span class="math inline">\([0, M) \subset
\R\)</span>, and use the DKW(M)-based continuous test given at the beginning
of this post. This will detect discrepancies exceeding <span class="math inline">\(\eps\)</span>
Kolmogorov-Smirnov distance in <span class="math inline">\(O\left(1/\eps^2\right)\)</span> samples.</p>
<p>Alternately, Gautam gives a test that detects discrepancies exceeding
<span class="math inline">\(\eps\)</span> total variation distance in <span class="math inline">\(O\left(\sqrt{M}/\eps^2\right)\)</span> samples, and
refers to a proof that this is asymptotically optimal.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>
This test can of course also be applied to (even multidimensional)
continuous distributions by binning. Depending on which distance is
more salient to the types of software errors one’s test suite is
looking for, one method or another can give favorable
performance.</p>
<h2 id="the-summary">The Summary</h2>
<p>In this post, we studied the Dvoretzky-Keifer-Wolfowitz (and Massart)
inequality, and worked out how to use it for a wide variety of
calibrated statistical tests for checking correctness of samplers.
DKW(M) leads to simple test procedures with exact finite-sample
guarantees on both false fail and false pass rates.</p>
<p>While DKW(M) is straightforward and general, tests based on other
inequalities may yield better sample efficiency and/or more intuitive
notions of distribution distance in the situations where they apply.</p>
<h2 id="the-notes">The Notes</h2>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      R: "{\\mathbb{R}}",
      Pr: "{\\mathrm{Pr}}",
      E: "\\mathbb{E}",
      eps: "\\varepsilon"
    }
  }
});
</script>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>I conjecture that the 4 in the final bound can be
brought down by using the one-sided version of the DKW(M) inequality,
and taking advantage of the fact that <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span> have to deviate
from <span class="math inline">\(G\)</span> in opposite directions to cause this test procedure to fail.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>If one of <span class="math inline">\(s_1\)</span> or <span class="math inline">\(s_2\)</span> is significantly more
expensive to sample from than the other, it’s possible to squeeze a
little performance out of drawing more samples from the cheaper one,
rather than the same <span class="math inline">\(n\)</span> from each. I leave that extension for the
intrepid reader to work out on their own.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The only tricky thing is that, if <span class="math inline">\(\eps n\)</span> is not an
integer, we should keep that sample, but down-weight it in the mean
computation. The point is to discard <span class="math inline">\(\eps\)</span> worth of probability mass
from the empirical sample before taking its mean, and then to add back
the upper bound of the support, weighted by <span class="math inline">\(\eps\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>These results are consistent with each other, because a
potentially large total variation distance can still look like a small
Kolmogorov-Smirnov distance, if the ordering lines up wrong. Indeed,
TVD can be <span class="math inline">\(O(M)\)</span> times larger than K-S between the same two
distributions.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Sun, 23 Dec 2018 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2018/statistical-testing-3/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Inference <em>by</em> Quadrature</title>
    <link>https://alexey.radul.name/ideas/2017/inference-by-quadrature/index.html</link>
    <description><![CDATA[<p>Production-level probabilistic inference is usually said to be about
very high-dimensional problems. The usual argument for the
techniques one learns (importance sampling, Markov chain Monte Carlo,
etc) starts from the curse of
dimensionality—that classic quadrature is hopelessly inefficient in
many (i.e., more than four) dimensions. But what if one wants
probability in a low-dimensional problem? For example, one
might have a low-dimensional sub-problem of a more complex problem, or
one might want to test a scalable sampling technique on a low-d
version of some problem of interest. How can quadrature be useful?</p>
<p>One way to use quadrature is to explicitly compute the normalization
constant of an unnormalized probability density. Then, up to the
numerical error in the quadrature, one has the <em>normalized</em> density
function, with which one can proceed to do whatever one wants.</p>
<p>Another way to use the same sort of grid of evaluations of the
un-normalized density is the technique called “Griddy Gibbs”.
Griddy Gibbs is applicable when the density of interest is <span class="math inline">\(p(x) = Z \tilde
p(x)\)</span>, where <span class="math inline">\(Z\)</span> is unknown but independent of <span class="math inline">\(x\)</span>, and <span class="math inline">\(\tilde p(x)\)</span>
is computationally tractable. One needs, furthermore, that</p>
<ul>
<li><span class="math inline">\(X\)</span> is one-dimensional;<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
<li>one wants to sample from an approximation to <span class="math inline">\(p(x)\)</span>; and</li>
<li>one can afford to evaluate <span class="math inline">\(\tilde p(x)\)</span> a fair number of times, say 100.</li>
</ul>
<p>When might this happen? The general lore is that if one’s entire
problem were one-dimensional, one would not need samples.
Nonetheless, I can imagine at least two uses for high-quality
one-dimensional sampling methods: testing, and participating in a
larger Gibbs-style sampler.</p>
<p>I have seen the latter arise in hierarchical Bayes quite often.
Generally, a hierarchical Bayesian model often factors as <span class="math inline">\(p(D|\theta)
p(\theta|z) p(z)\)</span>, where <span class="math inline">\(D\)</span> are the data, <span class="math inline">\(\theta\)</span> are some latent
parameters of immediate interest (like the means of some clusters one
is trying to infer) and <span class="math inline">\(z\)</span> are some deeper latent parameters, like
the concentration parameter of one’s prior on the number of clusters.
If, as happens all too often, there aren’t enough conjugacies to
elimiate <span class="math inline">\(z\)</span> or <span class="math inline">\(\theta\)</span>, one many choose a Gibbs-style sampler for
the overall problem, alternating sampling <span class="math inline">\(\theta_{t+1} \sim
p(\theta|D, z_t)\)</span> and <span class="math inline">\(z_{t+1} \sim p(z|\theta_{t+1})\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>
This kind of case fits Griddy Gibbs very well: I have
often seen the distribution <span class="math inline">\(p(z|\theta)\)</span> factor completely into
one-dimensional parts that can be sampled independently. Furthermore,
<span class="math inline">\(\theta\)</span> is often much smaller than <span class="math inline">\(D\)</span>, so the <span class="math inline">\(\theta_{t+1} \sim
p(\theta|D, z_t)\)</span> steps dominate the overall computation, and
therefore one can afford to evaluate <span class="math inline">\(\tilde p(z|\theta)\)</span> many times in each
<span class="math inline">\(z_{t+1} \sim p(z|\theta_{t+1})\)</span> step to get a good <span class="math inline">\(z_{t+1}\)</span>. Since
these models can be quite sensitive to the <span class="math inline">\(z\)</span> parameters, the
investment can pay off handsomely. For example, every sampler I know
of for the <a href="http://jmlr.org/papers/v17/11-392.html">CrossCat</a> model
uses some variant of Griddy Gibbs for the hyperparameters.</p>
<p>So, how does one actually do Griddy Gibbs? The way I learned it was
this:</p>
<ol type="1">
<li><p>Evaluate <span class="math inline">\(\tilde p(x)\)</span> at some grid of points <span class="math inline">\(x_i\)</span>.</p></li>
<li><p>Compute the normalization constant <span class="math inline">\(\hat Z = \sum \tilde p(x_i)\)</span>.</p></li>
<li><p>Approximate <span class="math inline">\(p(x)\)</span> as a categorical distribution on that grid,
<span class="math inline">\(p(x) \approx \frac{1}{\hat Z} \sum \tilde p(x_i) \delta_{x_i}(x)\)</span>.</p></li>
<li><p>Sample from that.</p></li>
</ol>
<p>This has always felt unsatisfying to me. For instance, this
approxmation never admits an off-grid value for <span class="math inline">\(x\)</span>. Surely, I
thought, one can do better. So I finally read the original Ritter and
Tanner 1991 techreport,<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> and found that Mssrs Ritter and
Tanner indeed suggested doing better: they argue in the paper that any
quadrature technique of one’s choosing can form a legitimate
approximation to <span class="math inline">\(p(x)\)</span> from a grid of evaluations of <span class="math inline">\(\tilde p(x)\)</span>,
wherefrom one can then sample.</p>
<p>Here’s one such algorithm, with a piecewise linear
rather than piecewise constant approximation to the CDF of <span class="math inline">\(p(x)\)</span>:</p>
<ol type="1">
<li><p>Evaluate <span class="math inline">\(\tilde p(x)\)</span> at some ordered grid of points <span class="math inline">\(x_0 &lt; x_1 &lt; \ldots &lt; x_i &lt; \ldots &lt; x_n\)</span>.</p></li>
<li><p>Compute <span class="math inline">\(\hat Z = \sum_{i=0}^{n-1} \tilde p(x_i) (x_{i+1} - x_i)\)</span>.</p></li>
<li><p>Approximate <span class="math inline">\(p(x)\)</span> as the piecewise constant PDF
<span class="math display">\[ p(x) \approx \hat p(x) = \begin{cases} 0 &amp; x &lt; x_0 \\ \frac{\tilde p(x_i)}{Z} &amp; x_i \leq x &lt; x_{i+1} \\ 0 &amp; x \geq x_n \end{cases} \]</span>
(whose CDF is therefore piecewise linear).</p></li>
<li><p>Sample from that.</p></li>
</ol>
<p>This looks a lot better. One reason I suspect it may not have caught
on as much is that there are several free choices embedded in what I
presented, leaving more work for the practitioner who wishes to adopt
it. For instance, instead of using the value of <span class="math inline">\(\tilde p\)</span> at the
lower end point of the interval <span class="math inline">\((x_i, x_{i+1})\)</span>, one can use the
average: <span class="math inline">\(\hat p(x) = (\tilde p(x_i) + \tilde p(x_{i+1})) / 2 \hat Z&#39;\)</span>
(which <span class="math inline">\(\hat Z&#39;\)</span> needs to be computed accordingly, and is not equal to
<span class="math inline">\(\hat Z\)</span>). Also, instead of setting the mass outside <span class="math inline">\((x_0, x_n)\)</span> to
<span class="math inline">\(0\)</span> as I have done, one can posit some tails of some functional form
with total mass <span class="math inline">\(\varepsilon\)</span> (which becomes a parameter of the
method) and scale the rest of the PDF accordingly. One can alternately deal
with infinite support by using some change of variable formula to do
the quadrature over a finite interval. In the context of defining a
general library or programming language for inference, I think the
gain is worth the cost, but I can see why a practitioner writing a
one-off inference scheme may decide to avoid this choice space.</p>
<p>I want to point out one more possible adjustment.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> As I learned Griddy
Gibbs, one performs inference by sampling directly from the computed
<span class="math inline">\(\hat p(x)\)</span>, accepting the difference between it and the target <span class="math inline">\(p(x)\)</span>
as error in one’s ultimate solution. However, if one is trying to set
up a convergent Markov chain, one can also treat <span class="math inline">\(\hat p(x)\)</span> as a
Metropolis-Hastings proposal distribution, and guarantee convergence
to the right target by accepting or rejecting it. Such a step looks
like</p>
<ol type="1">
<li><p>As above.</p></li>
<li><p>As above.</p></li>
<li><p>As above.</p></li>
<li><p>Propose to move to <span class="math inline">\(x_{t+1} \sim \hat p(x)\)</span>.</p></li>
<li><p>Compute the acceptance ratio<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>
<span class="math display">\[ \alpha = \frac{\tilde p(x_{t+1}) \hat p(x_t)}{\tilde p(x_t) \hat p(x_{t+1})}. \]</span></p></li>
<li><p>Move to <span class="math inline">\(x_{t+1}\)</span> with probability <span class="math inline">\(\max(1, \alpha)\)</span>, otherwise stay at <span class="math inline">\(x_t\)</span>.</p></li>
</ol>
<p>Here we have gained (as one always does with Metropolis-Hastings)
accuracy in the limiting distribution in exchange for the speed of
reaching it. Indeed, if the approximation <span class="math inline">\(\hat p(x)\)</span> is good
everywhere, then each term <span class="math inline">\(\hat p(x_t) / \tilde p(x_t)\)</span> will be close
to <span class="math inline">\(1\)</span>, and this chain will almost always accept. However, if the
approximation is off somewhere, that will show up as rejections.</p>
<p>As a further refinement, a practitioner could monitor the rejection
rate in order to assess how good of an approximation they got out of
their quadrature technique. I can even imagine automatically adapting
the fineness of the grid, say, to maintain a target acceptance rate as
a chain proceeds.</p>
<p>A benefit, as far as I can tell, of embedding Griddy Gibbs in
Metropolis-Hastings is that the usual problems of trying to develop a
generic numerical quadrature method get masked by the accept/reject
step. For instance, suppose <span class="math inline">\(\tilde p(x)\)</span> has a pole inside the
domain of integration. Then any approximation <span class="math inline">\(\hat p(x)\)</span> must cut
off substantial mass, due to finite resolution near the pole.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>
Evaluated as quadrature, this leads to potentially unacceptable error
in the answers. But evaluated as a proposal distribution, this just
leads to a slowdown in convergence—the chain will fix the
underapproximation of the pole by tending to reject moves away from
it. The same thing happens if the approximation’s tails are too thin
(as long as they are non-zero). The accept/reject step provides
an automatic mis-fit detection mechiansm.</p>
<p>In summary, I think quadrature is a very useful tool in the
inferential kit, and I’m sad that I didn’t get a chance to <a href="https://github.com/probcomp/Venturecxx/issues/642">implement
it in Venture</a>.</p>
<h2 id="references">References</h2>
<ul>
<li><p>Ritter, C. &amp; Tanner, M., “The Griddy Gibbs sampler”, 1991,
Technical Report #878, Department of Statistics, University of Wisconsin-Madison
<a href="https://www.stat.wisc.edu/sites/default/files/tr878_0.pdf">https://www.stat.wisc.edu/sites/default/files/tr878_0.pdf</a></p></li>
<li><p>Ritter, C. &amp; Tanner, M., “Facilitating the Gibbs Sampler: The Gibbs
Stopper and the Griddy-Gibbs Sampler”, 1992, Journal of the American
Statistical Association (87) pp. 861-868</p></li>
</ul>
<h2 id="notes">Notes</h2>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
MathJax.Hub.Config({
  TeX: {
    equationNumbers: { autoNumber: "AMS" },
    noErrors: { disabled: true },
  }
});
</script>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>The idea generalizes to any dimensionality, of course. It
is even plausibly useful in up to three or four, but I want to present
in 1-D for simplicity.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Both samples may be approximate. As long as the transition operators
<span class="math inline">\(T_{z_t}(\theta_{t+1}|\theta_t)\)</span> and <span class="math inline">\(T_{\theta_{t+1}}(z_{t+1}|z_t)\)</span>
are ergodic and have the stationary distributions <span class="math inline">\(p(\theta|D, z_t)\)</span>
and <span class="math inline">\(p(z|\theta_{t+1})\)</span>, respectively, the overall chain will converge
to <span class="math inline">\(p(\theta, z|D)\)</span> as <span class="math inline">\(t \to \infty\)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The published version, Ritter and Tanner 1992,
does not appear to be open access.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>I have not seen this refinement in the literature, but that’s probably
because I didn’t look hard enough.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>As written, <span class="math inline">\(\alpha\)</span> is correct if the choice of grid
<span class="math inline">\(x_i\)</span> does not depend on the current state <span class="math inline">\(x_t\)</span>. If <span class="math inline">\(x_i\)</span> does
depend on <span class="math inline">\(x_t\)</span>, computing the reverse probability calls for repeating
the quadrature on the grid points <span class="math inline">\(x_{t+1}\)</span> determines. This is fine—one gives up 2x
in affordable grid resolution in exchange for adapting the grid to the areas the
chain wants to explore. For instance, doing a pair of
<a href="https://en.wikipedia.org/wiki/Tanh-sinh_quadrature">tanh-sinh</a>
integrations over <span class="math inline">\((-\infty, x_t)\)</span> and <span class="math inline">\((x_t, \infty)\)</span> would be a way to
focus a lot of computational attention near the current point, which should
be good for exploring a tall peak (once the chain found it).<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Unless, of course, the operator knows where the pole is
and deploys appropriate tactics to tame it, but that can only
work on a case-by-case basis.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>true</pubDate>
    <guid>https://alexey.radul.name/ideas/2017/inference-by-quadrature/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Compositional statistical unit testing</title>
    <link>https://alexey.radul.name/ideas/2017/compositional-statistical-unit-testing/index.html</link>
    <description><![CDATA[<p>How do you unit-test a sampler? Run it a bunch and see whether the
histogram looks good—but there’s always <em>some</em> chance that your test
will fail randomly even though the sampler is good, or pass randomly
despite a bug. And if you try to have more than one test, the chance
of random errors goes up. How big of a chance? How much worse does
it get? What to do?</p>
<p>I’ve <a href="../../2016/on-testing-probabilistic-programs/">written</a> about
this problem before. That was a starting point. In this post,
I want to</p>
<ul>
<li><p>Work out a general formalism for reasoning
about the error rates of tests of stochastic software;</p></li>
<li><p>Work out the rules of composition for tests in this formalism; and</p></li>
<li><p>Build up to a usable test, namely discrete equality in distribution
(one variant where the expected distribution is known and one where
it is itself avaiable only as a sampler).</p></li>
</ul>
<p>This post is not so good for the math-averse—I need the crutch of
precise reasoning to make sure I don’t miss anything important.</p>
<h2 id="contents">Contents</h2>
<ul>
<li><p><a href="#calibrated-tests">Calibrated Tests</a></p></li>
<li><p><a href="#calibrated-test-suites">Calibrated Test Suites</a></p>
<ul>
<li><p><a href="#amplifying-confidence">Amplifying Confidence</a></p></li>
<li><p><a href="#composing-tests">Composing Tests</a></p></li>
<li><p><a href="#negating-tests">Negating Tests</a></p></li>
</ul></li>
<li><p><a href="#usable-tests">Usable Tests</a></p>
<ul>
<li><p><a href="#a-building-block">A Building Block</a></p></li>
<li><p><a href="#two-sample-equality-in-distribution">Two-Sample Equality in Distribution</a></p></li>
<li><p><a href="#one-sample-equality-in-distribution">One-Sample Equality in Distribution</a></p></li>
</ul></li>
<li><p><a href="#conclusion">Conclusion</a></p></li>
<li><p><a href="#notes">Notes</a></p></li>
</ul>
<h2 id="calibrated-tests">Calibrated tests</h2>
<p>Suppose we are trying to statistically assess the behavior of some
(presumably stochastic) software <span class="math inline">\(s\)</span>.
Let <span class="math inline">\(\S\)</span> be the space of possible true behaviors of <span class="math inline">\(s\)</span>, the software under test.
Then a single <em>calibrated test</em> <span class="math inline">\(T\)</span> is a 5-tuple: <span class="math inline">\((\tau, S^g, S^b,
\alpha, \beta)\)</span> where</p>
<ul>
<li><span class="math inline">\(\tau: \S \to \bool\)</span> is a stochastic test procedure returning
<span class="math inline">\(\text{True}\)</span> or <span class="math inline">\(\text{False}\)</span> (which presumably runs the software
under test repeatedly and measures its behavior somehow),</li>
<li><span class="math inline">\(S^g \subset \S\)</span> is the set of behaviors of <span class="math inline">\(s\)</span> that are “good”,</li>
<li><span class="math inline">\(S^b \subset \S\)</span> is the set of behaviors of <span class="math inline">\(s\)</span> that are “bad”, and</li>
<li><span class="math inline">\(\alpha \geq 0\)</span> and <span class="math inline">\(\beta \geq 0\)</span> are error tolerances, such that</li>
<li>the false-fail rate is low: if <span class="math inline">\(s \in S^g\)</span>, then <span class="math inline">\(p(\tau(s) = \text{False}) \leq \alpha\)</span>, and</li>
<li>the false-pass rate is low: if <span class="math inline">\(s \in S^b\)</span>, then <span class="math inline">\(p(\tau(s) = \text{True})  \leq \beta\)</span>.</li>
</ul>
<p>For example, consider the situation from <a href="../../2016/on-testing-probabilistic-programs/">my previous post on the
subject</a>: <span class="math inline">\(s\)</span> performs
some unknown computation and “fails” with unknown probability <span class="math inline">\(\pf\)</span>.
In this case, the number <span class="math inline">\(\pf\)</span> completely characterizes the behavior
of <span class="math inline">\(s\)</span> that we are interested in, so <span class="math inline">\(\S = [0,1]\)</span>. We wish to bound
<span class="math inline">\(\pf\)</span>, so we set <span class="math inline">\(S^g = [0, 0.1]\)</span>. Suppose our test procedure <span class="math inline">\(\tau\)</span>
is “Execute <span class="math inline">\(s\)</span> independently <span class="math inline">\(n = 10\)</span> times, and pass if <span class="math inline">\(s\)</span> ‘fails’
at most 3 of them, inclusive.” Then it’s easy enough to compute that
<span class="math inline">\(\alpha \geq 0.0128\)</span> will satisfy the false-fail rate equation. There
are many choices of <span class="math inline">\(S^b\)</span> and <span class="math inline">\(\beta\)</span> that will satisfy the false-pass
rate equation for this choice of <span class="math inline">\(\tau\)</span>, one of which is <span class="math inline">\(S^b = [0.7,
1], \beta = 0.011\)</span>. As one might imagine, increasing <span class="math inline">\(n\)</span> can let us
decrease <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, and enlarge <span class="math inline">\(S^b\)</span>.</p>
<p>Note that the above test only makes guarantees about the outcome of
<span class="math inline">\(\tau\)</span> if <span class="math inline">\(\pf &lt; 0.1\)</span> or <span class="math inline">\(\pf &gt; 0.7\)</span>, but says nothing for the
intervening interval. This is a general phenomenon, because
<span class="math inline">\(p(\tau(s) = True)\)</span> is continuous in the tested behavior <span class="math inline">\(s\)</span>.
Ergo, if <span class="math inline">\(\alpha + \beta &lt; 1\)</span>, the sets <span class="math inline">\(S^g\)</span> and <span class="math inline">\(S^b\)</span> must be
separated. If <span class="math inline">\(S^g\)</span> and <span class="math inline">\(S^b\)</span> are connected to each other in <span class="math inline">\(\S\)</span>,
this implies incomplete coverage: <span class="math inline">\(S^g \cup S^b \subsetneq \S\)</span>.</p>
<p>There are two symmetric ways to think about this gap. One is to say
that we, in testing <span class="math inline">\(s\)</span>, know what behavior <span class="math inline">\(S^g\)</span> is acceptable. In
this case, everything else is a “bug” which we would ideally like to
distinguish from <span class="math inline">\(S^g\)</span>, but, unless <span class="math inline">\(S^g\)</span> is both closed and open in <span class="math inline">\(\S\)</span>, we can’t. So
we settle for defining <span class="math inline">\(S^b\)</span> to be the bugs that are “severe enough”
that we commit to finding them. In so doing, we balance coverage (a
large <span class="math inline">\(S^b\)</span>) against confidence (small <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>), and
against the computational cost of <span class="math inline">\(\tau\)</span>.</p>
<p>Symmetrically, we could say that we know what behavior <span class="math inline">\(S^b\)</span> is
unacceptable, and define <span class="math inline">\(S^g\)</span> as the behaviors that are “obviously
good” enough to commit to pronouncing them correct. On this view we
are balancing the test’s robustness (large <span class="math inline">\(S^g\)</span>) against confidence
and computation.</p>
<h2 id="calibrated-test-suites">Calibrated test suites</h2>
<p>So much for an isolated calibrated test. How can we compose these
things? Like traditional (deterministic) unit tests, we can negate
test assertions, and connect multiple assertions with “and” (or “or”).
We just need to exercise some care with the resulting good/bad sets
and error rates. Unlike tests of deterministic software, it is also
meaningful to repeat a calibrated test, as a way of using computation
to gain more confidence. Let’s work these out in reverse order.</p>
<h3 id="amplifying-confidence">Amplifying confidence</h3>
<p>We can generically trade computation for confidence by
repeating a test independently more than once. Indeed, if <span class="math inline">\(T = (\tau, S^g,
S^b, \alpha, \beta)\)</span> is any calibrated test, we can drive down
the false-pass rate by forming <span class="math inline">\(T_\text{and}^k = (\tau^k_\text{and}, S^g, S^b, k\alpha,
\beta^k)\)</span>. Here, <span class="math inline">\(\tau^k_\text{and}\)</span> is “Run
<span class="math inline">\(\tau\)</span> independently <span class="math inline">\(k\)</span> times and return the logical ‘and’ of all the
results.” The only way for <span class="math inline">\(\tau^k_\text{and}\)</span> to pass is if all <span class="math inline">\(k\)</span>
runs of <span class="math inline">\(\tau\)</span> pass. If this is to be a false pass, i.e., if <span class="math inline">\(s\)</span> is
actually in <span class="math inline">\(S^b\)</span>, the probability of each pass of <span class="math inline">\(\tau\)</span> is bounded
by <span class="math inline">\(\beta\)</span>, which bounds the false-pass rate of <span class="math inline">\(\tau^k_\text{and}\)</span> by
<span class="math inline">\(\beta^k\)</span>, as desired. However, the chance of a false-fail goes up:
for <span class="math inline">\(\tau^k_\text{and}\)</span> to falsely fail, it suffices for at least one
of the runs of <span class="math inline">\(\tau\)</span> to fail even though <span class="math inline">\(s \in S^g\)</span>. The
probability of this happening is bounded by
<span class="math display">\[ \text{if } s \in S^g\quad p(\tau^k_\text{and}(s) = \text{False}) \leq 1 - (1 - \alpha)^k \leq k\alpha. \]</span>
The former inequality is tight if <span class="math inline">\(\alpha\)</span> was a tight bound on the
false fail rate of <span class="math inline">\(\tau\)</span>, but the latter is generally conservative.
However, it’s simple, and if <span class="math inline">\(k\alpha\)</span> is, as is usually desired, much
less than <span class="math inline">\(1\)</span>, then the quadratic and higher terms in <span class="math inline">\((1 - \alpha)^k\)</span>
are likely to be negligible.</p>
<p>Symmetrically, we can drive down the false-fail rate by forming
<span class="math inline">\(T_\text{or}^k = (\tau^k_\text{or}, S^g, S^b, \alpha^k, k\beta)\)</span>,
where <span class="math inline">\(\tau^k_\text{or}\)</span> is “Run <span class="math inline">\(\tau\)</span> independently <span class="math inline">\(k\)</span> times and
return the logical ‘or’ of all the results.” Since the error rate we
are driving down falls exponentially while the other only climbs
linearly, we can amplify any non-trivial initial calibrated test <span class="math inline">\(T\)</span> into one
with arbitrarily small <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. (In fact, with a suitable
“<span class="math inline">\(k\)</span> of <span class="math inline">\(m\)</span> passes” scheme, any initial error
rates satisfying <span class="math inline">\(\alpha + \beta &lt; 1\)</span> are enough.)</p>
<h3 id="composing-tests">Composing tests</h3>
<p>The calibrated test formalism gets more interesting when we consider
combining different tests. Suppose we have
<span class="math inline">\(T_1 = (\tau_1, S^g_1, S^b_1, \alpha_1, \beta_1)\)</span> and
<span class="math inline">\(T_2 = (\tau_2, S^g_2, S^b_2, \alpha_2, \beta_2)\)</span>. Then we can get a more
specific test that’s still calibrated. Let <span class="math inline">\(\tau_1 \cdot \tau_2\)</span> be the
testing procedure that runs <span class="math inline">\(\tau_1\)</span> and <span class="math inline">\(\tau_2\)</span> independently and returns
the logical “and” of the answers. Then
<span class="math display">\[ T_1 \cdot T_2 = (\tau_1 \cdot \tau_2, S^g_1 \cap S^g_2, S^b_1 \cup S^b_2, \alpha_1 + \alpha_2, \max(\beta_1, \beta_2)) \]</span>
is a calibrated test. Why? If the software is good according to both
<span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span>, the false fail rate is bounded by
<span class="math display">\[ p((\tau_1 \cdot \tau_2)(s) = \text{False}) \leq 1 - (1 - \alpha_1)(1 - \alpha_2) \leq \alpha_1 + \alpha_2.\]</span></p>
<p>Conversely, if the software is bad according to either <span class="math inline">\(T_1\)</span> or <span class="math inline">\(T_2\)</span>,
then it has to obey one or the other false-pass rate equation, so
<span class="math inline">\(\max(\beta_1, \beta_2)\)</span> bounds the overall false-pass rate.
Note, by the way, that if we stick to the tight bound <span class="math inline">\(1 - (1 -
\alpha_1)(1 - \alpha_2)\)</span> rather than the simple <span class="math inline">\(\alpha_1 + \alpha_2\)</span>,
the <span class="math inline">\(\cdot\)</span> operation on tests becomes associative.</p>
<p>What have we gained? By being a little careful with the confidences,
we can form compound tests that ensure all of a set of desirable
properties <span class="math inline">\(S^g_i\)</span> obtain together, or, equivalently, weed out any of
a set of severe bugs <span class="math inline">\(S^b_i\)</span>. In other words, we can build up complex
specifications of “correctness” by intersecting simpler specifications
<span class="math inline">\(S^g_i\)</span>, while maintaining reasonable coverage through unioning the
corresponding unacceptable sets <span class="math inline">\(S^b_i\)</span>. This is the basis on which
developers of stochastic software <span class="math inline">\(s\)</span> can construct a test suite—if
they take care to tighten the <span class="math inline">\(\alpha\)</span> bounds of the tests from time
to time so that the <span class="math inline">\(\alpha\)</span> of the overall test suite stays within
acceptable limits.</p>
<p>For the record, the symmetric definition of <span class="math inline">\(\tau_1 + \tau_2\)</span> as the logical “or” of
<span class="math inline">\(\tau_1\)</span> and <span class="math inline">\(\tau_2\)</span> gives us
<span class="math display">\[ T_1 + T_2 = (\tau_1 + \tau_2, S^g_1 \cup S^g_2, S^b_1 \cap S^b_2, \max(\alpha_1, \alpha_2), \beta_1 + \beta_2), \]</span>
which is a way of building up compound unacceptable behaviors by
intersecting simpler ones.</p>
<h3 id="negating-tests">Negating tests</h3>
<p>Finally, no notion of a test suite is complete without the idea of a
test that is known to fail, because of a known bug in the underlying
software that just hasn’t been fixed yet. The corresponding concept
here is test negation: given a calibrated test <span class="math inline">\(T = (\tau, S^g, S^b,
\alpha, \beta)\)</span>, we can invert the sense of <span class="math inline">\(\tau\)</span> by running it once
and negating the answer. The resulting <span class="math inline">\(\tilde \tau\)</span> gives the calibrated test
<span class="math display">\[ \tilde T = (\tilde \tau, S^b, S^g, \beta, \alpha),\]</span>
which makes the same distinction as <span class="math inline">\(T\)</span> with the same confidence,
but with the reverse sense.</p>
<p>This does not fully cover the role “expected failure” plays in
deterministic testing. The latter has the property that if some test
fails for an unknown reason that one does not currently have time to
deal with, one can mark that test “expected failure”, get the test
suite to pass that way, and then return to investigating the problem
later. Negating a statistical test does not reliably obtain the same
effect, because the software <span class="math inline">\(s\)</span> could be in the dead zone <span class="math inline">\(\S - (S^g
\cup S^b)\)</span>, where neither <span class="math inline">\(T\)</span> nor <span class="math inline">\(\tilde T\)</span> will pass reliably.
Sadly, there is nothing that can be done at the level of opaque
calibrated tests in this situation. Perhaps a practical testing framework should
have an additional marking whose effect is “run this test (to check
that it completes without crashing) but ignore the answer” to
accommodate this situation.</p>
<h2 id="usable-tests">Usable tests</h2>
<p>Let us now turn our attention to using this machinery to build the
simplest actually usable calibrated tests: discrete equality in
distribution. We will get two variants: the general version, where
the “expected” distribution is itself given by a sampler, and a more
efficient version where the “expected” distribution is an explicit
list of objects and their probabilities. The latter is also called a
“goodness of fit” test.</p>
<p>The student of statistics will recognize that both of these tasks have
well-known half-calibrated tests (the <span class="math inline">\(\chi^2\)</span> family is popular). I
call the standard tests “half”-calibrated because, while their
false-fail rates are well-understood, I am not aware of any
characterization of their false-pass rates. The obstacle, presumably,
is that such a characterization requires understanding the behavior of
the test statistic for any possible “bad” state of the software under
test.</p>
<h3 id="a-building-block"><em>A</em> Building Block</h3>
<p>Let us start with a two-sample inequality in probability test, as
this will let us build both of our promised compound tests. To
wit, suppose our software <span class="math inline">\(s\)</span> under test provides two operations,
<span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span>, each of which returns true or false. We wish to
check that <span class="math inline">\(p_1 = p(s_1 = \text{True}) \leq p_2 = p(s_2 = \text{True})\)</span>.</p>
<p>Formally, our software <span class="math inline">\(s\)</span> is characterized completely by the two
numbers <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span>, so <span class="math inline">\(\S = [0,1] \times [0,1]\)</span>. Our desired
“good” region is <span class="math inline">\(S^g = \{p_1 \leq p_2\}\)</span>. <span class="math inline">\(S^b\)</span> must be disconnected
from <span class="math inline">\(S^g\)</span>, but it would be nice to have a parameter by which we can
let it approach arbitrarily close; so let’s pick <span class="math inline">\(S^b = \{ p_1 \geq
p_2 + \eps \}\)</span> for some <span class="math inline">\(\eps &gt; 0\)</span>.</p>
<p>What can be do for a testing procedure <span class="math inline">\(\tau\)</span>? Not much, really. The
simplest option<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> is to run <span class="math inline">\(s_1\)</span> some number <span class="math inline">\(n_1\)</span> times, run
<span class="math inline">\(s_2\)</span> <span class="math inline">\(n_2\)</span> times, and count the numbers <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span> that they
respectively produce True. Having obtained those results, use some
decision rule to emit True or False from the test as a function of
<span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span>, and then calibrate <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> by computing
or bounding the probability of False if <span class="math inline">\(p_1 \leq p_2\)</span>, and of
True if <span class="math inline">\(p_1 \geq p_2 + \eps\)</span>.</p>
<p>I don’t have any special insight on the best way to choose such a
decision rule, but here’s one that will do:
<span class="math display">\[ \tau(n_1, n_2, \eps)(s) = \begin{cases} \text{True} &amp; \text{if } \frac{k_1}{n_1} \leq \frac{k_2}{n_2} + \frac{\eps}{2} \\
  \text{False} &amp; \text{otherwise}. \end{cases} \]</span></p>
<p>What false-fail rate does this have? Well, if <span class="math inline">\(s\)</span> is actually
characterized by <span class="math inline">\(p_1, p_2\)</span>, then the probability of any given outcome <span class="math inline">\(k_1, k_2\)</span> is
<span class="math display">\[ p(k_1, k_2|p_1, p_2) = {n_1 \choose k_1} p_1^{k_1} (1 - p_1)^{n_1 - k_1}
  {n_2 \choose k_2} p_2^{k_2} (1 - p_2)^{n_2 - k_2}. \]</span>
The probability of failure is the probability that any of the outcomes
leading to False will occur, and the worst-case false-fail rate
is that probability for the <span class="math inline">\(p_1 \leq p_2\)</span> that maximize it:
<span class="math display">\[
\begin{align*}
 \alpha &amp; = \max_{p_1 \leq p_2} \left[ \sum_{\frac{k_1}{n_1} &gt; \frac{k_2}{n_2} + \frac{\eps}{2}}
  {n_1 \choose k_1} p_1^{k_1} (1 - p_1)^{n_1 - k_1}
  {n_2 \choose k_2} p_2^{k_2} (1 - p_2)^{n_2 - k_2} \right] \\
 &amp; = \max_{p_1 \leq p_2}
  \left[ \sum_{k_2} {n_2 \choose k_2} p_2^{k_2} (1 - p_2)^{n_2 - k_2}
    \left( \sum_{\frac{k_1}{n_1} &gt; \frac{k_2}{n_2} + \frac{\eps}{2}}
      {n_1 \choose k_1} p_1^{k_1} (1 - p_1)^{n_1 - k_1}
    \right) \right]. \\
\end{align*}
\]</span></p>
<p>The inner sum is convenient, being the survivor function of the
binomial distribution of <span class="math inline">\(n_1\)</span> trials with success rate <span class="math inline">\(p_1\)</span>. Since
the binomial survivor function is at every point monotonic in the
success probability, it’s safe to say that for any given <span class="math inline">\(p_2\)</span>, the
maximum over <span class="math inline">\(p_1\)</span> is at the highest value admissible by the
constraint, in this case <span class="math inline">\(p_1 = p_2\)</span>. This reduces our optimization
problem to one dimension.</p>
<p>The false-pass rate is similar:
<span class="math display">\[ \beta = \max_{p_1 \geq p_2 + \eps}
  \left[ \sum_{k_2} {n_2 \choose k_2} p_2^{k_2} (1 - p_2)^{n_2 - k_2}
    \left( \sum_{\frac{k_1}{n_1} \leq \frac{k_2}{n_2} + \frac{\eps}{2}}
      {n_1 \choose k_1} p_1^{k_1} (1 - p_1)^{n_1 - k_1}
    \right) \right], \]</span>
with a similar simplification due to the inner sum being a binomial
cumulative distribution function, which is maximized by minimizing
<span class="math inline">\(p_1\)</span>.</p>
<p>The above optimizations are not very hard to solve. Here are a few
computed error rates for tests with given <span class="math inline">\(\eps\)</span>, <span class="math inline">\(n_1\)</span>, and <span class="math inline">\(n_2\)</span>
parameters:</p>
<table>
<thead>
<tr>
<th><span class="math inline">\(\eps\)</span></th>
<th><span class="math inline">\(n_1\)</span></th>
<th><span class="math inline">\(n_2\)</span></th>
<th><span class="math inline">\(\alpha\)</span></th>
<th style="text-align: left;"><span class="math inline">\(\beta\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td>0.20</td>
<td>30</td>
<td>30</td>
<td>0.183155</td>
<td style="text-align: left;">0.253557</td>
</tr>
<tr>
<td>0.20</td>
<td>100</td>
<td>100</td>
<td>0.072367</td>
<td style="text-align: left;">0.082141</td>
</tr>
<tr>
<td>0.20</td>
<td>200</td>
<td>200</td>
<td>0.020901</td>
<td style="text-align: left;">0.023023</td>
</tr>
<tr>
<td>0.20</td>
<td>271</td>
<td>271</td>
<td>0.009036</td>
<td style="text-align: left;">0.009910</td>
</tr>
<tr>
<td>0.10</td>
<td>30</td>
<td>30</td>
<td>0.349442</td>
<td style="text-align: left;">0.347353</td>
</tr>
<tr>
<td>0.10</td>
<td>100</td>
<td>100</td>
<td>0.218377</td>
<td style="text-align: left;">0.260811</td>
</tr>
<tr>
<td>0.10</td>
<td>300</td>
<td>300</td>
<td>0.102815</td>
<td style="text-align: left;">0.117143</td>
</tr>
<tr>
<td>0.10</td>
<td>1000</td>
<td>1000</td>
<td>0.011947</td>
<td style="text-align: left;">0.013132</td>
</tr>
<tr>
<td>0.10</td>
<td>1085</td>
<td>1085</td>
<td>0.009634</td>
<td style="text-align: left;">0.009974</td>
</tr>
<tr>
<td>0.05</td>
<td>30</td>
<td>30</td>
<td>0.448711</td>
<td style="text-align: left;">0.397353</td>
</tr>
<tr>
<td>0.05</td>
<td>100</td>
<td>100</td>
<td>0.361886</td>
<td style="text-align: left;">0.361332</td>
</tr>
<tr>
<td>0.05</td>
<td>300</td>
<td>300</td>
<td>0.270163</td>
<td style="text-align: left;">0.269769</td>
</tr>
<tr>
<td>0.05</td>
<td>1000</td>
<td>1000</td>
<td>0.127058</td>
<td style="text-align: left;">0.136325</td>
</tr>
<tr>
<td>0.05</td>
<td>3000</td>
<td>3000</td>
<td>0.025816</td>
<td style="text-align: left;">0.026889</td>
</tr>
<tr>
<td>0.05</td>
<td>4334</td>
<td>4334</td>
<td>0.009880</td>
<td style="text-align: left;">0.009995</td>
</tr>
</tbody>
</table>
<h3 id="two-sample-equality-in-distribution">Two-sample equality <em>in</em> distribution</h3>
<p>With that capability in hand, we can now ask for something practically
usable that I don’t know any other way to get: a calibrated test that
two discrete samplers are sampling from the same distribution. To
wit, suppose <span class="math inline">\(s\)</span> now consists of two programs <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span>, each of
which randomly emits one of <span class="math inline">\(D\)</span> tokens. We wish to test that <span class="math inline">\(s_1\)</span>
and <span class="math inline">\(s_2\)</span> have the same probability of emitting each token.</p>
<p>This can be accomplished with a conjunction of <span class="math inline">\(D\)</span> of the two-sample
inequality tests described above: just assert that the probability of
<span class="math inline">\(s_1\)</span> generating each token <span class="math inline">\(d\)</span> is no more than the probability of
<span class="math inline">\(s_2\)</span> generating the same token:
<span class="math display">\[ \text{And}_{d \in D} \big[p(s_1 = d) \leq p(s_2 = d)\big]. \]</span>
The reverse inequalities follow by conservation of belief—the
total probability in both distributions must be 1.</p>
<p>One could use this to, for example, design a calibrated unit test for,
say, a Chinese Restaurant Process sampler. The CRP is a probability
distribution on partitions. There are 18 possible partitions of a set
of 4 distinct objects into clusters. So, one could design a unit test
that checks whether two purported samplers for the CRP distribution on 4 objects in
fact agree. In this style, that test would check, independently for
each of the 18 possible partitions, that the probability of drawing
that one from program 1 is no more than the probability of drawing it
from program 2. If one designed each test to make <span class="math inline">\(n\)</span> trials, one
would need <span class="math inline">\(18n\)</span> trials in total, and, using the above
single-inequality procedure, one could obtain the following
computation-precision-confidence trade off:</p>
<table>
<thead>
<tr>
<th><span class="math inline">\(\eps\)</span></th>
<th style="text-align: left;"><span class="math inline">\(n\)</span></th>
<th style="text-align: left;"><span class="math inline">\(18n\)</span></th>
<th><span class="math inline">\(\alpha\)</span></th>
<th style="text-align: left;"><span class="math inline">\(\beta\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td>0.20</td>
<td style="text-align: left;">30</td>
<td style="text-align: left;">540</td>
<td>0.973787</td>
<td style="text-align: left;">0.253557</td>
</tr>
<tr>
<td>0.20</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">1800</td>
<td>0.741313</td>
<td style="text-align: left;">0.082141</td>
</tr>
<tr>
<td>0.20</td>
<td style="text-align: left;">300</td>
<td style="text-align: left;">5400</td>
<td>0.109433</td>
<td style="text-align: left;">0.007154</td>
</tr>
<tr>
<td>0.20</td>
<td style="text-align: left;">530</td>
<td style="text-align: left;">9540</td>
<td>0.009010</td>
<td style="text-align: left;">0.000531</td>
</tr>
<tr>
<td>0.10</td>
<td style="text-align: left;">30</td>
<td style="text-align: left;">540</td>
<td>0.999564</td>
<td style="text-align: left;">0.347353</td>
</tr>
<tr>
<td>0.10</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">1800</td>
<td>0.988144</td>
<td style="text-align: left;">0.260811</td>
</tr>
<tr>
<td>0.10</td>
<td style="text-align: left;">300</td>
<td style="text-align: left;">5400</td>
<td>0.858134</td>
<td style="text-align: left;">0.117143</td>
</tr>
<tr>
<td>0.10</td>
<td style="text-align: left;">1000</td>
<td style="text-align: left;">18000</td>
<td>0.194546</td>
<td style="text-align: left;">0.013132</td>
</tr>
<tr>
<td>0.10</td>
<td style="text-align: left;">2120</td>
<td style="text-align: left;">38160</td>
<td>0.009576</td>
<td style="text-align: left;">0.000572</td>
</tr>
<tr>
<td>0.05</td>
<td style="text-align: left;">30</td>
<td style="text-align: left;">540</td>
<td>0.999978</td>
<td style="text-align: left;">0.397353</td>
</tr>
<tr>
<td>0.05</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">1800</td>
<td>0.999692</td>
<td style="text-align: left;">0.361332</td>
</tr>
<tr>
<td>0.05</td>
<td style="text-align: left;">300</td>
<td style="text-align: left;">5400</td>
<td>0.996548</td>
<td style="text-align: left;">0.269769</td>
</tr>
<tr>
<td>0.05</td>
<td style="text-align: left;">1000</td>
<td style="text-align: left;">18000</td>
<td>0.913356</td>
<td style="text-align: left;">0.136325</td>
</tr>
<tr>
<td>0.05</td>
<td style="text-align: left;">3000</td>
<td style="text-align: left;">54000</td>
<td>0.375490</td>
<td style="text-align: left;">0.026889</td>
</tr>
<tr>
<td>0.05</td>
<td style="text-align: left;">8480</td>
<td style="text-align: left;">152640</td>
<td>0.009889</td>
<td style="text-align: left;">0.000573</td>
</tr>
</tbody>
</table>
<p>If those trial runs look a bit large, there are couple reasons:</p>
<ul>
<li><p>Each individual test of the 18 is pessimistic, guaranteeing its
error rates even if the probability of that particular partition is
the most confusing possible, namely close to 1/2. But, of course,
they can’t all be close to 1/2. A directly synthetic test like the
<span class="math inline">\(\chi^2\)</span> test of independence is likely to be much more efficient.
Learning how to calibrate its false pass rate would be a useful
advance.</p></li>
<li><p>Relatedly, the design outlined here wastes computation on assuring
the independence of the individual tests. Namely, when one is
measuring the frequency of some partition, one could presumably use
those samples to measure the frequency of each of the 17 others as
well. I think it would be fruitful to think through what
restrictions, if any, there are on sharing samples across tests of
different facets of the same sampler.</p></li>
<li><p>In this particular design, I arbitrarily chose that <span class="math inline">\(\tau(n_1, n_2, \eps)
= \text{True}\)</span> when <span class="math inline">\(k_1/n_1 \leq k_2/n_2 + \eps/2\)</span>. That threshold
balances the false fail and false pass rates for each individual
test, but the compound test, being an “and”, has a substantially
higher false fail rate than each component, but the same false pass
rate. Therefore, some more efficiency can be obtained by making the
threshold larger than <span class="math inline">\(k_2/n_2 + \eps/2\)</span>. That way, the computation
used to ensure the false fail rate is acceptable is not wasted on
driving the false pass rate lower than needed.</p></li>
</ul>
<h3 id="one-sample-equality-in-distribution">One-sample Equality <em>in</em> Distribution</h3>
<p>We can actually gain a lot of computational efficiency if we know the
expected distribution analytically. Then we can use a one-sample test
on each dimension, and save many trials by not having to account for
too many hard-to-assess probabilities that are near 0.5.</p>
<p>In the case of the Chinese Restaurant Process, the analytic
distribution on partitions is known. For example, if the
concentration parameter is 0.5, the 18 partitions have these
probabilities:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Partition</th>
<th style="text-align: left;">Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">[1, 1, 1, 1]</td>
<td style="text-align: left;">0.457142857143</td>
</tr>
<tr>
<td style="text-align: left;">[1, 1, 1, 2]</td>
<td style="text-align: left;">0.0761904761905</td>
</tr>
<tr>
<td style="text-align: left;">[1, 1, 2, 1]</td>
<td style="text-align: left;">0.0761904761905</td>
</tr>
<tr>
<td style="text-align: left;">[1, 1, 2, 2]</td>
<td style="text-align: left;">0.0380952380952</td>
</tr>
<tr>
<td style="text-align: left;">[1, 1, 2, 3]</td>
<td style="text-align: left;">0.0190476190476</td>
</tr>
<tr>
<td style="text-align: left;">[1, 2, 1, 1]</td>
<td style="text-align: left;">0.0761904761905</td>
</tr>
<tr>
<td style="text-align: left;">[1, 2, 1, 2]</td>
<td style="text-align: left;">0.0380952380952</td>
</tr>
<tr>
<td style="text-align: left;">[1, 2, 1, 3]</td>
<td style="text-align: left;">0.0190476190476</td>
</tr>
<tr>
<td style="text-align: left;">[1, 2, 2, 1]</td>
<td style="text-align: left;">0.0380952380952</td>
</tr>
<tr>
<td style="text-align: left;">[1, 2, 2, 2]</td>
<td style="text-align: left;">0.0761904761905</td>
</tr>
<tr>
<td style="text-align: left;">[1, 2, 2, 3]</td>
<td style="text-align: left;">0.0190476190476</td>
</tr>
<tr>
<td style="text-align: left;">[1, 2, 3, 1]</td>
<td style="text-align: left;">0.0190476190476</td>
</tr>
<tr>
<td style="text-align: left;">[1, 2, 3, 2]</td>
<td style="text-align: left;">0.0190476190476</td>
</tr>
<tr>
<td style="text-align: left;">[1, 2, 3, 3]</td>
<td style="text-align: left;">0.0190476190476</td>
</tr>
<tr>
<td style="text-align: left;">[1, 2, 3, 4]</td>
<td style="text-align: left;">0.00952380952381</td>
</tr>
</tbody>
</table>
<p>Using the one-sample test design function from <a href="../../2016/on-testing-probabilistic-programs/">my previous
post</a>, we can design 18
tests corresponding to these probabilities. These designs will
naturally spend less computation on the low-probability partitions,
because those are easier to reliably falsify. With that design, we
can get to <span class="math inline">\(0.01\)</span> error rates with these total trial counts:</p>
<table>
<thead>
<tr>
<th><span class="math inline">\(\eps\)</span></th>
<th>Trials</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.2</td>
<td>1244</td>
</tr>
<tr>
<td>0.1</td>
<td>3894</td>
</tr>
<tr>
<td>0.05</td>
<td>12813</td>
</tr>
</tbody>
</table>
<p>A dramatic improvement over the two-sample situation.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I think the composable structure of calibrated tests forms a
sufficient backbone for an intellectually sound unit testing framework
for stochastic software. Of course, a few more things do need to be
worked out before one can become practical. Some come to mind
offhand:</p>
<ol type="1">
<li><p>It would be nice to get more efficient calibrated tests (perhaps in
the <span class="math inline">\(\chi^2\)</span> style) for equality in distribution than those
presented here. I just started with these to prove that it could
be done, and to demonstrate how the compositions tend to work out.</p></li>
<li><p>Continuous distributions can be attacked by binning, but it would
be more satisfying to calibrate something like the
<a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov-Smirnov</a>
test.</p></li>
<li><p>Any library would do well to provide tests of nearness (as opposed
to equality) in distribution, with the distance bound given either
as an explicit number or implicitly as the distance between two
other distributions. This should be particularly useful for
testing approximate inference algorithms, which promise only to
move a distribution toward a goal, not to actually get there.
Designing such tests directly could be substantially more efficient
than composing them out of 1-D probability inequalities.</p></li>
<li><p>There is an interesting design space of continuous integration
tools, since now there is confidence to be gained by rerunning a
test that already passed, or increasing its computation budget
internally.</p></li>
<li><p>I’ve presented forward computations, deriving the best obtainable
error bounds from a given test procedure. I think a tool could
become substantially more practical if it automated the experiment
design problem: The user tells the framework the <span class="math inline">\(S^g, S^b,
\alpha\)</span>, and <span class="math inline">\(\beta\)</span> they want, and the framework finds parameters
for the testing procedure(s) <span class="math inline">\(\tau\)</span> to meet those requirements
while using as little computation as it can. This becomes very
interesting when compound tests are involved, because the framework
could (within limits) squeeze more confidence out of cheaper
individual tests, and go a bit easier on expensive ones, and still
get its overall error rate bounds.</p></li>
</ol>
<p>But, that’s all!</p>
<h2 id="notes">Notes</h2>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      S: "{\\mathbb{S}}",
      bool: "{\\mathbb{B}}",
      pf: "p_{\\text{fail}}",
      and: "\\text{and}",
      eps: "\\varepsilon"
    }
  }
});
</script>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Not the only option, as one can imagine choosing <span class="math inline">\(n_1\)</span> and
<span class="math inline">\(n_2\)</span> dynamically, as results from previous trials come in. Such a
technique may be a way to save computation, but a sound analysis is
beyond the scope of this blog post.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Sun, 19 Nov 2017 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2017/compositional-statistical-unit-testing/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Musings <em>on</em> Probprog</title>
    <link>https://alexey.radul.name/ideas/2017/musings-on-prob-prog/index.html</link>
    <description><![CDATA[<p>People in my circles periodically talk and write about the nature of
this emerging new beast that is called probabilistic programming.
There’s various talk about how it’s about samplers, or density
functions, or Markov chains, or making machine learning or artificial
intelligence more accessible, or various other such stuff. Certainly
those things hover in the air around probabilistic programming, but I don’t
think that gets to the essence of it. I think that
probabilistic programming, as opposed to standard programming, is a
new scope of problems that can be solved by computing.</p>
<style>
.entry-content dl dt {
  font-weight: bold;
  display: inline;
  margin-left: 0px;
  float: left;
  text-indent: 0px;
  margin-bottom: 0px;
  margin-right: 0.5em;
}

.entry-content dl dd {
  margin-left: 0px;
}

</style>
<p>This new scope
is obtained by relaxing what it means for a problem to be “solved”.
To wit, a problem like “Consider a hypothetical Earth satellite in
geosynchronous orbit, weighing 500 kg. What country plausibly
launched it and why?” has no definite “answer”. Yet, in the presence
of a model of satellites, computation may be applied and plausible
(and, one hopes, useful) candidates (e.g., “It could be a US
communications satellite”) may be generated.</p>
<p>To me, this shift is analogous to my college experience of shifting
from studying math to studying computers. The standard I learned
from the math department for what constitutes a “solution” was the presence
of a clear, understandable, and communicable answer, supported by a
(potentially very difficult or obscure) proof of its correctness. The
Fermat-Wiles theorem provides a spectacular example: For what integers
<span class="math inline">\(n &gt; 0\)</span> are there any triples of positive integers <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and <span class="math inline">\(z\)</span>
such that <span class="math inline">\(x^n + y^n = z^n\)</span>? The answer is simple and clear:
<span class="math inline">\(n \leq 2\)</span>. The proof is an eighty-page book about elliptic curves that
not many people in the world can read, but the problem is considered
solved.</p>
<p>The standard of solution I learned from the computer science
department is different: a problem is solved when there is a program
that computes the answer acceptably quickly. It is necessary to trust
or verify that the program is correct, but it is not necessary to be
able to predict its output on any given instance. The problem of
finding the shortest path from one node of a network to another
is classic. Even though someone can hand me a network in which I
won’t be able to intuit shortest paths, I nonetheless consider the
problem solved (as a computational problem) because effective
algorithms are well-disseminated and high quality implementations are
available.</p>
<p>In some sense, the computational standard is a relaxation of the
mathematical one. An answer that the mathematicians consider good
looks to the CS people like an extremely fast program—much faster
than necessary. And so the scope of addressable problems broadens.
Computer programming offers attacks on all sorts of problems that
simply do not have simple, understandable answers.</p>
<p>The relationship between classical and probabilistic programming seems
similar to me. A “normal” program computes an answer. As a
probability distribution, this has zero entropy—much less than
strictly necessary, from the probprog lens. And so the scope,
similarly, broadens, to problems like our satellite that simply
do not have definite answers.</p>
<p>So what is the shape of this new scope of attackable problems? What
do the solutions look like? The common characteristic of the problems is
the presence of modelable uncertainty. That’s why it’s called
“probabilistic”: probability theory serves as the framework in which
to approach uncertainties in one’s problem with computational objects.</p>
<p>As for solutions, and the internal workings of the discipline, I can
name several relatively novel features:</p>
<dl>
<dt>
Uncertainty in, uncertainty out.
</dt>
<dd>
Imprecisely posed questions
necessarily have imprecise answers. It <em>could</em> be a US
communications satellite; it could be a Chinese one too, though
perhaps that’s less likely. The “solution” to such a problem, then,
is (a program that samples from) a probability distribution on
possible answers.
</dd>
<dt>
Knowable unknowns.
</dt>
<dd>
Generally, the uncertainty within a given model
of a given situation can be characterized. For instance, the
meta-question “What is the probability (in this model) that it’s a
US communications satellite?” does have a definite answer. Such
answers are often intractable to compute to full floating-point
precision, but can always be approximated arbitrarily well with
sufficient computation.
(The <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a> guarantees
<span class="math inline">\(O(\sqrt{N})\)</span> accuracy for <span class="math inline">\(O(N)\)</span> computation for many questions,
including probabilities.) There are, of course, also many such
probability questions to which the exact answer is known.
</dd>
<dt>
New problems, new tactics.
</dt>
<dd>
Expanding the space of approachable
problems has the side-effect of expanding the space of valid
subproblems. This, in turn, expands the universe of tactics by
which problems may be decomposed and solved. For instance, instead
of directly forming a probabilistic model of 500-kg satellites in
geosynchronous orbit, or even of the Earth satellites that exist, we
can form a probabilistic model of possible structures for a domain
of satellites. If we then ask that model “How is the true satellite
domain plausibly structured, in light of this data set about real
satellites?”, answers to that question can become models of the
extant Earth satellites, which can then be queried for inferences
about origin and purpose from mass and orbit class. It often turns out
that this indirection is actually an easier path to decent answers
for the original question.
</dd>
<dt>
Inverse reasoning.
</dt>
<dd>
A specific problem template to which probability
applies very well is “Here is an uncertain model of a causal
process; here is an observed result; how might the process have gone
to produce it?” The problem is formalized as conditioning the joint
distribution over cause-result pairs on the observed result. There
is no acceptably efficient general solution, but many usable
(usually approximate) methods have been developed. This template is
fruitful enough that many authors view it as paradigmatic of the
entire field.
</dd>
<dt>
All models are wrong, but some are useful.
</dt>
<dd>
It seems, looking at the
practice of probabilistic problem solving, that it tolerates more
severe, or at least more explicit, gaps between the real-world
problems it attacks and the models with which it attacks them than
classical programming does. Perhaps this is because uncertainty is
already present within the formalism, so the uncertainty in the
model-reality gap falls under some of the same quality assurance
processes as the in-model uncertainty does.
</dd>
<dt>
Robustness.
</dt>
<dd>
An interesting side-effect of models that explicitly
admit uncertainty is that they can often “go with the flow” if
something unexpected happens. If the data fails to conform to the
model, the model just concludes that there must have been a lot of
noise, and makes the best of it. This both makes it easier to get
useful results about a new situation than traditional programming,
and harder to diagnose errors in model specification.
</dd>
<dt>
Natural approximation.
</dt>
<dd>
To the extent that “a sample from a
probability distribution”, or “a sampler for a probability
distribution”, constitutes a solution to a problem, the probability
paradigm admits a new natural notion of approximation. To wit, a
sample or a sampler may often still be acceptable if its
distribution is merely close (in, say, Kullback-Leibler divergence)
to the “exact” distribution. Many probabilistic techniques
naturally trade computational cost for nearness of approximation;
much effort can sometimes be saved by not making the approximation
gap very much smaller than the model-reality gap.
</dd>
<dt>
Squishy Testing.
</dt>
<dd>
Testing a probabilistic program is <a href="../../2016/on-testing-probabilistic-programs">squishier</a>.
After all, even a correct program can produce arbitrarily weird
results by sheer chance, as can an incorrect program produce
plausible results by sheer chance. So the testing is also a
probabilistic activity, though generally its error can be driven
down by computation.
</dd>
<p>What, then, is probabilistic programming? It is the practice of
applying computation to generate probabilistic solutions to
probabilistically posed problems. It is also the practice of making
(software) agents that operate in light of a capability to solve such
problems.</p>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
MathJax.Hub.Config({
  TeX: {
    equationNumbers: { autoNumber: "AMS" },
    noErrors: { disabled: true },
  }
});
</script>]]></description>
    <pubDate>Mon, 07 Aug 2017 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2017/musings-on-prob-prog/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title><em>On</em> Testing Probabilistic Programs</title>
    <link>https://alexey.radul.name/ideas/2016/on-testing-probabilistic-programs/index.html</link>
    <description><![CDATA[<p>Testing traditional software strives for an ideal of exact
determinacy: If a test fails, there is a bug and the developers must
investigate; and if a test passes, the bugs it is testing for are not
present, so development can proceed without worrying about them. This
is not possible with <a href="/ideas/2016/on-intentionally-random-programs">probabilistic</a>
<a href="/ideas/2016/probabilistic-programming-habits">programs</a>—even a correct sampler
for a probability distribution always has <em>some</em> chance of producing
arbitrarily weird output by bad luck, and an incorrect one can equally
well look fine by coincidence. In this piece, I want to
lay out my current thinking on what to do about this problem.</p>
<h2 id="contents">Contents</h2>
<ul>
<li><a href="#a-general-case">A General Case</a>
<ul>
<li><a href="#significance">Significance</a></li>
<li><a href="#power">Power</a></li>
</ul></li>
<li><a href="#test-framework">Test Framework</a>
<ul>
<li><a href="#scaling">Scaling</a></li>
</ul></li>
<li><a href="#statistical-optimizations">Statistical Optimizations</a></li>
<li><a href="#computational-optimizations">Computational Optimizations</a></li>
<li><a href="#composition">Composition</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#references">References</a></li>
<li><a href="#notes">Notes</a></li>
</ul>
<h2 id="a-general-case"><em>A</em> General Case</h2>
<p>I’d like to start by investigating as general a probabilistic testing
situation as I can think of, and then refine what we find in a few
directions.</p>
<p>Consider the following stochastic testing scenario.</p>
<blockquote>
Here is a black box, <span class="math inline">\(\F\)</span>, which performs some arbitrary stochastic
computation under test and somehow evaluates whether the result is
good. I consider the behavior of <span class="math inline">\(\F\)</span> acceptable if its probability of
reporting a bad result is less than <span class="math inline">\(p\)</span>.
</blockquote>
<p>For instance, one might test a program for, say, Gaussian process
function learning, by showing it examples drawn from a known function,
running training for a while, and asking it to extrapolate to a new
input point. Presumably one cannot analytically predict what the
extrapolation distribution should be, but one can call a trial “good”
if it’s within some distance of the true function’s value there.
Presumably one also cannot analytically predict the probability that a
correct training pipeline will still extrapolate badly on this example
by chance; but one could make up a plausible minimum rate of success
and claim that the whole training pipeline works “well enough” if it
produces “bad” extrapolations at most <span class="math inline">\(20\%\)</span> of the time.</p>
<p>What is a testing framework to do with a test like this? At this
interface, the only thing that can be done is to run the black box
some number of times <span class="math inline">\(n\)</span>, and report the overall test as having failed if
the black box reports bad results at least some fraction <span class="math inline">\(f\)</span> of the
time.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<h3 id="significance">Significance</h3>
<p>How well does this work? For any given <span class="math inline">\(f\)</span> and <span class="math inline">\(n\)</span>, we can compute
(an upper bound on) the probability that a correctly-implemented <span class="math inline">\(\F\)</span>
will nonetheless fail such a test by chance.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> This
probability is called the <em>statistical significance</em> of the
test.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Here’s a chart showing how the probability of failing a
<span class="math inline">\(10\)</span>-trial test with cutoff <span class="math inline">\(3\)</span> (marked at <span class="math inline">\(f = 0.35\)</span>)
varies with the true bad result rate of <span class="math inline">\(\F\)</span>.</p>
<div style="width:100%">
<p><img src="example-sig.png" /></p>
</div>
<p>The figure marks an example value for the “no bug” probability <span class="math inline">\(p\)</span> at <span class="math inline">\(0.26\)</span> and
traces the false fail rate (<span class="math inline">\(\approx 0.2479\)</span>) thus obtained. Changing <span class="math inline">\(f\)</span> moves the
inflection point of the curve, and increasing <span class="math inline">\(n\)</span> makes the curve
steeper.</p>
<p>The false fail rate bound is
given by the worst still-acceptable behavior of <span class="math inline">\(\F\)</span>, namely to report
a bad result with probability exactly <span class="math inline">\(p\)</span>. In this case, for the overall test
to fail we must see at least <span class="math inline">\(fn\)</span> bad results in <span class="math inline">\(n\)</span> trials, the
probability of which,
<span class="math display">\[
\sum_{i=\lceil fn \rceil}^n {n \choose i} p^i (1-p)^{n-i}
  = 1 - \textrm{CDF}_{\textrm{binomial}(n, p)}(fn),
\]</span>
is given by the survivor function of the binomial distribution on <span class="math inline">\(n\)</span>
trials with probability <span class="math inline">\(p\)</span>. This quantity is readily computable, and
tends to zero as <span class="math inline">\(n\)</span> tends to infinity, provided <span class="math inline">\(f &gt; p\)</span>. In other
words, if we set the bad result rate cutoff <span class="math inline">\(f\)</span> above the maximum
acceptable true probability of bad results <span class="math inline">\(p\)</span>, we can drive the
probability of false alarms in our test suite arbitrarily low (but
never to exactly zero) by spending more computation (increasing <span class="math inline">\(n\)</span>).
As far as false alarms go, this is the next best thing to ideal testing.</p>
<h3 id="power">Power</h3>
<p>So far so good. But, of course, we don’t need any of this math to
make a very fast test with a very low rate of false alarms—just
always pass! Testing has another desideratum, namely keeping the
chance of passing despite the presence of bugs down as well. This
chance is called the <em>statistical power</em>.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>Unfortunately, it is not possible to obtain high significance and high
power at once when trying to arbitrate an infintely fine boundary with
a finite amount of computation. If we want high significance, then
however large we may set the number
of trials <span class="math inline">\(n\)</span>, we have to set the
decision boundary <span class="math inline">\(f\)</span> at least a little larger than the “acceptable”
bad result rate <span class="math inline">\(p\)</span>. But then, if the true bad result
rate of <span class="math inline">\(\F\)</span> is, say, between <span class="math inline">\(p\)</span> and <span class="math inline">\(f\)</span>, this is nominally “a bug”,
but the chance of detecting it and failing the overall test will not
be high.</p>
<p>So we have to fall back one more time, and say that we will tolerate
our test suite being unable to reliably catch bugs with “small”
consequences, provided it has high power against bugs whose effects
are “severe enough”. This idea is often called the <em>effect size</em> in the
statistical hypothesis testing literature.</p>
<p>To wit, we can name an arbitrary <span class="math inline">\(q
&gt; p\)</span> and say that <span class="math inline">\(\F\)</span> exhibits a “severe bug” if its true probability
of reporting a bad result is at least <span class="math inline">\(q\)</span>. We can add this <span class="math inline">\(q\)</span> to our chart:</p>
<div style="width:100%">
<p><img src="example-pwr.png" /></p>
</div>
<p>The curve now also gives the probability of passing (right scale) the
overall test. The updated figure now marks an example “severe
bug” probability <span class="math inline">\(q\)</span> at <span class="math inline">\(0.44\)</span>, and traces the false pass rate
(<span class="math inline">\(\approx 0.2877\)</span>) thus obtained. Moving the inflection point by
changing <span class="math inline">\(f\)</span> trades off significance for power in a given experimental
design. Making the curve steeper by increasing <span class="math inline">\(n\)</span> offers greater
significance and power at once, at the cost of additional computation.</p>
<p>Formally, a test consisting
of <span class="math inline">\(n\)</span> trials and failing if at least an <span class="math inline">\(f\)</span>-fraction of them are bad
will have a false pass rate of at most
<span class="math display">\[
\sum_{i=0}^{\lfloor fn \rfloor} {n \choose i} q^i (1-q)^{n-i}
  = \textrm{CDF}_{\textrm{binomial}(n, q)}(fn).
\]</span>
This quantity is also readily computable, and also tends to zero as
<span class="math inline">\(n\)</span> tends to infinity, provided <span class="math inline">\(f &lt; q\)</span>. So for any desired
significance and power, and any finite effect size to be detected, we
can construct a test with those characteristics by picking some <span class="math inline">\(p &lt; f
&lt; q\)</span> and some sufficiently large <span class="math inline">\(n\)</span>. (Which we might as well
minimize subject to these constraints.)</p>
<h2 id="test-framework">Test Framework</h2>
<p>So here we have four variables of interest to whoever is running the
test suite:</p>
<ul>
<li><p>Statistical significance, or probability of a false alarm even if
the tested system works;</p></li>
<li><p>Effect size, or some measure of the severity of the effects of bugs
with detection guarantees;</p></li>
<li><p>Statistical power, or the probability of a false pass even if the
tested system actually contains a severe bug; and</p></li>
<li><p>Computational cost of the test.</p></li>
</ul>
<p>A test framework can permit the operator to specify any desired three
of these (e.g., by command line arguments) and deduce the fourth (and
the allowable bad result rate, which is needed for running the test but
is not otherwise interesting). Here is a Python function that
computes <span class="math inline">\(n\)</span> and <span class="math inline">\(fn\)</span> from <span class="math inline">\(p\)</span>, <span class="math inline">\(q\)</span>, the significance, and the power:</p>
<pre><code>from scipy.stats import binom

def design(p, q, sig, pwr):
    # p is the max bug-free probability of a bad trial
    assert p &gt;= 0 and p &lt; 1
    # q is the min severe-bug probability of a bad trial
    assert q &gt; p and q &lt;= 1
    # sig is the desired max probability of a false fail
    assert sig &gt; 0 and sig &lt;= 1
    # pwr is the desired max probability of a false pass
    assert pwr &gt; 0 and pwr &lt;= 1
    n = 1
    while True:
        # Minium k for the desired false alarm rate is given
        # by the inverse survivor function of the binomial
        # distribution
        min_k = binom.isf(sig, n, p)
        # Maximum k for the desired false pass rate is given
        # by the inverse CDF of the binomial distribution
        max_k = binom.ppf(pwr, n, q)
        if max_k &gt; min_k:
            # scipy&#39;s fenceposts are such that running n
            # trials and failing if strictly more than min_k
            # of them are bad achieves the requested
            # significance and power.
            return (n, min_k)
        else:
            n += 1</code></pre>
<p>Exercise for the reader: Prove that the optimal design is
unique. That is, when <code>design</code> returns, <code>max_k == min_k + 1</code>.</p>
<p>The other directions are simpler: if the computational budget <span class="math inline">\(n\)</span> and,
say, the desired significance <code>sig</code> are fixed, the number of bad results
that must be tolerated is given by <code>min_k = binom.isf(sig, n, p)</code> as
above. Tolerating no more than that simultaneously optimizes the
power for all possible effect sizes. The resulting strength of
detection curve can be reported to the test suite operator to let them
judge whether it is acceptable.</p>
<h3 id="scaling">Scaling</h3>
<p>Permit me a few observations on how well designs constructed by
<code>design</code> stand to perform.</p>
<ul>
<li><p>Some modest number of samples like <span class="math inline">\(50\)</span> really don’t get one all
that far. For example,</p>
<p><code>design(0.05, 0.3, 1e-3, 1e-3) == (73, 10)</code>.</p></li>
<li><p>Detecting small effects with good significance and power can take
<em>many</em> trials, especially if <span class="math inline">\(p\)</span> is already large. For example,</p>
<p><code>design(0.3, 0.31, 1e-3, 1e-3) == (80983, 24698)</code>.</p></li>
<li><p><span class="math inline">\(n\)</span> should be logarithmic in power, b/c running the same test
twice and failing if either fails is a lower bound on what you
can get by doubling the number of trials, and squares the
probability of a false pass.</p></li>
<li><p>Similary, <span class="math inline">\(n\)</span> should be logarithmic in significance, b/c running
twice and failing only if both fail squares the probability of a
false fail.</p></li>
<li><p>If <span class="math inline">\(np(1-p)\)</span> is large relative to <span class="math inline">\(1\)</span>, <span class="math inline">\(n\)</span> should be quadratic
in <span class="math inline">\(1/(q-p)\)</span> (inverse absolute effect size). Why? For fixed
significance, the decision boundary needs to exclude that much
mass in the failure frequency distribution. If <span class="math inline">\(n\)</span> is large
relative to <span class="math inline">\(p(1-p)\)</span>, the shape of the distribution will be
Gaussian, so the mass excluded by a decision boundary will be
given by the number of standard deviations that boundary is away
from the mode. Squeezing the effect size squeezes the room
available to place the decision boundary, so will require a
proportional squeeze on the standard deviation, which will
require a quadratic increase in <span class="math inline">\(n\)</span>.</p></li>
<li><p>For small <span class="math inline">\(p\)</span> and fixed <em>relative</em> effect size, <span class="math inline">\(n\)</span> should be
linear in <span class="math inline">\(1/p\)</span>. Why? Holding <span class="math inline">\(np\)</span> constant will leave the
distribution on observed spurious failures more or less fixed,
near a Poisson with rate <span class="math inline">\(np\)</span>. By “fixed relative effect size”
I mean that <span class="math inline">\(q = cp\)</span> for some constant <span class="math inline">\(c\)</span>. In this regime,
holding <span class="math inline">\(np\)</span> fixed will also hold <span class="math inline">\(nq\)</span> fixed, thus keeping the
distribution on observed failures given the minimal severe bug
fixed as well, near a Poisson with rate <span class="math inline">\(nq\)</span>. The desired
decision boundary and obtained significance and power will be
determined by those two Poisson distributions.</p></li>
<li><p>For fixed <em>absolute</em> effect size, at some point reducing <span class="math inline">\(p\)</span> will
stop significantly moving <span class="math inline">\(q\)</span> or making significant room to
obtain the desired power by moving the decision boundary, so <span class="math inline">\(n\)</span>
will asymptote to a constant and the obtained false failure rate
will tend to zero.</p></li>
</ul>
<h2 id="statistical-optimizations">Statistical Optimizations</h2>
<p>The above discussion assumes very little about the procedure under
test. Often enough, there will be circumstances where more is known.
In this case, doing more math will let one obtain the same
discrimination strength with less computation.</p>
<p>For example, suppose the procedure <span class="math inline">\(\P\)</span> under test were a random
function returning objects from the set {apple, banana, orange, pear},
and one analytically knew the exact probability distribution on
outputs that it should have if implemented correctly. Sticking
strictly to the above interface, one would have to come up with some
crockery like “run <span class="math inline">\(\P\)</span> fifty times, compute the <span class="math inline">\(\chi^2\)</span> statistic on
the results (against the expected frequencies), and assert that it is
less than <span class="math inline">\(1\%\)</span> no more than <span class="math inline">\(1\%\)</span> of the time.” Which the above test
framework would then decide to run maybe a hundred times to obtain the
desired overall significance, for a total of <span class="math inline">\(5,\!000\)</span> runs of <span class="math inline">\(\P\)</span>.</p>
<p>Needless to say, this is rather inefficient. <span class="math inline">\(5,\!000\)</span> runs of <span class="math inline">\(\P\)</span> are
enough to get plenty good significance by doing a single big
<a href="https://en.wikipedia.org/wiki/Pearson_chi-squared_test"><span class="math inline">\(\chi^2\)</span> test</a>
and inspecting its p-value. The <span class="math inline">\(\chi^2\)</span> test has well-defined
measures of effect size,<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> and corresponding power analyses, so much
computation can be saved by directly implementing a test controller
for <span class="math inline">\(\chi^2\)</span> with knobs for significance, power, effect size, and
compute budget, instead of going through the Boolean case above.
Presumably, many other standard statistical tests can be treated
similarly to good effect.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>The concept of “alternate hypothesis” becomes more relevant for a more
structured test than for the basic Boolean scenario. Suppose I am
actually writing a regression test, and I have a definite other
distribution that I expect <span class="math inline">\(\P\)</span> to produce if it contains the
particular implementation bug I am looking for. It seems appropriate
for a testing framework to allow (but not require) a test author to
specify such an alternative (or set of alternatives) in the test
definition. It would then be up to the test framework to automatically
compute what “effect size” that corresponds to and make sure that the
specified alternative registers as a sufficiently “severe” bug that the
overall test power guarantee applies to it.</p>
<h2 id="computational-optimizations">Computational Optimizations</h2>
<p>A different kind of optimization would be stopping early when the
trials are coming up sufficiently extreme. For example, if one is
starting <span class="math inline">\(100\)</span> scheduled runs of some testee <span class="math inline">\(\F\)</span> that’s expected to
report bad results no more than <span class="math inline">\(20\%\)</span> of the time, and one finds that <span class="math inline">\(19\)</span> of the
first <span class="math inline">\(20\)</span> trials were bad, one would not be unjustified in thinking
that failing immediately is sound and would speed the overall test up
by a factor of four.</p>
<p>Indeed, it’s tempting to follow the above intuition and define a
testing procedure for <span class="math inline">\(\F\)</span> like this:</p>
<ol type="1">
<li>Repeat:</li>
<li>Run <span class="math inline">\(\F\)</span> once and record the result;</li>
<li>If the probability, assuming no bug, of at least as many bad results
as recorded is less than <code>sig</code>, stop and report failure;</li>
<li>If the probability, assuming a severe bug, of at least as many good results
as recorded is less than <code>pwr</code>, stop and report success;</li>
<li>Else continue.</li>
</ol>
<p>This procedure does have the merit that if the true behavior of <span class="math inline">\(\F\)</span>
is very far from the decision boundary, it will run fewer than <span class="math inline">\(n\)</span>
trials, and come to a conclusion in less time. It has the demerit,
however, that its significance and power are different from the given
<code>sig</code> and <code>pwr</code>. Why? Consider some sequence of <span class="math inline">\(n\)</span> results that
would cause the test to pass if observed in full. It could, by
chance, have been skewed to have relatively more bad results in the
beginning, and may therefore cause the above procedure to fail the
test instead, increasing the false failure rate. Of course, another
sequence that leads to a failure if observed in full could be skewed
optimistic, and lead to an early pass, lowering the false failure
rate. Which force dominates is not apparent a priori, but I suspect
that the overall rate of mistakes will be higher than advertised if
early stopping is permitted.</p>
<p>Does that mean there is no hope for running fewer than the allotted
<span class="math inline">\(n\)</span> trials in extreme circumstances? No. It just means that the
end-to-end behavior of the test driver needs to studied. A
general deterministic driver is a decision function that maps
sequences of observed trials to one of three actions: “Stop and fail”,
“Stop and pass”, or “Continue”. At any given effect size of interest,
the significance and power obtained by such a function are well
defined. Studying these, and choosing good functions for various testing
situations,<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> is a corner of statistics named <em>sequential inference</em>,
for example <a href="https://projecteuclid.org/download/pdf_1/euclid.aoms/1177731118">Wald 1945</a>.
In fact, that paper presents a design for a sequential testing procedure
that does obtain any requested significance and power, and claims an
expected <span class="math inline">\(\approx 50\%\)</span> reduction in the expected number of trials needed
vs static experimental design.</p>
<p>A different kind of computational optimization comes by borrowing from
traditional software development the idea of continuous integration.
The standard story is that one gets a server (or a software as a
service provider) to watch one’s version control system, and on every
commit, rebuild the software and rerun the test suite. For software
that’s intended to be deterministic, that is the maximum possible use
of computation for testing: full test run (<span class="math inline">\(n=1\)</span>) on every commit.</p>
<p>For probabilistic programs, however, the benefit that can be derived
from “always on” resources is unlimited—having the budget to raise
<span class="math inline">\(n\)</span> can always lead to better significance or better power or both.
Alternately, the benefit can be thought of as spreading the work
required for a large <span class="math inline">\(n\)</span> over multiple testing stages, and producing a
live report of current success/fail state and significance/power
thereof in the presence of adequate incremental progress.</p>
<p>There is an additional opportunity around being able to pool testing
results across different versions (commit states) of the software
under test. The challege preceding that opportunity is to derive or
infer a reasonable model of which commits did or did not have
meaningful effects on which tests in the test suite. I think all of
this would be a very fruitful avenue for a toolsmith to explore.</p>
<h2 id="composition">Composition</h2>
<p>So far, we’ve only talked about iterating a single test, but in this
regime, composing multiple tests into a test suite is also somewhat
trickier than in the deterministic case. Indeed, suppose we have
<span class="math inline">\(100\)</span> tests, each independently capable of failing by chance even if
the code is correct with probability, say, <span class="math inline">\(10^{-5}\)</span>. Then an overall
test suite consisting of running each of those tests once will fail by
chance with a probability just shy of <span class="math inline">\(10^{-3}\)</span>. A test framework
should be aware of this, either by automatically adjusting the
significance expected of individual tests in order to meet a
test-suite-level significance goal, or at least by reporting the
overall significance obtained by any given test run.</p>
<p>Statistical power composes differently from statistical significance.
Since an aggregate test suite is judged to pass only if all its
constituent tests pass, the probability of a false pass in any given
state of bugginess can only go down as more tests are added. And
indeed, it goes down quite precipitously if we add redundant tests
that cover the same underlying bugs, or if our continuous integration
system redundantly re-runs the tests we have.</p>
<p>However, there is a sense in which having more tests demands greater
power as well. To wit, a larger test suite presumably exercises more
potential bugs, so our prior (before testing) state of belief about
the software places more weight on some covered bug being present.
Therefore, we may reasonably wish for more power in each individual
test to obtain a comparable post-test state of belief that none of the
covered bugs are present.</p>
<p>This phenomenon can be quantified in the simple (and also pessimistic)
case where we assume that all the tests cover completely disjoint
aspects of the software, so that any given bug of interest will only
affect the one test that covers it. Suppose we have <span class="math inline">\(100\)</span> potential
such bugs, and we assume each is independently present with a prior
probability of <span class="math inline">\(10\%\)</span>. If we run a single test with a false pass rate
of <span class="math inline">\(0.01\)</span> and a true pass rate of <span class="math inline">\(0.99\)</span> and it passes, the
posterior probability of the bug being absent is<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a><a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p><span class="math display">\[\begin{eqnarray*}
p(no\ bug\ |\ pass) &amp; = &amp; \frac{p(no\ bug)p(pass\ |\ no\ bug)}{p(pass)} \\
&amp; = &amp; \frac{0.9 \cdot 0.99}{0.1 \cdot 0.01 + 0.9 \cdot 0.99} \\
&amp; \approx &amp; 1 - 1.121 \cdot 10^{-3} \\
&amp; = &amp; 891:1 \odds \approx 29.5 \db \evid.
\end{eqnarray*}\]</span></p>
<p>Now suppose we run a test suite of <span class="math inline">\(100\)</span> tests, each covering exactly
one of our bugs, and they all pass. Then the posterior probability
of none of those bugs being present is</p>
<p><span class="math display">\[\begin{eqnarray*}
&amp; &amp; p(no\ bugs\ |\ 100\ passes) \\
&amp; = &amp; \frac{0.9^{100} 0.99^{100} }
   {\sum_{i=0}^{100} {100 \choose i} 0.1^i 0.9^{100 - i} 0.01^i 0.99^{100 - i}} \\
&amp; \approx &amp; 1 - 1.061 \cdot 10^{-1} \\
&amp; \approx &amp; 8.43:1 \odds \approx 9.26 \db \evid.
\end{eqnarray*}\]</span></p>
<p>In other words, the chances of a lurking bug are about <span class="math inline">\(100\)</span> times
higher. To obtain the same posterior on being bug-free that we had
before, we would need to increase the power of each individual test.
In this case, setting the false pass rate to <span class="math inline">\(10^{-4}\)</span> yields</p>
<p><span class="math display">\[\begin{eqnarray*}
p(no\ bugs\ |\ 100\ strict\ passes) &amp; \approx &amp; 1 - 1.122 \cdot 10^{-3} \\
&amp; \approx &amp; 890:1 \odds \\
&amp; \approx &amp; 29.5 \db \evid.
\end{eqnarray*}\]</span></p>
<p>With these numbers, about the same effect can be obtained by rerunning
the suite of <span class="math inline">\(100\)</span> tests three times (if it passes all three times).
Controlling the power of the individual tests, however, can often
yield the same effect with less computation.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In sum, I think Someone<sup>TM</sup> should write a statistical test
framework along the above lines for testing programs that are supposed
to exhibit stochastic behavior. I posit that the four-way interaction
between significance, effect size, power, and computational cost is
the next best thing to the unobtainable ideal of deterministic
testing.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> And it’s implementable, composable, and can
make use of known statistical analysis to improve performance. What
more can one ask?</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>Thanks to Taylor Campbell, Gregory Marton, and Ulrich Schaechtle for
commentary on a draft.</p>
<h2 id="references">References</h2>
<ul>
<li>A. Wald, “Sequential Tests of Statistical Hypotheses”,
Ann. Math. Statist. 16(2), 1945, pp. 117-186.
<a href="https://projecteuclid.org/euclid.aoms/1177731118">https://projecteuclid.org/euclid.aoms/1177731118</a></li>
</ul>
<h2 id="notes">Notes</h2>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      F: "{\\mathcal{B}}",
      P: "{\\mathcal{F}}",
      odds: "{\\ \\mathrm{odds}}",
      db: "{\\ \\mathrm{db}}",
      evid: "{\\ \\mathrm{evidence}}",
    },
    equationNumbers: { autoNumber: "AMS" },
    noErrors: { disabled: true },
  }
});
</script>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Of course, when testing real software, <span class="math inline">\(\F\)</span> could also
crash. Presumably, <span class="math inline">\(\F\)</span> will never crash if the procedure under test
is correct, so the test framework can signal failure immediately if
that happens. In the rest of the post I concern myself with
executions that could be consistent with correct behavior.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Readers who know that I generally tend to favor the
Bayesian approach to empirical reasoning may be wondering why I am
framing the testing problem in frequentist style. Indeed, why study
the worst-case (with respect to the true bad result rate of <span class="math inline">\(\F\)</span>)
false pass and false fail rates of the test suite (holding uncertainty
over the test results obtained)? Why not instead consider the
posterior (after running the tests) strength of belief that there is a
bug (given the results that were, in fact, obtained)?
A few answers: First, frequentist analysis aesthetically feels like a better fit, because
(for once!) we really are faced with a perfect infinitely repeatable
experiment. Second, I don’t actually have good priors ready to hand about how
likely various bugs of various severities may be. Third,
while the experimental results are obtained by the test runner,
they are effectively not obtained by the developer looking at
the “pass” or “fail” summary. Thus, when designing the test runner’s
policy, it makes sense to consider said developer’s uncertainty
over said results.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Except with the sense reversed: One speaks of a “very
significant” test, i.e. of “large significance”, if the probability of
false alarm is near zero.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The sense of power is also reversed: One speaks of a “very
high power” test if the probability of false pass is near zero.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Nitpick: The kinds of effects to which one big <span class="math inline">\(\chi^2\)</span>
test is sensitive are not exactly the same as the kinds of effects to
which checking that many small <span class="math inline">\(\chi^2\)</span> tests pass independently is
sensitive. I expect that, in the limit of infinite computation, both
styles will detect any deviation in the behavior of <span class="math inline">\(\P\)</span>, but they
will catch different bugs at any fixed significance and power. I don’t
think the difference is very important, and in any case, I expect it
to be swamped by the big test’s superior efficiency.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Even in circumstances lacking analytic results
about the power of some statistical test (I’m looking at you,
<a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov-Smirnov</a>),
it should be possible to assess a test’s power empirically, by
simulating instances of the expected “severe bug” situation and seeing
how often the test still passes. On the one hand, doing this to good
accuracy is likely to be rather computationally intensive, but on the
other hand, the result can be cached across test runs. It raises an
interesting theoretical wrinkle worth working out, namely the
semantics of a test-suite-level power guarantee if the power of one of
the constitutent tests is uncertain (but where the uncertainty admits
a known distribution, deduced from the empirical study).<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>The natural Bayesian procedure is an interesting
candidate here. To wit, start with some prior probability
distribution on <span class="math inline">\(\F\)</span>’s true probability of bad results, and some payoff
matrix for correctly and incorrectly passing or failing the overall
test, as well as some assumed cost of additional computation. At each
point, decide whether to continue by computing the expected value of
information from one more trial, and when done report the test as a
pass or a fail to maximize posterior expected payoff. To guarantee
termination, it seems this method still needs a notion of “effect
size” in its payoff matrix, to prevent it from oscillating forever if
the true bad result rate turns out to be at its decision boundary.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Had you given up on me being able to sneak Bayes’ Rule
into this post after all?<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>The “evidence” here is just the odds
measured in decibels: A proposition with <span class="math inline">\(X \odds\)</span> has <span class="math inline">\(10\log_{10}(X)
\db\)</span> evidence. Why do this? Two-exclusive-hypothesis posterior updating is
summation of evidence; and decibels seem to be a pretty intuitive unit
of measure for at least binary probabilities.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>And indeed, it recovers that ideal as a special case:
if <span class="math inline">\(p = 0\)</span> and <span class="math inline">\(q = 1\)</span>, one trial suffices for arbitrarily good
significance and power.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Fri, 29 Apr 2016 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2016/on-testing-probabilistic-programs/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Probabilistic Programming Habits</title>
    <link>https://alexey.radul.name/ideas/2016/probabilistic-programming-habits/index.html</link>
    <description><![CDATA[<p>Programming <a href="/ideas/2016/on-intentionally-random-programs">intentionally random
programs</a> presents its own special
software engineering considerations, in addition to the usual ones. I
have been surprisingly slow to realize this, but two years in to
working on a <a href="http://probcomp.csail.mit.edu/venture">probabilistic programming
platform</a> I can recommend some
specific habits around software engineering of intentionally random
programs.</p>
<p><strong>Reproducibility</strong>. Ironic as this may sound, my first suggestion for
how to write programs that are supposed to be random is to make them
deterministic. That is, put all the program’s randomness into the
initialization of the pseudo-random number generator (PRNG),
and don’t draw any more entropy from external sources.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Why?
Because then it’s easy to save that initial entropy and reuse it to
reproduce the program’s behavior: Regenerate exactly the same plot you
saw last time, exactly reproduce the conditions that caused that rare
crash your automated test failed with, etc.</p>
<p>Corollary: Manage your PRNG objects so that independent sections of
the program consume independent streams of random numbers. This is
starkest if the program is parallel, because then each thread
absolutely needs its own PRNG state, or else they will both slow down
due to contention and become unreproducible again due to reordering;
but the advice applies to any notionally separate serial operations as well.</p>
<p>However: Initialize any new PRNG states your program creates
internally from deterministic entropy sources, namely a PRNG available
at the point of creation. If you are worried about statistical
anomalies from PRNG algorithms that were not designed to support
parallel streams of random numbers, use a cryptographically strong
algorithm—either for the whole program, or at least to create the
necessary PRNG seeds.</p>
<p><strong>Plotting</strong>. Intentionally random programs are supposed to produce
probability distributions on outputs, and will tend to have
probability distributions on intermediate quantities as well. Your
eyeballs are <em>much</em> more efficient at extracting information about
aggregate behavior of distributions from plots than from printouts of
samples (even if the samples are the only thing the downstream
consumer of your program can use). Write any needed code for plotting
early and use it often.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p><strong>Crash Testing</strong>. The intentionally random programs I have encountered
tend to have several adjustable outer loops of the same kinds: a knob
for precision (i.e., number of independent samples drawn to push
variance down via the law of large numbers), a knob for accuracy
(i.e., number of steps in a Markov chain method, or number of trials
in an importance-weighted method, determining closeness of
approximation to the exact answer), and, for “machine learning” types
of tasks, a knob for number of training data points.</p>
<p>The habit here is to make enough of those knobs externally adjustable
to be able to do very fast sanity check runs: draw three samples from
two steps of inference on five training points projected to two
dimensions and get all the way to the end result of the program. The
goal is to be able to have a development cycle of (ideally) tenths of
seconds for flushing out coding mistakes that don’t depend on the
probability distribution of the intermediate computations. Have a version of your
automated test suite that does only such crash tests, for rapid
feedback on glaring errors (even in sections of the code that would
normally take a long time to reach in production-size runs).</p>
<p>Small, fast test cases are a good idea for all software development.
In this setting, they are not enough: such tests will not catch
mistakes that push the distribution on outputs enough to matter but
not enough to be noticeable with only two or three samples taken. For
me, it was a lesson of experience that crash tests are worth having
anyway.</p>
<p><strong>Statistical Testing</strong>. The goal of intentionally random programs is
to produce outputs that obey desired probability distributions, so
testing that they are working requires evaluating and comparing
probability distributions. For a distribution represented by a
program that purports to draw samples according to it, this task is
surprisingly non-trivial. Fortunately, this situation is exactly the
domain of classical (frequentist) statistics: runs of the program with
distinct initial entropy are identical independent experiments, that
can be repeated ad nauseam. Therefore, the null hypothesis that the
program does not have any mistakes can be tested with arbitrary
discrimination power (for the low price of consuming arbitrary
electrical power). Such test cases still require thinking to set up,
however: to find appropriate invariants-in-distribution to test, to
find appropriate statistical measurements for them,<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>
to test equalities that are only expected to hold approximately, and
to choose how to actually trade off computation for discrimination
power.</p>
<p><strong>Golden Output Testing</strong>. My experience of maintaining
intentionally random programs has been that one still spends plenty of
time working on refactorings that are expected to produce exactly no
effect on the random portion of the computation: exactly the same set
of pseudo-random numbers is still called for, they are to be combined
in exactly the same way, and should produce exactly the same answer.</p>
<p>For this scenario, I recommend making it easy to write test cases that
save the pre-refactoring program’s exact output as produced by a short
but end-to-end run with a given seed entropy, and compare it
bit-for-bit with output produced by the post-refactoring program with
the same inputs and seed entropy. This is not difficult to set up,
and fills an important niche: It will catch mistakes during such
refactorings more effectively than your crash tests, but use much less
computation than your statistical test suite.</p>
<p>A word of caution on this kind of “golden file” testing: You should
only be comparing differences in thusly saved outputs to find the
mistake that caused them. Edits that are expected to change the
program’s behavior at the level of which pseudo-random numbers are
generated should invalidate the affected golden files completely, and
be assessed by your crash tests, statistical tests, and functional
tests.</p>
<p><strong>Continuous Integration</strong>. Intentionally random programs provide an
opportunity for a new sense of “continous integration”. In
traditional software engineering, that phrase means having an
always-on computer somewhere that gets notifications from the version
control system and reruns the test suite on every commit (or as
frequently as practicable). The purpose is to provide developers with
rapid feedback on the status of the build and test suite, without
requiring them to use their personal workstations to run it all the
time.</p>
<p>When developing intentionally random programs, the opportunity to gain
insight from computation is unlimited, even if the program is not
changing very rapidly: one can always run one’s statistical tests with
more samples (and therefore higher discrimination power), or repeat
one’s tests with different initial entropy. I do not, however, yet
have tools or best practices to recommend on this point. Besides just
implementation effort, the challenge seems to be to find a good
balance between invalidating runs performed against old versions of
the program and reusing computation across changes that did not affect
the component or behavior under test.</p>
<h2 id="notes">Notes</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>And also squash all the traditional sources of
non-determinism that also apply to programs that aren’t supposed to be
random: order-sensitive traversals of hash tables, race conditions in
parallel programs, etc.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>I am not yet satisfied with the available standard plot
types. Eyeballing histograms is better than nothing, but I expect
that any given project will want <a href="https://en.wikipedia.org/wiki/P%E2%80%93P_plot">probability-probability
plots</a> and
<a href="https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot">quantile-quantile
plots</a> (<a href="https://v8doc.sas.com/sashtml/qc/chap8/sect9.htm">what’s the
difference?</a>) for
comparing similarity of 1-D distributions, as well as domain-specific
visualizations of uncertainty in structured objects.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>My go-to tests now are <a href="https://en.wikipedia.org/wiki/Pearson&#39;s_chi-squared_test">Pearson chi-square</a>
or the <a href="https://en.wikipedia.org/wiki/G-test">G-test</a> for small-domain
discrete distributions,
<a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov-Smirnov</a>
for one-dimensional continuous distributions, and ad-hoc reductions to
the above cases for others (binning, projection).<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Mon, 21 Mar 2016 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2016/probabilistic-programming-habits/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title><em>On</em> Intentionally Random Programs</title>
    <link>https://alexey.radul.name/ideas/2016/on-intentionally-random-programs/index.html</link>
    <description><![CDATA[<p>For a little over two years, I have been professionally dealing with
programs whose behavior is intentionally random. Why would one even
have intentionally random programs?</p>
<p>The first kind of “random” program I encountered was the <strong>randomized
algorithm</strong>, but I do not consider these to be intentially random in
this sense. Such a program’s purpose is to compute something
deterministic, and randomness is a trick employed to be sure the
algorithm avoids all possible bad patterns.</p>
<p>One classic example of this is the <a href="https://en.wikipedia.org/wiki/Fermat_primality_test">Fermat primality
test</a>. The goal
is to determine whether some large <span class="math inline">\(n\)</span> is prime, which is not a random
thing at all. The method is to try various <span class="math inline">\(a\)</span> to see whether
<span class="math inline">\(a^{n-1} \not \equiv 1 \mod n\)</span>—if at least one such witness is
found, <span class="math inline">\(n\)</span> is certainly composite, and if not, <span class="math inline">\(n\)</span> is “probably”
prime. The randomization helps in the following way: if <span class="math inline">\(n\)</span> is
composite (and not a Carmichael number, which are much less
common than primes), then at least half of the possible <span class="math inline">\(1 &lt; a &lt; n-1\)</span> will
be witnesses, but there is no theory indicating which ones they will
be. So we cannot prove that any particular search pattern will quickly
find a witness, but we can prove that choosing candidates at random
will quickly find a witness with high probability.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>The second kind of “random” program I encountered is the creation of a
<strong>cryptographic secret</strong> (such as a large semiprime for use in the RSA
algorithm). I guess key generation really is “intentionally” random,
but it nonetheless has a different flavor from the kinds of
intentionally random programs I have been working with. The output is
a single sample from the probability distribution on secrets. The
name of the game is to prevent adversaries from guessing the output,
to which end the distribution is engineered to be as high entropy as
possible, with as little other structure as possible.</p>
<p>What I have been working with, however,
are programs that are intended to produce interestingly random results—programs whose
<strong>probability distribution on results is itself the design
objective</strong>, and subject to arbitrary domain-specific desiderata. One
nice source of examples is
<a href="https://stanford.edu/~dritchie/procmod-smc.pdf">recent</a>
<a href="https://stanford.edu/~dritchie/graphics-hmc.pdf">work</a> in “procedural
modeling”: teaching computers to come up with (random) suggestions
for designs, for example for computer generated graphics in movies or
games. This business gets tricky when the artistic objective has
requirements that are unlikely to be met purely at random: trees that
grow around obstacles, networks of pipes that cast a specific shadow,
space ships or cities that match a given overall shape.</p>
<p>In probabilistic procedural modeling, the goals of randomization are
very different from randomized algorithms or cryptography: the output
is several different instances of random objects of the same
specification; each is expected to more or less meet given artistic
constraints; and they should otherwise be as varied (in a domain-specific sense)
as practicable, to
provide appropriate options for selection or an appropriate base for
inspiration.</p>
<p>Another, more staid, source of examples is the field of Bayesian
statistics, such as the <a href="https://esa.un.org/unpd/wpp/publications/Files/WPP2012_Methodology.pdf">recently adopted United Nations methodology
for projecting human
populations</a>.
In this case, the output of the intentionally random component is
thousands (or millions, for all I know) of candidate population
trajectories. These trajectories are used to form the published
predictions, and the published error bars on those predictions.</p>
<p>The underlying population model uses probability theory to quantify at
least two different kinds of uncertainty—uncertainty about general
properties of population growth, which is mitigated but not eliminated
by calibrating the model on historical population data, and
uncertainty about how population will actually grow in whatever
(uncertain) specific circumstances arise in the future. The design
requirement for the computer program that realizes this model is for
the probability distribution of the output trajectories to faithfully
represent the model’s residual uncertainty about what will actually
happen.</p>
<p>Why is the United Nations’ population program random? The end-to-end
process, from gathering population data to publishing graphs and data
tables with projections, might be called “unavoidably random”. No
doubt the UN Population Division would prefer to be able to
deterministically compute what numbers to put into the Population
Prospects report to faithfully represent the uncertainty quantified by
their model. This is even theoretically possible—the 80th
percentile value in their model’s probability distribution over world
population in 2057 is some mathematically defined real number. The
trouble is that this number is defined in terms of horrible
multi-dimensional integrals over all possible unknowns in the model.
For any but the simplest models, these integrals cannot be determined
analytically, and are not tractable to accurately determine by
numerical integration. The best the UN can do is draw many random
samples from (an approximation to) the model’s probability
distribution on trajectories and describe the aggregate, relying on
the law of large numbers to reduce the residual randomness in their
final report to an acceptable level.</p>
<p>The last two examples motivate what <strong>probabilistic programming</strong> is for.
Both procedural graphics and population projections (and hosts of
other applications I haven’t mentioned) have components that are
random programs manipulating complex objects, whose space of possible
behavior is the design criterion of interest. Even when there is a
“final” output whose randomness is either undesirable (such as the
population report) or irrelevant (such as the details of a
good-looking computer-generated snowstorm), the program that produces
it contains internal interfaces where correctness is defined by the
probability distribution on returned samples. <a href="http://probabilistic-programming.org/wiki/Home">Probabilistic
programming languages</a>
are for writing such programs (more concisely
and with fewer errors than otherwise), and probabilistic software
engineering needs to be about inspecting, testing, debugging,
optimizing, and maintaining such programs.</p>
<h2 id="references">References</h2>
<ul>
<li><p>Daniel Ritchie, Ben Mildenhall, Noah D. Goodman, and Pat Hanrahan,
“Controlling Procedural Modeling Programs with
Stochastically-Ordered Sequential Monte Carlo”, SIGGRAPH 2015.
Preprint:
<a href="https://stanford.edu/~dritchie/procmod-smc.pdf">https://stanford.edu/~dritchie/procmod-smc.pdf</a></p></li>
<li><p>Daniel Ritchie, Sharon Lin, Noah D. Goodman, and Pat Hanrahan,
“Generating Design Suggestions under Tight Constraints with
Gradient-based Probabilistic Programming”, Eurographics 2015.
Preprint:
<a href="https://stanford.edu/~dritchie/graphics-hmc.pdf">https://stanford.edu/~dritchie/graphics-hmc.pdf</a></p></li>
<li><p>United Nations, Department of Economic and Social Affairs,
Population Division (2014). “World Population Prospects: The 2012
Revision, Methodology of the United Nations Population Estimates
and Projections”, Working Paper No. ESA/P/WP.235.
<a href="https://esa.un.org/unpd/wpp/publications/Files/WPP2012_Methodology.pdf">https://esa.un.org/unpd/wpp/publications/Files/WPP2012_Methodology.pdf</a></p></li>
</ul>
<h2 id="notes">Notes</h2>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>In practice, of course, the “random” choices of <span class="math inline">\(a\)</span>
are generally pseudo-random, in the sense of being generated from a
small amount of physical entropy amplified by a pseudo-random number
generator (which is a deterministic program designed to be difficult
to reverse-engineer) into a large enough stream of bits to form the
desired <span class="math inline">\(a\)</span>s. In this case, any particular initial seed for the PRNG
does define a specific deterministic search pattern; the argument that
this is nonetheless acceptable turns into one about the PRNG not being
accidentally ill-aligned with the primality testing problem.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Sun, 20 Mar 2016 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2016/on-intentionally-random-programs/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title><em>On</em> Good Software</title>
    <link>https://alexey.radul.name/ideas/2015/on-good-software/index.html</link>
    <description><![CDATA[<p>Good software is software that admits a simple mental model.</p>
<p>For all that I have observed and participated in plenty of discussions
about one or another piece of software as to whether it is or is not
good, I am surprised to say that I have never seen a single defining
principle clearly articulated. Permit me, therefore, to propose this
one:</p>
<blockquote>
<p>Good software is software that admits a simple mental model.</p>
</blockquote>
<p>Or, more precisely, software is better inasmuch as the
simplicity-accuracy curve for models of it is more favorable.</p>
<p>Let us consider some generally avowed qualities of good software and
see how they relate to mental models.</p>
<ul>
<li><p>“Good software works.” One can view this desideratum as separate
from any talk about mental models, and I would not argue very hard
with such a one. But one can also view this desideratum as saying
that good software is appropriate to model as
“something that performs the task”—which is generally simpler
than a detailed understanding of how that task is performed.</p></li>
<li><p>“Good software is robust [to unusual conditions].” This desideratum
is about not needing to decorate one’s mental model of good software
with various “if”s, “and”s, or “but”s about situations where it
behaves strangely.</p></li>
<li><p>“Good software is maintainable.” What is software maintenance?
Adjusting it in light of new desiderata or clarified understanding
of existing desiderata. For this to be easy, the developer doing it
must have an accurate mental model of the software being maintained,
and of what will happen under various possible changes.</p></li>
<li><p>“Good software is reusable.” That is, the people in a position to
reuse it have clear enough and good enough mental models of the
software to reuse it. Reusability also has to do with generality,
which is also the same sort of thing: General things are simple
things, because complexity only arises when dealing with specifics.</p></li>
<li><p>“Good software is extensible.” This one is about decomposing the
mental model of the task into additive capabilities, and making the
structure of the software follow that decomposition in such a way
that additional capabilities correspond directly to additional
components. And such that those components have simple yet adequate
interfaces to the rest of the system.</p></li>
<li><p>“Good software is debuggable.” Meaning that it produces sufficient
information (e.g., logs) about how it operates that a developer can
quickly isolate and repair any problem that arises. This is also
about mental models: good software should provide the information needed to
amplify the baseline mental model the developer has into a
complete explanation of the (presumably undesirable) phenomenon they
are trying to investigate at any particular point.</p></li>
<li><p>“Good software is secure.” This desideratum can also be viewed as
being about matching mental models: pretty much all security
violations are abuses that a developer or user implicitly assumes
are not possible. The thing an exploit exploits is a discrepancy
between how a given piece (or collection) of software actually
operates and the meanings and implicit rules its legitimate users
ascribe to it.</p></li>
<li><p>“Good software is modular.” That is, it is broken down into parts
in such a way that it can profitably be modeled by modeling those
parts individually and observing that their composition is fairly
simple.</p></li>
<li><p>“Good software is testable.” That is, it should be easy to compare
one’s mental model of the software (or some part of it) against the
reality.</p></li>
<li><p>“Good software is efficient.” In one sense, calling this one as
being about mental models is a bit of a stretch; but in another,
it’s also about expectation matching: the software does not take
appreciably more resources to perform its task than one would
assume.</p></li>
<li><p>“Good software is scalable.” That is, it’s easy and favorable
enough to get it to do more work by throwing more hardware at it.
This is also about mental models: good software does not impede the
performance of its task in a larger computational environment; or,
alternately, is able to take advantage of the computational
resources one would think it should.</p></li>
<li><p>“Good software is learnable.” That is, the simplicity/accuracy
curve of mental models of the software is not too difficult to
traverse towards greater accuracy.</p></li>
<li><p>“Good software has few bugs.” What is a bug? It is a discrepancy
between a mental model and the object being modeled. Usually bugs
arise because the model of composing some parts is “they work
together smoothly” even when they do not; or, viewed another way,
because the model of the rest of the world from the point of view of
one component is not properly met by another.</p></li>
<li><p>“Good software is compatible [with the surrounding ecosystem].”
This is again about the accuracy of an implicit mental model, namely
that starting to use a piece of software will smoothly add a
capability to one’s workflow/life, and interoperate flawlessly
with everything else one is already using. If only users would
appreciate how much work this is!</p></li>
</ul>
<p>Which brings me to a critical corollary of my proposed principle:
since simplicity is relative (to what one has already learned and
internalized), the quality of software is also relative. In my own
experience, examples abound: I happen not to think an application is
any good unless it runs on GNU/Linux, though the majority seems to
disagree; many programmers dismiss languages from the Lisp family as
complex, because they learned infix rather than prefix notation in
school; I think Emacs is a great text editor, but maybe that’s just
because my fingers don’t know any other sets of keybindings.</p>
<blockquote>
<p>Software is good in your eyes if you can easily form a good mental
model of it.</p>
</blockquote>
<h2 id="notes">Notes</h2>
<p>Perhaps the relativity of the goodness of software is at the root of
the classic distinction between the software design styles that have
been <a href="https://www.dreamsongs.com/RiseOfWorseIsBetter.html">named</a>
the New Jersey and MIT schools. Where the latter strives for
software that is well modeled directly, regardless of the underlying
platform, the former strives for software that is well modeled as a
simple implementation on top of an underlying platform that is
assumed to be worth understanding regardless.</p>]]></description>
    <pubDate>Sun, 21 Jun 2015 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2015/on-good-software/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Best Effort <em>vs</em> Strictly Safe</title>
    <link>https://alexey.radul.name/ideas/2015/best-effort-vs-strictly-safe/index.html</link>
    <description><![CDATA[<p>Possibly the greatest tension in the design of programming languages
occurs when encountering a user program that doesn’t quite make sense.
The two coherent schools of thought on the subject are</p>
<ol type="a">
<li><p>The platform should make its <em>best effort</em> to continue the
computation—choose reasonable rules for resolving potential
ambiguities, so as to avoid crashing the program until absolutely
necessary.</p></li>
<li><p>The platform should be <em>strictly safe</em>—detect potential
ambiguities as early as possible and force the programmer to
clarify until crash-free execution can be guaranteed.</p></li>
</ol>
<p>For example, what should a programming language do when the user
writes a program that might attempt to add a number to a string?
Haskell will give a type error when compiling the program, forcing the
programmer to adjust their code (possibly by inserting an explicit
conversion).<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Common Lisp will run the program, and
raise an exception if the program actually does attempt to add a
number to a string.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> Perl will parse the string as a number
and proceed (some would say “blithely”).<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>Which way is best? Disciples of the best effort school point out
that such systems tend to be more robust, continuing to work (somehow)
in unanticipated situations (including tolerating some bugs in their
own programming). Disciples of the strictly safe school point out
that such systems tend to have fewer mistakes and more predictable
behavior. Which kinds of programs are easier to modify and to build
upon? The debate fills volumes.</p>
<p>Let’s think about basic data types—algebraic data types in Haskell,
versus records in various Lisp dialects. One major mental difference
between Haskell and Lisp, that seems to trip up people learning their
second of those, is that Haskell always requires explicit
conversions between different algebraic data types, even if they look
similar; whereas Lisp is perfectly happy to plug together any producer
with any consumer, and as long as the producer only happens to make
records of types the consumer can handle, everything will be fine.
<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>There is an obvious cost to the Haskell way, which is needing to
write code like</p>
<pre><code>foo_to_bar :: Foo -&gt; Bar
foo_to_bar Foo1 = Bar1
foo_to_bar Foo2 = Bar2</code></pre>
<p>and use it all over the place. This is tedious, but not particularly
error-prone, because the type system helps you get it right.</p>
<p>On the other hand, there is a major benefit to the Haskell way: If all
conversions and injections are explicit, then it is possible to
statically determine what type everything should have, which quickly
catches whole slews of common and irritating errors. (And also serves as
compiler-checked documentation of program structure).</p>
<p>This benefit has a deep version, too: if in Haskell one tries to put a
<code>Foo'</code> into a container meant to hold only <code>Foo</code>, one will quickly get an
error message that points both to the guilty insertion and to the
point in the code that requires it to be a container of <code>Foo</code>. In Lisp,
in contrast, the insertion will proceed unimpeded, and the runtime
error message will point to the program fragment that expected the
<code>Foo</code>, implicitly blaming it for being unable to deal with the <code>Foo'</code>.
This leads to bugs that are difficult to track down, because the
actual mistake occurred long before and far away from the time and
place where it was detected.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>For a while I thought the above was the whole story, but on further
reflection I see a deep advantage to the Lisp way, too. The advantage
relates to programming in the large—composing compound systems from
parts developed by independent organizations, which themselves are
composed from parts developed by independent organizations.</p>
<p>Consider: you are writing some package that uses version 1 of some
library, and the library exposes a very useful function. OK, fine, no
problem. Package written, function called, everything works great.
Now suppose after a little while, the author of the library releases
version 2, which generalizes the original function by accepting a
broader range of inputs. If this is happening in Lisp, your package
can link against version 2 <em>without modification</em>—the necessary
conversion from the constrained calling convention you are using to
the more general one the function now accepts happens implicitly, so
that your package will just work on a system that has version 2
installed.</p>
<p>On the other hand, in Haskell, the author probably had to change the
type of the input you need to pass to the library function.
So now you have to write that <code>foo_to_bar</code>
function and add a call to it in order to work with version 2 of the
library. And doing so prevents you from working with version 1.
Oops. This is a problem even if you wrote the package for your own
personal use, but it turns into a real pain point if there are more
users. Suppose someone wants to use your package, and also some other
package that also uses the library under discussion. In Lisp,
everything is fine: your package works with whatever version this
third user has. In Haskell, either the package management system has
to deal with multiple versions of the library coexisting, and with
linking each package against the version it wants (which it doesn’t,
though rumor has it that they are working to fix that),
or you and the author of the other package have to agree on which
version of the library you are supporting. The latter is practically
impossible, because the obligation to agree was not even a choice
either of you made—it was imposed on you by a third party. So it
doesn’t happen, and the poor user cannot use your two packages
together.</p>
<p>The Haskell afficionado in the audience will doubtless point out many
ways the author of that library could have avoided having this
problem. Yes, growth can be made smoother if one anticipates it. The
point is that a strictly-safe style tends to increase coordination
burdens, which in turn tends to impede unanticipated growth, when
no one person controls the entire code base.</p>
<p>Of course, the situation I just described can happen in any
programming environment; and does—so often that it has a
<a href="https://en.wikipedia.org/wiki/Dependency_hell">name</a>. A best-effort
style of trying to make things work out when possible reduces the
incidence of this problem, but doesn’t actually solve it. Does that
reduction change the quality of programmers’ and library authors’
lives? Do library ecosystems form and grow differently on best-effort
as opposed to strictly-safe platforms?</p>
<h2 id="notes">Notes</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>As of version 7.6, the Glasgow Haskell Compiler
has the <code>-fdefer-type-errors</code> flag, which does allow the programmer to
try to execute a program with such an inconsistency. However, that
type error will cause a crash if the offending code path is entered,
even if it would not have gone though with the dubious addition.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The programmer can define a handler for such conditions,
which gives a variety of options for proceeding, including substituting
some value for the result of the addition and continuing from there.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>If the whole string doesn’t parse, Perl will take the longest
prefix that does. If no prefix of the string is a number at all, Perl
will interpret the empty prefix as denoting 0.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>In ADT-language, one can say that Lisp records are
product types, and describe Lisp as synthesizing sum types on the fly.
The thing that’s interesting is that Lisp also supplies implicit
(partial) conversions between (synthesized) sum types with factors of
the same names.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>The one exception to this rule among Lisps that I know of
is the contract and blame system in Racket, which to my knowledge does
let one write generic container data structures that can be equipped
with use-site contracts restricting the things that can be added to
them, and laying blame at both the producer and the consumer of an
object if they disagree about the object’s invariants.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Sat, 23 May 2015 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2015/best-effort-vs-strictly-safe/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>How <em>to</em> Compute <em>with a</em> Probability Distribution</title>
    <link>https://alexey.radul.name/ideas/2015/how-to-compute-with-a-probability-distribution/index.html</link>
    <description><![CDATA[<p>What makes a good representation for computing with probability
distributions? The two canonical options are samplers and probability
density functions. Both are valuable; and the relationship between
them turns out to hide two fruitful variations on the idea of a
sampler, that I will call “importanter” and “rejecter”.
The purpose of this essay is to carefully study these four
objects and the interrelations between them, and the light they shed
on the ubiquitous rejection sampling and importance sampling algorithms.</p>
<p>The probabilistic programming system
<a href="http://probcomp.csail.mit.edu/venture/">Venture</a> that I am working on
makes heavy use of the idea that a (computable) probability
distribution is very effectively represented by a stochastic machine
that computes samples from that distribution. A couple months ago I
had the privilege of discussing this topic with <a href="https://twitter.com/ccshan/">Ken
Shan</a>, which conversation
caused me to think through these foundational relationships. Any
elegance in the result is due entirely to Ken; the mistakes are of
course my own.</p>
<p>To make the content computationally concrete, I will spell out what I
am saying in pseudo-Haskell as well as English. Why Haskell? Because
its type system is rich enough to capture the interesting structure
very well. In fact, don’t read the code—read the type signatures.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<h2 id="contents">Contents</h2>
<ul>
<li><a href="#sampling">Sampling</a>
<ul>
<li><a href="#expectation">Expectation</a></li>
<li><a href="#measures">Measures</a></li>
<li><a href="#operations-on-samplers">Operations on Samplers</a></li>
</ul></li>
<li><a href="#densities">Densities</a>
<ul>
<li><a href="#composition-of-densities">Composition of densities</a></li>
</ul></li>
<li><a href="#importance-weighting">Importance Weighting</a>
<ul>
<li><a href="#measuring-importance">Measuring Importance</a></li>
<li><a href="#composition-of-importanters">Composition of Importanters</a></li>
<li><a href="#weighted-expectations">Weighted Expectations</a></li>
<li><a href="#importance-sampling">Importance Sampling</a></li>
<li><a href="#proposals">Proposals</a></li>
<li><a href="#resampling">Resampling</a></li>
</ul></li>
<li><a href="#rejection">Rejection</a>
<ul>
<li><a href="#measuring-rejection">Measuring Rejection</a></li>
</ul></li>
<li><a href="#relationship">Relationship</a></li>
<li><a href="#exchangeable-coupling">Exchangeable Coupling</a></li>
<li><a href="#notes">Notes</a></li>
</ul>
<h2 id="sampling">Sampling</h2>
<blockquote class="pullquote-display">
<p>
A sampler is a machine that represents a probability distribution by its behavior.
</p>
</blockquote>
<p>Let us start our exploration of representations of probability
distributions with the (exact) sampler. A <em>sampler</em> is a machine that
represents a probability distribution by its (random)
behavior:<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<pre><code>type Sampler a  -- A source of random as.</code></pre>
<p>I elide questions of the entropy source; think an infinite stream of
uniformly random bits in the sky. Since I like computation, I will
also require the samplers I think about to terminate with probability
1. A “good” sampler is one that delivers its samples quickly—with
little computation.</p>
<p>Samplers support the following three natural operations (which I will
name by their conventional names):</p>
<pre><code>return :: a -&gt; Sampler a
return x = &quot;sample&quot; by always emitting x, consuming no randomness

fmap :: (a -&gt; b) -&gt; Sampler a -&gt; Sampler b
fmap f sx = sample a b by sampling an a and then calling f on it

join :: Sampler (Sampler a) -&gt; Sampler a
join ssx = sample a (Sampler a) from the input,
           then sample an a from that</code></pre>
<p>These operations make samplers composable.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>Exercise: Prove that these definitions for <code>return</code>, <code>fmap</code>, and
<code>join</code> satisfy the <a href="https://en.wikipedia.org/wiki/Monad_%28functional_programming%29#fmap_and_join">Monad
laws</a>
and conclude that samplers form a monad. If you don’t know what I’m
talking about when I say “monad”, don’t worry—for the purposes of
this post, a sufficient intuition is that composing samplers is
“well-behaved”.</p>
<p>Note that there is no performance penalty for composition: the cost of
sampling from the output is a direct consequence of the costs of
running the inputs, without significant overhead.</p>
<h3 id="expectation">Expectation</h3>
<p>If we have a sampler over <span class="math inline">\(\R\)</span> (actually, any vector space, but <span class="math inline">\(\R\)</span>
will do), we can form finite estimates of the expectation (which
estimates are themselves random) by computing some samples and
averaging. In types and code that looks like</p>
<pre><code>finite_expectation :: Int -&gt; Sampler R -&gt; Sampler R
finite_expectation n sx = fmap average (replicateM n sx)</code></pre>
<p>Theorem (<a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">Law of Large Numbers</a>):
For any sampler <code>sx :: Sampler R</code>, as <span class="math inline">\(n \to \infty\)</span>, the finite
expectations <code>finite_expectation n sx</code> converge (as distributions) to
<code>return mean_sx :: Sampler R</code> for some number <code>mean_sx</code>.</p>
<p>The constant <code>mean_sx</code> is of course the expected value of the
distribution given by the sampler <code>sx</code>. The law of large numbers
gives us licence to call it <em>the</em> expected value of <code>sx</code>. By abuse
of notation, we can write that idea down as (well-typed!) pseudo-code:</p>
<pre><code>expect :: Sampler R -&gt; R
expect sx = mean_sx where
  return mean_sx = infinite_limit (\n -&gt; finite_expectation n sx)</code></pre>
<h3 id="measures">Measures</h3>
<p>Once we have the idea of expectations, a sampler for any type <code>a</code> gives rise to a
<a href="https://en.wikipedia.org/wiki/Measure_%28mathematics%29">measure</a> over
the set <span class="math inline">\(A\)</span> of all objects of type <code>a</code>. This links our computational objects
to the standard mathematical foundations of probability theory.</p>
<p>Intuitively, the size of a subset under this measure is the
probability that the sampler produces an object from that subset. Formally,
for <code>sx :: Sampler a</code>, and any subset <span class="math inline">\(S \subset A\)</span> given by an
indicator function <span class="math inline">\(f_S : A \to \{0,1\}\)</span>, we can define <span class="math inline">\(\mu(S)\)</span> as
the expected value of composing the sampler with the indicator function:</p>
<p><span class="math display">\[ \mu(S) = \texttt{expect (fmap f sx)}. \]</span></p>
<p>Exercise: Prove that <span class="math inline">\(\mu\)</span> given by the above definition is a
<a href="https://en.wikipedia.org/wiki/Probability_measure">probability measure</a>.</p>
<p>Exercise: Prove that integration with respect to <span class="math inline">\(\mu\)</span> is expectation
under the sampler:</p>
<p><span class="math display">\[ \forall (c:A \to \R), \int c d\mu = \texttt{expect (fmap c sx)}. \]</span></p>
<p>Insofar as probability measures are accepted as a reasonable
definition of “what a probability distribution is”, the two above
facts mean that a sampler can validly be said to represent
the probability distribution on its outputs.</p>
<h3 id="operations-on-samplers">Operations <em>on</em> Samplers</h3>
<p>How well do samplers implement the operations we like to perform on
probability distributions?</p>
<ul>
<li><p>Joint distributions: If the sampler <code>sx :: Sampler a</code> represents the
distribution <span class="math inline">\(p(a)\)</span>, and the function <code>f :: a -&gt; Sampler b</code>
represents the conditional distribution <span class="math inline">\(p(b|a)\)</span>, then getting a
sample from the joint distribution consists of drawing a sample from
<code>sx</code>, applying <code>f</code> to get a sampler for <code>b</code>s, drawing a sample from
that, and emitting the pair:</p>
<pre><code>joint :: Sampler a -&gt; (a -&gt; Sampler b) -&gt; Sampler (a, b)
joint sx f = do
  a &lt;- sx
  b &lt;- f a
  return (a, b)</code></pre></li>
<li><p>Marginal distributions: If we have a sampler that represents the
probability distribution <span class="math inline">\(p(a,b)\)</span>, then drawing a sample from the
marginal distribution <span class="math inline">\(p(a)\)</span> is just drawing a sample from the joint
and throwing away the unneeded component:</p>
<pre><code>marginal :: Sampler (a, b) -&gt; Sampler a
marginal = fmap fst</code></pre></li>
<li><p>Conditional distributions: However, a sampler for <span class="math inline">\(p(a,b)\)</span> does not
easily lend itself to a sampler for the conditional <span class="math inline">\(p(b|a=x)\)</span>.
Conditional distributions are hard.</p>
<pre><code>conditional :: Sampler (a, b) -&gt; a -&gt; Sampler b
conditional = ???</code></pre></li>
</ul>
<p>Exercise: Prove that the above two constructions actually work, namely
that <code>joint sx f</code> represents the correct joint probability measure and
<code>marginal sxy</code> represents the correct marginal probability measure.</p>
<p>The lack of natural samplers for conditioning is unfortunate, because
conditioning is an important operation. Conditioning is arguably
<em>the</em> operation that gives probability theory its practical
significance, since it is the operation that poses the causal
inference problems we need probability for: “given a cause-and-effect
model <span class="math inline">\(p(a,b)\)</span>, and given that effect <span class="math inline">\(A\)</span> happened, what causes <span class="math inline">\(B\)</span>
for it are probable?”</p>
<h2 id="densities">Densities</h2>
<p>The other common representation for probability distributions is the
density function<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. A <em>density function</em> (or just density
for short) is a way to evaluate how “dense” a probability distribution
(measure) is at some particular value. Such an evaluation is perforce
relative to some other measure, which is taken to represent our notion
of “uniformly dense” (even though it can really be pretty much any
measure on the same space).</p>
<pre><code>type Density a = a -&gt; R  -- positive only; the base measure is implicit</code></pre>
<p>Given a base measure <span class="math inline">\(\mu\)</span> on <code>a</code>, a density <code>d :: Density a</code>
defines a measure <span class="math inline">\(\mu_d\)</span> by</p>
<p><span class="math display">\[ \mu_d(S \subset A) = \int_S d d\mu. \]</span></p>
<p>Exercise: Prove that the integration rule for <span class="math inline">\(\mu_d\)</span> is</p>
<p><span class="math display">\[ \forall (c:a \to \R), \quad \int c d\mu_d = \int d \cdot c d\mu, \]</span>
where the multiplication on the right hand side is taken pointwise.
This is the expected value of the function <span class="math inline">\(c\)</span> under the probability
distribution given by <span class="math inline">\(d\)</span>.</p>
<p>Corollary: If <span class="math inline">\(\mu\)</span> is a probability measure, then <span class="math inline">\(\mu_d\)</span> is also,
provided <span class="math inline">\(\int_A d d\mu = 1\)</span>.</p>
<p>Exercise: Prove that <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\mu_d\)</span> determine the density function
uniquely (up to the usual caveats of continuous analysis):</p>
<p><span class="math display">\[ d(a) = \lim_{\mu(S) \to 0} \frac{\mu_d(S)}{\mu(S)} \qquad \textrm{for } a \in S \subset A.  \]</span>
Not all pairs <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\mu_d\)</span> give rise to finite density functions
<span class="math inline">\(d\)</span>, but exploring that topic would take us too far afield.</p>
<blockquote class="pullquote-display">
<p>
Both a density and a sampler give a probability distribution, but
they offer operationally different information about it.
</p>
</blockquote>
<p>Both a <code>Density a</code> and a <code>Sampler a</code> give a measure on <code>a</code>, but they
offer operationally different information about it. The sampler gives
a computational mechanism for drawing examples, the distribution of
which obeys the measure. The density function gives a computational
mechanism for evaluating any given object under the measure.
Recovering either of these operations from the other requires
integration (either with respect to the base measure of the density or
the measure given by the sampler), which cannot in general be done
cheaply and exactly.</p>
<p>Consequently, it can be useful to carry both a sampler and a density
for the same measure:</p>
<pre><code>type Dist a = (Sampler a, Density a)  -- for the same measure
  -- the base measure of the density is implicit</code></pre>
<p>Well-studied probability distributions typically have both efficient
samplers and efficient density functions, hence the name <code>Dist</code> for
the type.</p>
<h3 id="composition-of-densities">Composition <em>of</em> Densities</h3>
<p>Densities also technically obey the monad laws, but only if one is
willing to take integrals (or sums in the discrete case). Since
integrals are awkward to express in code, I will record them in math.
Also, the integrals make more sense at the level of measures; the
actual densities can be derived as limits thereof as usual.</p>
<pre><code>return :: a -&gt; Density a
fmap :: (a -&gt; b) -&gt; Density a -&gt; Density b
join :: Density (Density a) -&gt; Density a</code></pre>
<p><span class="math display">\[ \begin{eqnarray*}
 \mu_{\texttt{return x}}(S \subset A) &amp; = &amp; \begin{cases} 1 \textrm{ if } \texttt{x} \in S \\ 0 \textrm{ otherwise} \end{cases}, \\
 \mu_{\texttt{fmap f d}}(S \subset B) &amp; = &amp; \int_{f^{-1}(S)} 1\ d\mu_{\texttt{d}}, \\
 \mu_{\texttt{join dd}}(S \subset A) &amp; = &amp; \int_{\textrm{densities } d \textrm{ on } A} \left( \int_S 1 d \mu_d\right) d \mu_{\textrm{dd}}.
\end{eqnarray*} \]</span></p>
<p>Exercise: Prove that the above operations are well-formed, that is
that if the arguments represent probability distributions then the
results do too.</p>
<p>Exercise: Prove that densities obey the monad laws with the above
operations.</p>
<p>How well do densities perform the operations we like on probability
distributions?</p>
<ul>
<li><p>Joint distributions: If <span class="math inline">\(p(a)\)</span> is represented by a density, and <span class="math inline">\(p(b|a)\)</span>
is represented by a function from <code>a</code> to a density, then the joint
distribution <span class="math inline">\(p(a, b)\)</span> is represented by the product:</p>
<pre><code>joint :: Density a -&gt; (a -&gt; Density b) -&gt; Density (a, b)
joint dx fxdy (x,y) = dx x * fxdy x y</code></pre></li>
<li><p>Marginal distributions: Marginal distributions are actually the
place where the integrals in the monad laws come from. If
<span class="math inline">\(p(a,b)\)</span> is represented by a density, then to compute the density
of <span class="math inline">\(p(a)\)</span> it is necessary to integrate the density function over
all possible <span class="math inline">\(b\)</span> (with respect to the projection of the base
measure):</p>
<pre><code>marginal :: Density (a, b) -&gt; Density a
marginal dxy x = -- integral over y of dxy (x,y)</code></pre></li>
<li><p>Conditional distributions: Conditional distributions are the place
where densities show their true worth—taking conditional densities
is just currying:</p>
<pre><code>conditional :: Density (a, b) -&gt; a -&gt; Density b
conditional dxy x y = dxy (x,y)  -- unnormalized</code></pre></li>
</ul>
<p>There is actually an important subtlety in the last of these, which is
that the result <code>conditional dxy x</code> will not integrate to 1 over <code>y</code>
unless we scale it by the appropriate integral. This is unfortunate,
because that integral is all too often intractable, but even an
unnormalized density is better than nothing.</p>
<p>Exercise: Prove that these formulas are correct, namely that the
results of <code>joint</code>, <code>marginal</code>, and <code>conditional</code> actually represent
the respective joint, marginal, and conditional measures (in the
latter case, up to multiplication by a constant). In the case of
<code>joint</code>, the base measure on <code>b</code> should not depend on the value
passed to the function <code>fxdy</code>.</p>
<blockquote class="pullquote-display">
<p>
Much of the theory of Bayesian inference is a search for various
ways to turn a density into a sampler for the same distribution.
</p>
</blockquote>
<p>Conditioning is why densities are interesting. But samplers are nicer
to compute with. Is there a way to get from a density to a sampler
for the same distribution? Much of the theory of Bayesian inference
is a search for various ways to do that with acceptable performance
and acceptable degree of approximation.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> But let us start
with basics.</p>
<h2 id="importance-weighting">Importance Weighting</h2>
<p>So, suppose we have a <code>Density a</code> and we want something like a sampler
for the same distribution. What can we do? Well, the density is
against a base measure, which we presumably understand. So we can
perhaps draw samples from the base measure and weight them by the
density. What would that get us? Eventually it will get us to the
ubiquitous <a href="https://en.wikipedia.org/wiki/Importance_sampling">importance
sampling</a> and
<a href="https://en.wikipedia.org/wiki/Rejection_sampling">rejection sampling</a>
algorithms, but let’s take it slow and think through each step as it
comes.</p>
<p>So, weighted samples:</p>
<pre><code>type Weight = R  -- should be non-negative
type Importanter a = Sampler (a, Weight)

weighted :: Sampler a -&gt; Density a -&gt; Importanter a
weighted sx dx = do
  x &lt;- sx
  return (x, dx x)</code></pre>
<p>What is this object that we get as a result? An importanter over <code>a</code>
is a machine that emits random values of type <code>a</code> together with the
weights (which are real numbers) those values should be given.
How meaningfully does an importanter represent a probability
distribution?</p>
<h3 id="measuring-importance">Measuring Importance</h3>
<p>We can capture the idea that the samples emitted by an importanter
“should be” taken with the grains of salt given by the weights, by
defining the measure on <code>a</code> that an importanter gives to take that
information into account.</p>
<p>Formally, consider <code>ix :: Importanter a</code>. Being a sampler, <code>ix</code>
defines a measure <span class="math inline">\(s\)</span> on the set of pairs <span class="math inline">\(A \times \R\)</span>. For any
subset <span class="math inline">\(S \subset A\)</span> with indicator function <span class="math inline">\(f_S\)</span>, we can define the
weight of <span class="math inline">\(S\)</span> under <span class="math inline">\(s\)</span> as</p>
<p><span class="math display">\[ W(S) = \int_{(x,w)} w f_S(x) ds, \]</span>
which is the expected weight of elements of <span class="math inline">\(S\)</span> generated by <code>ix</code>.
Then we can define <span class="math inline">\(\mu\)</span> on <span class="math inline">\(A\)</span> as the fraction of the total expected
weight contained in <span class="math inline">\(S\)</span>:</p>
<p><span class="math display">\[ \mu(S \subset A) = \frac{W(S)}{W(A)}, \]</span>
provided the denominator is finite and positive.</p>
<p>Exercise: Prove that <span class="math inline">\(\mu\)</span> is a probability measure if <span class="math inline">\(s\)</span> is
and <span class="math inline">\(0 &lt; W(A) &lt; \infty\)</span>.</p>
<p>Exercise: Prove that if <code>dx :: Density a</code> is a density and <code>sx :: Sampler a</code> is a sampler for the base measure of <code>dx</code>, then <code>weighted sx dx :: Importanter a</code> is an importanter representing the same
probability distribution as <code>dx</code>.</p>
<p>Exercise: Prove that the integration rule under <span class="math inline">\(\mu\)</span> is given by</p>
<p><span class="math display">\[ \forall (c:A \to \R), \quad \left(\int c d\mu\right) W(A)
  = \int \texttt{comp c } ds, \]</span>
where</p>
<pre><code>comp :: (a -&gt; R) -&gt; (a, R) -&gt; R
comp c (x, weight) = (c x) * weight</code></pre>
<p>This integration rule formalizes the idea that to make conclusions
about <span class="math inline">\(\mu\)</span> based on being able to compute with <span class="math inline">\(s\)</span>, we have to weight
every <code>x</code> we get out of <span class="math inline">\(s\)</span> by the weight it came with, and discount
our overall conclusions by the overall weight <span class="math inline">\(W(A)\)</span>.</p>
<h3 id="composition-of-importanters">Composition <em>of</em> Importanters</h3>
<p>is enough like composition of samplers that I omit the discussion to
save space.</p>
<h3 id="weighted-expectations">Weighted Expectations</h3>
<p>The integration rule for the measure denoted by an importanter
tells us how to compute expectations with weighted samples, which
conveniently agrees with what one would expect:</p>
<pre><code>finite_weighted_expectation :: Int -&gt; Importanter R -&gt; Sampler R
finite_weighted_expectation n ix = fmap w_avg $ replicateM n ix where
  w_avg :: [(R, Weight)] -&gt; R
  w_avg samples = (sum $ map times samples) / (sum $ map snd samples)
  times (x, w) = x * w</code></pre>
<p>Theorem (Law of Large Numbers with weights): For any importanter <code>ix :: Importanter R</code>, as
<span class="math inline">\(n \to \infty\)</span>, the finite weighted expectations
<code>finite_weighted_expectation n ix</code> converge (as distributions) to
<code>return mean_ix :: Sampler R</code> where the constant <code>mean_ix</code> is the
expected value of the distribution on <span class="math inline">\(\R\)</span> denoted by the importanter
<code>ix</code>.</p>
<p>This theorem justifies the definition</p>
<pre><code>weighted_expect :: Importanter R -&gt; R
weighted_expect ix = mean_ix where
  return mean_ix = infinite_limit (\n -&gt; finite_weighted_expectation n ix)</code></pre>
<h3 id="importance-sampling">Importance Sampling</h3>
<p>Getting a (terminating, exact) sampler out of an importanter, however,
is more complicated. The trouble is
that no matter how many times we’ve run our importanter, it’s possible
that the next run will produce a new value with a huge weight, and
throw off all our previous conclusions. But let’s take that one step
at a time.</p>
<blockquote class="pullquote-display">
<p>
No matter how many weighted samples one has drawn, the next one might
have such a huge weight that it throws off all previous conclusions.
</p>
</blockquote>
<p>There is of course an obvious and computationally efficient way to get
some sampler for <code>a</code> out of an importanter over <code>a</code>—just drop the
weights:</p>
<pre><code>importance_approximation :: Importanter a -&gt; Sampler a
importance_approximation = fmap fst</code></pre>
<p>The trouble is, of course, that the sampler we get does not sample
from the distribution the importanter denotes—unless the weights are
all equal. And indeed, the closer the weights are to equal, the
better an approximation it is to just drop them. It turns out
that if we define</p>
<pre><code>weight_distribution :: Importanter a -&gt; Sampler Weight
weight_distribution = fmap snd</code></pre>
<p>then we get the</p>
<p>Theorem: The quality of the <code>importance_approximation</code> goes
as the quality of the approximation <code>return . expect</code> to the
<code>weight_distribution</code>.</p>
<p>So a “good” importanter is one that uses little computation to run and
produces weights concentrated around one value.</p>
<h3 id="proposals">Proposals</h3>
<p>Above, we constructed an <code>Importanter a</code> out of a <code>Density a</code> by
drawing samples from the base measure and weighting them by the
density. The trouble is that if the density is peaky, this will yield
an importanter with a wide spread of weights, which may not be very
efficient. If we can (efficiently) account for some of that variation
by drawing those samples from some other probability distribution,
that is perhaps closer to the target, we may be able to get a better
importanter.</p>
<p>To wit, we can use any <code>Dist a</code> (whose density has the same base
measure as the target) as a <em>proposal distribution</em> whose samples we
can weight to make an importanter. The weight of a proposal is its
value under our goal density, divided by its value under the proposal
density.</p>
<pre><code>proposal_to_importanter :: Dist a -&gt; Density a -&gt; Importanter a
proposal_to_importanter (propose, prop_density) target_density = do
  sample &lt;- propose
  let d_target = target_density sample
      d_prop = prop_density sample
  return (sample, d_target / d_prop)</code></pre>
<p>Theorem: Given any proposal distribution <code>xs</code> and a target density
<code>target</code>, the importanter <code>proposal_to_importanter xs target</code> denotes
the same measure on <code>a</code> as the <code>target</code>, provided:</p>
<ul>
<li>the two densities are with respect to the same base measure on <code>a</code>
(but it doesn’t matter what that base measure is!), and</li>
<li>and their ratio at every <code>a</code> is finite (that is, <code>xs</code> has a positive
density at any <code>a</code> with positive density under <code>target</code>).</li>
</ul>
<p>The division of densities also amounts to changing the base measure of
the target density to be the measure denoted by the sampler of the
proposal distribution.</p>
<p>A “good” proposal distribution for a given target is one that leads to
a good importanter—ideally, the proposal (and the density ratio) are
efficient to evaluate, but at least as importantly we want the density
ratio to be concentrated around the mean, rather than varying widely.
For that, we want the proposal distribution to be close to the target,
and in particular not to under-cover any region too severely (because
that can lead to very large weights). In the limit where the proposal
distribution is exactly the target distribution, the weights always
come out exactly 1.</p>
<h3 id="resampling">Resampling</h3>
<p>Even if we can’t find a good proposal distribution,
we can trade work for a more concentrated weight distribution. There
is a universal trick called <em>resampling</em> for trading computation for
improving the importance approximation of any importanter. It consists of computing <span class="math inline">\(n\)</span>
weighted samples, picking one with probability proportional to the
weights, and emitting it with the combined weight of all the samples you
drew. In a sense, that one sample summarizes the information gained
from the <span class="math inline">\(n\)</span> runs of the importanter. In code:</p>
<pre><code>finite_resample :: Int -&gt; Importanter a -&gt; Importanter a
finite_resample n ix = do
  samples &lt;- replicateM n ix
  result &lt;- weighted_select samples
  return (result, sum $ map snd samples)
  where weighted_select :: [(a, Weight)] -&gt; Sampler a
        -- picks an element from the given list with probability
        -- proportional to its weight</code></pre>
<p>Exercise: Prove that for any <span class="math inline">\(n &gt; 0\)</span> and any <code>ix :: Importanter a</code>,
the measure on <code>a</code> given by <code>finite_resample n ix</code> is the same as the
measure given by <code>ix</code> itself.</p>
<p>Exercise: Prove that for fixed <code>ix :: Importanter a</code>, as <span class="math inline">\(n\)</span>
increases, the <code>weight_distribution</code> of <code>finite_resample n ix</code>
concentrates, thereby improving the <code>importance_approximation</code>.</p>
<p>Theorem: In the limit as <span class="math inline">\(n\)</span> tends to <span class="math inline">\(\infty\)</span>, the distribution
denoted by the sampler <code>importance_approximation $ finite_resample n ix</code> converges to the distribution denoted by the importanter <code>ix</code>.</p>
<p>Exercise: Implement the resampling idea without knowing <span class="math inline">\(n\)</span> in advance
and without consuming intermediate storage that is linear in <span class="math inline">\(n\)</span>.</p>
<pre><code>rolling_resample :: Importanter a -&gt; [Importanter a]
-- the nth element of rolling_resample ix should be equivalent to
-- finite_resample n ix</code></pre>
<p>The trick is that weighted selection is associative.</p>
<p>Why might resampling be of use? That is, why throw out the samples we
(presumably) spent so much computation on instead of providing all of
them? Because actually, fewer samples will require less computation
downstream; and the resampling rule will tend to pick samples with
large weight, so the samples that are thrown away were less important
anyway.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<h2 id="rejection">Rejection</h2>
<p>Having studied importance, let us turn to another foundational method
of creating samplers for new probability distributions.
<a href="https://en.wikipedia.org/wiki/Rejection_sampling">Rejection
sampling</a> is the
probabilist’s name for “generate and test”—make up an object, and if it is
“good”, keep it, otherwise try again. The great advantage of
rejection sampling is how little information it requires to operate;
little enough that it can serve as a definition for the idea of
conditional probability.</p>
<p>We can package up the generation part and the test part in a single
intermediate object that we can reason about as a whole:</p>
<pre><code>type Rejecter a = Sampler (Maybe a)</code></pre>
<p>The way to read this is that a <code>Rejecter</code> over <code>a</code> is a sampler that
can fail: it either successfully produces <code>Just</code> an <code>a</code>, or produces a
sentinel value called <code>Nothing</code> that indicates failure. The
relationship between rejecters and samplers is that one can recover a
sampler by trying a rejecter repeatedly until it succeeds:<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<pre><code>rejection :: Rejecter a -&gt; Sampler a
rejection r = fmap head $ fmap catMaybe $ replicateM r</code></pre>
<p>Computing expectations from a rejecter consists of turning it into a
sampler and computing expectations.</p>
<h3 id="measuring-rejection">Measuring Rejection</h3>
<p>The link to measure theory lets us directly define which
distribution over <code>a</code> a given <code>Rejecter a</code> represents, without having
to appeal to the <code>rejection</code> algorithm to be definitional (and
therefore without having to re-analyze its behavior whenever the idea
of rejection appears):</p>
<p>By virtue of being a sampler, a rejecter denotes a measure over
<code>Maybe a</code>. We can associate a measure over <code>a</code> with it by
saying that the size of any subset <span class="math inline">\(S\)</span> is the size of <span class="math inline">\(S\)</span> viewed as a
subset of <span class="math inline">\(A \cup \{\texttt{Nothing}\}\)</span>, scaled up by dividing it by
the probability of the rejecter accepting (provided that probability
is positive).</p>
<p>Formally, given a <code>xs :: Rejecter a</code>, let <span class="math inline">\(s\)</span> be the measure on <code>Maybe a</code> defined by <code>xs</code>. For any subset <span class="math inline">\(S \subset A\)</span>, we can abuse
notation to define <span class="math inline">\(\texttt{Just } S\)</span> to be the set of objects of type
<code>Maybe a</code> that are <code>Just</code> some element of <span class="math inline">\(S\)</span>. Then we can set</p>
<p><span class="math display">\[ \mu(S) = \frac{s(\texttt{Just } S)}{s(\texttt{Just } A)}, \]</span>
provided the denominator is positive.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p>Exercise: Prove that <span class="math inline">\(\mu\)</span> is a probability measure whenever the
acceptance probability <span class="math inline">\(s(\texttt{Just } A)\)</span> is positive.</p>
<p>Exercise: Prove that integration under <span class="math inline">\(\mu\)</span> is given by the
rule</p>
<p><span class="math display">\[ \forall (c:A \to \R), \quad \left(\int c d\mu\right) s(\texttt{Just } A)
  = \int \texttt{comp c } ds, \]</span>
where</p>
<pre><code>comp :: (a -&gt; R) -&gt; Maybe a -&gt; R
comp c (Just x) = c x
comp c Nothing  = 0</code></pre>
<p>One interpretation of this rule is that to turn the measure <span class="math inline">\(s\)</span> on
<code>Maybe a</code> into the measure <span class="math inline">\(\mu\)</span> on <code>a</code>, we just pretend that <code>Maybe a</code> was <code>a</code>, except we demand that all users of the <code>Maybe a</code> interpret
<code>Nothing</code> results as “no effect” (which is what zero does for
integration), and scale their conclusions by the inverse of the
probability of acceptance. In a manner of speaking, we moved the
rejection into the continuation.</p>
<p>Exercise: Prove the soundness of the <code>rejection</code> algorithm. To wit,
for any <code>xs :: Rejecter a</code> that terminates with probability 1 and accepts
with positive probability, prove that</p>
<ul>
<li><p>the sampler <code>rejection xs</code> terminates with probability 1, and</p></li>
<li><p>the measure <span class="math inline">\(\mu_1\)</span> on <code>a</code> given by <code>rejection xs</code> is the same as
the measure <span class="math inline">\(\mu_2\)</span> on <code>a</code> given directly by <code>xs</code> through the above
definition.</p></li>
</ul>
<p>Note: The expected number of times the <code>rejection</code> algorithm will invoke
the rejecter is the inverse of the probability of acceptance. Thus, a
“good” rejecter for <span class="math inline">\(\mu\)</span> is one that uses little computation per
attempt, and accepts with reasonably high probability, so that
applying <code>rejection</code> to it produces a good sampler.</p>
<h2 id="relationship">Relationship</h2>
<p>Now it is time to tie these concepts together.</p>
<blockquote class="pullquote-display">
<p>
If we have an upper bound on the weights, we can recover an exact
sampler by converting those weights into probability of acceptance.
</p>
</blockquote>
<p>The best we could do with just an importanter is to resample it some
number of times and hope the resulting approximation to the
distribution that importanter represents is good enough. With a
little more information, though, it is possible to turn an importanter
into a rejecter (and therefore a sampler) denoting exactly the same measure.
To wit, if we somehow (analytically?) know
an upper bound on the weights produced by some importanter, we can
turn it into a rejecter that produces samples from the same
distribution:</p>
<pre><code>type WeightBound = Double

importanter_to_rejecter :: WeightBound -&gt; Importanter a -&gt; Rejecter a
importanter_to_rejecter bound xs = do
  (sample, weight) &lt;- xs
  u &lt;- unit_random
  if u * bound &lt; weight then
    return $ Just sample
  else
    return Nothing</code></pre>
<p>The intuition for this algorithm is that it translates the weight that
should be attached to any given <code>a</code> that comes out of <code>xs</code> into the
probability that this particular <code>a</code> will be accepted by the test. In
order to do that coherently, though, an upper bound on weights is
needed, to make sure that all the probabilities are scaled correctly.
If the bound not tight, the resulting rejecter will accept less often
than it might.</p>
<p>Theorem: If <code>bound</code> is larger than any weight <code>xs :: Importanter a</code>
can ever return, then <code>importanter_to_rejecter bound xs</code> denotes the
same measure on <code>a</code> as <code>xs</code> does.</p>
<p>Conjecture: If the integral of returnable weights that are above the
<code>bound</code> is small, then <code>importanter_to_rejecter bound xs</code> denotes a
measure close to the measure denoted by <code>xs</code>.</p>
<p>Thus a “good” importanter for which one also knows a tight upper bound
on the returned weights leads to a “good” rejecter. The exactness
provided by rejection comes at a price—one needs to have an upper
bound, and the rejecter will perform worse if one’s bound is overly
conservative.</p>
<p>Now we have all the pieces to understand the standard names from the
field. Importance sampling is usually presented as the composition
of proposal weighting, resampling some number of times, and dropping
the weights:</p>
<pre><code>importance_sampling :: Int -&gt; Dist a -&gt; Density a -&gt; Sampler a
importance_sampling n prop target =
  importance_approximation $     -- the result is approximate
  finite_resample n $
  proposal_to_importanter prop target</code></pre>
<p>Rejection sampling is usually presented as a different composition,
of proposal weighting, converting weights into probabilities of
acceptance, and looping until an acceptable sample is generated:</p>
<pre><code>rejection_sampling :: WeightBound -&gt; Dist a -&gt; Density a -&gt; Sampler a
rejection_sampling bound prop target =
  rejection $
  importanter_to_rejecter bound $
  proposal_to_importanter prop target</code></pre>
<p>I find the decomposition into distinct <code>Sampler</code>s, <code>Rejecter</code>s, and
<code>Importanter</code>s more aesthetic—and tending toward greater parsimony
and generality of analysis.</p>
<h2 id="exchangeable-coupling">Exchangeable Coupling</h2>
<p>And now for something completely different that this view sheds light
on.</p>
<p>The phenomenon of <a href="https://en.wikipedia.org/wiki/Exchangeable_random_variables">exchangeable
sequences</a>
arises from there being two different ways to get a <code>Sampler [a]</code> from
(a desired length and) a <code>Sampler (Sampler a)</code>. That is, if you have
a machine that makes random machines that make random objects, and you
want a random sequence of objects, you have options, and they are not
the same.</p>
<p>A distribution on length-<span class="math inline">\(n\)</span> lists <code>xs :: Sampler [a]</code> is said to be
<em>independent and identically distributed (IID) with distribution <code>x</code></em> if
<code>x :: Sampler a</code> and <code>xs = replicateM n x</code>. That is, as the name says,
each object was generated independently from the others from the same known
distribution.</p>
<p>A distribution on length-<span class="math inline">\(n\)</span> lists is said to be <em>exchangeable</em> if all
permutations of a given list are equiprobable under it. All IID
distributions are exchangeable, but not vice versa.</p>
<p>So, if you have a <code>Sampler (Sampler a)</code> and you want independent <code>a</code>s,
you can make a new machine for each object, and use it once:</p>
<pre><code>independently :: Int -&gt; Sampler (Sampler a) -&gt; Sampler [a]
independently n xss = replicateM n $ join xss</code></pre>
<p>makes an IID sampler with element distribution <code>join xss</code> (in
probabilist-speak, the one-element distribution marginalizing out the
machines).</p>
<p>On the other hand, you could also make just one machine, and generate
all your samples from it:</p>
<pre><code>exchangeably :: Int -&gt; Sampler (Sampler a) -&gt; Sampler [a]
exchangeably n xss = join $ fmap (replicateM n) xss
                -- = xss &gt;&gt;= (replicateM n)</code></pre>
<p>This sampler is not IID,<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> but is still
exchangeable.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>Theorem (de-Finetti): All exchangeable distributions can (in principle) be
represented as an application of <code>exchangeably</code> to some distribution
over distributions.</p>
<p>Typical probabilistic programming languages make the difference
between <code>independently</code>, <code>exchangeably</code>, and <code>fmap (replicateM n)</code> a
pain to think about, because they implicitly <code>join</code> everywhere, so one
has to write one’s code carefully to get the effect one wants.</p>
<h2 id="thanks">Thanks</h2>
<p>to Tanya Khovanova, Alex Plotnick, and David Wadden for reading drafts
of this.</p>
<h2 id="notes">Notes</h2>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      R: "{\\mathbb{R}}",
      eps: "\\varepsilon"
    },
    equationNumbers: { autoNumber: "AMS" },
    noErrors: { disabled: true },
  }
});
</script>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>If you don’t know Haskell, don’t worry: I will say
everything in English first. You will just have to take my word that
there are simple algorithms. Here is a quick glossary for
how to interpret the type signatures, so you can follow the shape
of the argument:</p>
<ul>
<li><p><code>--</code> (two hyphens) begins a comment, which continues to the end
of the line.</p></li>
<li><p><code>::</code> (two colons) means “of type” or “has type”. It says that the
expression on the left has the type given on the right.</p></li>
<li><p><code>-&gt;</code> (rightward arrow) is an infix type operator meaning “function”.
That is, <code>Foo -&gt; Bar</code> is a function taking an object of type <code>Foo</code>
and returning one of type <code>Bar</code>. The arrow associates to the right:
<code>Foo -&gt; Bar -&gt; Baz</code> is <code>Foo -&gt; (Bar -&gt; Baz)</code>, which is a function
taking a <code>Foo</code> and returning a function that takes a <code>Bar</code> and
produces a <code>Baz</code>. Such a beast is operationally equivalent to
a binary function that takes a <code>Foo</code> and a <code>Bar</code> and produces
a <code>Baz</code>, and Haskell automatically applies the equivalence whichever
way is most convenient.</p></li>
<li><p>(whitespace) is type constructor application. For example,
one writes <code>Set Integer</code> to denote the type of sets of integers.
So a function from sets of integers to sets of floating point
numbers would have type <code>Set Integer -&gt; Set Double</code>.</p></li>
<li><p>Haskell type signatures can have variables in them.
<code>Set a -&gt; Set a</code> means “a function from sets of anything to sets
of the same thing.” Actually, it means something stronger than
that: the function is taken to be <a href="https://en.wikipedia.org/wiki/Parametric_polymorphism">parametrically
polymorphic</a>,
which is to say it can’t manipulate the individual objects of
type <code>a</code>, but operate only on the set.</p></li>
<li><p><code>[]</code> (square brackets) mean “list of” in Haskell.</p></li>
<li><p><code>(,)</code> (comma-separated list in round brackets) is for tuples.
The components of 2-tuples are accessed by the functions <code>fst</code>
and <code>snd</code>.</p></li>
<li><p>In the code, (whitespace) is function application. In Haskell,
one writes <code>f x</code> to mean “apply f to x”. This choice is natural
for a functional language, where one is applying functions all
the time, but can be a bit confusing to someone who is not used
to juxtaposition signifying that. This operator has the highest
precedence: <code>f x + 4</code> is “apply f to x, then add 4”, not “apply
f to x+4”. There is also the <code>$</code> operator, which is function
application but with the lowest precedence, so <code>f $ x + 4</code> is
“apply f to x+4”.</p></li>
</ul>
<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn2"><p>Samplers are different from random variables as
traditionally defined. One of the formulations of traditional random
variables is “(measurable) functions from a probability space”.
Samplers are also functions from a probability space, namely the space
of unbounded numbers of uniform random bits. The difference is that
traditional random variables can be functions from the same
probability space, and thus can exhibit dependence; whereas I treat
Samplers as getting independent random bits every time they are
called. The two formulations are equi-expressive (at least if
restricted to computable situations): any system of random variables
can be modeled as a big Sampler for their joint distribution; and any
invocation of a Sampler can be viewed as a system of random variables
(one per intermediate value in the Sampler’s computation).<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>In case the <code>join</code> operation looks a bit strange, imagine
randomly choosing a coin and then flipping it once. If the coins may
have different weights, that’s a probability distribution (the choice)
over probability distributions (the possible biases of the flip).
<code>join</code> just tells us that we can view that compound process as a
single probability distribution over heads/tails outcomes.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>One also talks about <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">cumulative distribution
functions</a> (CDFs)
when one talks about probability distributions over the real numbers,
but I won’t bother because a CDF is just the integral of the density,
so carries (more of) the same sort of information. Not all of the
subsequent discussion applies, because there is a good way to recover
a sampler from a CDF, but CDFs are also much less commonly available
than densities.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>That is, sampling from a different distribution that is
easier to sample from but approximates the distribution given by the
density; or, in some cases, sampling from a different distribution
entirely, but whose samples in some way help to compute desired
expectations with respect to the density of interest.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>In fact, drawing multiple independent samples from the
same base set and continuing the computation with all of them is also
useful, and also called resampling. One abstract view of the thing
called a “particle filter” is interleaving resampling steps between
a series of <code>bind</code>s in the <code>Importanter</code> monad.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>The attentive reader may notice that I am not
passing a count to <code>replicateM</code> here. I mean a combinator that cannot
be defined in Haskell in general, which emits a lazy stream of results
from a monadic action. For samplers this is OK though, because
drawing randomness commutes in distribution:</p>
<pre><code>replicateM :: Sampler a -&gt; Sampler [a]</code></pre>
<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn8"><p>The denominator <span class="math inline">\(s(\texttt{Just } A)\)</span> in this formula is
of course just the probability that <code>xs</code> accepts, that is returns
<code>Just</code> something as opposed to <code>Nothing</code>. We can compute it (to
arbitrarily good approximation) as the expectation of the indicator
function for <span class="math inline">\(\texttt{Just } A\)</span> over <code>xs</code>, but we don’t need to in
practice because <code>rejection</code> builds that correction in for us.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>The distribution over a single element is still <code>join xss</code>, but now they are coupled through the common machine. To reprise
the choice of coins example, a sequence of flips generated by choosing
one coin with unknown bias and flipping it repeatedly is not IID,
because seeing the beginning of the sequence gives information about
the end by learning (something about) the bias of the coin.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>If you wrote <code>exchangeably</code> without the <code>join</code>, it would
produce a sampler for samplers for IID sequences. However, you
wouldn’t know a priori which IID sequence you were going to get, so
predictions about the future behavior of any one of them would be
affected by observations of its past behavior, by inferring the
internal structure of the machine.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Sun, 15 Feb 2015 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2015/how-to-compute-with-a-probability-distribution/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Social Function <em>of</em> Module Systems</title>
    <link>https://alexey.radul.name/ideas/2014/social-function-of-module-systems/index.html</link>
    <description><![CDATA[<p>Observing the state of modern software practice, one might notice that
a rough tower of different abstraction mechanisms has emerged. One
might then wonder: why do we need so many concepts, if their
fundamental job is to give a name to some software, and allow one to
use it just by referring to its name?</p>
<p>Indeed, why do we need</p>
<ul>
<li>functions (procedures, methods), <em>and</em></li>
<li>classes (in languages that have them), <em>and</em></li>
<li>modules, <em>and</em></li>
<li>packages, <em>and</em></li>
<li>distributions (such as the Debian or Red Hat distributions of GNU/Linux)?</li>
</ul>
<p>What drives the choice of one or another mechanism when deciding how
to abstract something? What drives a language designer’s choice of
how to make one or another of these kinds of mechanisms behave?</p>
<p>I hypothesize that the answers are at the bottom social. Let us
examine these different abstraction mechanisms in turn.</p>
<h2 id="functions">Functions</h2>
<p>The size of functions is bounded from below by them being the smallest
unit of code resue (or code sharing, or abstraction); and from above
by huge code style pressure to keep them small. (Which I agree with!)
Partly by being the smallest and most fundamental, functions are more
or less required to be recursive (that is, a function can call another
function, and in functional languages can also define, emit, or
consume another function) and configurable (functions generally take
arguments).</p>
<h2 id="classes">Classes</h2>
<p>The size of classes is bounded from below by them (often) being the
smallest first-class<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> unit of code reuse (sharing;
abstraction). Because of this, there is pressure on classes to be
parametrizable and recursive in the same way as functions—hence
constructors taking arguments, the ability to take instances as
arguments to methods, and Java’s inner classes, for example.</p>
<p>In languages with classes, they (especially “toplevel” classes) also
seem to serve the same function as the smallest modules do in
functional languages, which puts pressure on classes to have the
properties of modules also—hence the idea of “private members” that
cannot be read or written from outside the class’s own methods.</p>
<h2 id="modules">Modules</h2>
<p>The word “module” actually means many different things in different
programming languages. Here I will talk about what a “module” is in
Python, Haskell, or Racket. An overlapping function is served by
“namespaces” in Clojure and C++ and “packages” in Java; though in the
latter two, a (large) class often serves this function as well.</p>
<p>Modules, at least the smallest ones, tend to correspond to chunks of
code that one programmer can be expected to keep in their head at
once, after some study. Hence the tendency for one module to occupy
exactly one source file. Also, boundaries between developers on a
team tend to fall along module boundaries.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> This is why the
question of “information hiding” arises: the user of a module is
pretty likely to be a different person from its author/maintainer, so
reasonable effort should be made to keep them from having to
synchronize changes (in either direction!)</p>
<p>This tendency changes the demands on modules as an abstraction
mechanism. If one wants subtle recursion or parametrizability, the
thinking goes, one can use a function or a class.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> What is needed
from a module, instead, is the ability to use it easily, without
having to understand its internals, and without having to react to
someone else changing its internals. In compiled languages, this
often comes with the desideratum of separate compilation: rebuilding
the full system after an internal change to one module should ideally
take time proportional to the size of that module, plus perhaps the
number of modules, but not the total size of the whole system.</p>
<h2 id="packages">Packages</h2>
<p>Packages are to organizations as modules are to individuals. The
size and scope of a package is bounded above by the social requirement
to be able to enforce at least some policies across the whole package.
For example, typical package systems rely on module names being unique
within a package. Hierarchy appears, either in the module system or
in the package system of a programming language, because organizations
are often hierarchical. That way, at least for names, each level in
the hierarchy need only enforce relatively local uniqueness, but fully
qualified pathnames end up globally unique.</p>
<p>Packages are also the smallest unit of software distribution—that
is, of the transfer of software from one organization to (members of)
another. That imposes a heavy requirement that the insides of
packages leak very little across their published interfaces; but it
also makes configurability and recursion even less of an issue than
for modules, because configurability must be preceded by understanding
to be useful.</p>
<h2 id="distributions">Distributions</h2>
<p>One more phenomenon occurs these days, which is “distributions”. The
paradigmatic instance of distributions is GNU/Linux, though many
programming languages besides C effectively have distributions of
their software as well: Hackage for Haskell, Quicklisp for Common
Lisp, C{P,T,R}AN for Perl, TeX and R, Rubygems for Ruby, etc.</p>
<p>I think of a
(good) distribution as sort of a co-package: organization-level
consistency, but by and for consumers rather than producers of
packages. The thing that a distribution is supposed to accomplish is
defining a coherent universe of possible packages (including with
unique package names), such that any subset of them (that obeys the
dependency relation) can coexist and be simultaneously usable. Of
course, there are a variety of global resources that packages may
compete for (name space of program elements, name space in the file
system for e.g. temporary storage, exclusive control of various
devices, etc), so this is in general a very difficult and ongoing
problem.</p>
<p>A distribution is the level at which the <a href="https://en.wikipedia.org/wiki/Dependency_hell">diamond dependency problem</a>
becomes severe. If there are, in fact, global resources that packages
compete over, then there may be situations where two packages cannot
coexist in the same system. Most often this happens when the “two”
packages are actually different, slightly incompatible versions of the
same package. They will naturally tend to define the same names, but
differently, and try to use the same resources.</p>
<p>Anyway, if it is possible for two packages to be unable to coexist,
then (in current package management systems) that relation tends to be
contagious: if A depends on B and C depends on D where B and D cannot
coexist, then typically A and C also cannot coexist. And this is a
screw: even if no one in their right mind would want B and D together
(e.g., because D is actually B-2.0), someone in their right mind could
very well want A and C together, because they could be completely
unrelated. And if A is produced by one organization, C by another,
and the “someone” resides in yet a third, that someone has a real
problem.</p>
<p>Such situations tend not to be big problems at smaller levels than
packages, because then A, B, C, and D are typically all in the same
organization, so there are channels for communication and leverage to
resolve the problem (typically either by making B and D compatible, or
by changing either A or C so they depend on the same thing such that B
and D need not coexist). But at the level of packages, where
production and use are supposed to cross organizational boundaries,
diamond dependencies can be a severe headache. I know I have been
bitten by them several times.</p>
<p>The outstanding challenge for the design of a programming language’s
module or package system is therefore this: Make it possible to write
packages in such a way that two distinct but similar versions of the
same package can coexist. Maybe not “be usable together directly from
the same third package”, but at least “be usable together through a
diamond”. In other words, what a package depends on should not leak
through to affecting what it can interoperate with.</p>
<h2 id="notes">Notes</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>By “first-class” I mean that historically,
object-oriented languages have tended not to allow class methods to be
stored in variables or data structures, passed as arguments, and so
forth—that privilege is reserved for instances of classes.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Typically, one programmer on a team will understand
several modules (and their interactions). I would suggest that good
style is to make modules as small as possible such that any module
boundary can be a person-understanding boundary. That gives the
greatest flexibility in adjusting which people carefully understand
what.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The module system of Standard ML is one major exception, but
I do not understand it enough to comment.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Sat, 26 Jul 2014 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2014/social-function-of-module-systems/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Stochasticity <em>is a</em> Quantifier</title>
    <link>https://alexey.radul.name/ideas/2014/stochasticity-is-a-quantifier/index.html</link>
    <description><![CDATA[<p>In English, quantifiers are words like “all”, “one”, or “some” that
indicate how broadly true the quantified clause is. Formal logic has
adopted symbols for such words, namely “all”, “exactly one”, and
“some” (in the sense of “at least one”). Probability theory offers us
a reason to incorporate another symbol, with a meaning along
the lines of “some, and I have a sense of how to find them”.</p>
<p>I want to present the notation, justify it as a coherent and uniform
extension of classic logical practice, and use it to explain something
I understood much less clearly before: the nature of the distinction
between frequentist and Bayesian statistics, specifically on the
example of comparing confidence intervals to credibility intervals.</p>
<h2 id="contents">Contents</h2>
<ol type="1">
<li><a href="#formulae">Formulae</a>
<ol type="1">
<li><a href="#from-logic-to-games">From Logic to Games</a></li>
<li><a href="#the-random-player">The Random Player</a></li>
<li><a href="#back-to-logic">Back to Logic</a></li>
<li><a href="#summary">Summary</a></li>
</ol></li>
<li><a href="#statistics">Statistics</a>
<ol type="1">
<li><a href="#frequentist">Frequentist</a></li>
<li><a href="#bayesian">Bayesian</a></li>
<li><a href="#comparison">Comparison</a></li>
</ol></li>
<li><a href="#reflection">Reflection</a></li>
<li><a href="#notes">Notes</a></li>
</ol>
<h2 id="formulae">Formulae</h2>
<h3 id="from-logic-to-games"><em>From</em> Logic <em>to</em> Games</h3>
<p><a href="https://en.wikipedia.org/wiki/Game_semantics">Logic can be embedded into game theory</a>. A
(closed) logical formula with quantifiers (in <a href="https://en.wikipedia.org/wiki/Prenex_normal_form">prenex form</a>) can be taken to be
a two-player
zero-sum game where I choose a value for every exists-quantified
variable and my adversary chooses
a value for every forall-quantified variable. I win if
the quantifier-free part ends up being true,
and the adversary wins if the quantifier-free part ends up being false.
The order of making choices and the information available to each
player has to follow quantifier scope. In this embedding, we call
a quantified formula “True” if I win this game under perfect play, and
“False” if the adversary does.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>For example, the formula
<span class="math display">\[ \forall k \in \Z. \exists n \in \Z. n &gt; k \]</span>
is the game</p>
<ol type="1">
<li>Adversary chooses an integer <span class="math inline">\(k\)</span> (not knowing <span class="math inline">\(n\)</span>)</li>
<li>I choose an integer <span class="math inline">\(n\)</span> (knowing <span class="math inline">\(k\)</span>)</li>
<li>I win if <span class="math inline">\(n &gt; k\)</span>, adversary wins if not <span class="math inline">\(n &gt; k\)</span>.</li>
</ol>
<p>Since we are playing over the integers, this game is one I can
always win, which is the same as saying that this formula is true.</p>
<p>The order of quantification, or in other words the order of choices in
the game,<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> matters—the formula
<span class="math display">\[  \exists n \in \Z. \forall k \in \Z. n &gt; k \]</span>
is the game</p>
<ol type="1">
<li>I choose an integer <span class="math inline">\(n\)</span> (not knowing <span class="math inline">\(k\)</span>)</li>
<li>Adversary chooses an integer <span class="math inline">\(k\)</span> (knowing <span class="math inline">\(n\)</span>)</li>
<li>I win if <span class="math inline">\(n &gt; k\)</span>, adversary wins if not <span class="math inline">\(n &gt; k\)</span>,</li>
</ol>
<p>which under correct play the adversary can always win. (To wit, the
formula is false). I will not bore you with the argument that this
embedding is exact (it proceeds by induction on the number of
quantifiers in the formula).</p>
<h3 id="the-random-player"><em>The</em> Random Player</h3>
<p>So far, so good. What does this have to do with probability, you ask?
Well, game theorists have noticed that it can be useful to allow a
different kind of player in their games—one that behaves randomly
instead of trying to optimize some payoff like the other players. This
random player is often called Nature (exercise for the reader: why
does game theory never need more than one random player?) and the
probability distribution(s) governing Nature’s behavior are taken to
be part of the definition of the game (just like the legal move sets and
objective functions governing the behavior of the normal players).</p>
<p>For instance, the card game
Bridge fits into this framework: at the beginning, Nature makes a move
dealing the cards (which is traditionally taken to be a uniformly
random choice among all possible deals), then each of the four players
observes a portion of the deal (to wit, their own hand), and they
start making moves according to the (now deterministic) rules of
Bridge. After the bidding, three of the players observe some more of
the deal (the dummy’s hand), and then continue making strategic moves.</p>
<h3 id="back-to-logic">Back <em>to</em> Logic</h3>
<p>We can bring the Nature player back to logic. I
propose using the symbol <span class="math inline">\(\st\)</span> (a backwards letter ‘S’, for Stochastic) as a quantifier for randomly
chosen variables.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Just like you have to say what set an <span class="math inline">\(\exists\)</span> or
a <span class="math inline">\(\forall\)</span> are drawn from, you have to say what probability
distribution an <span class="math inline">\(\st\)</span> is drawn from. The usual quantifier order and
scope rules apply.</p>
<p>As to semantics, let us say that a
(probabilistic) formula is “true with probability <span class="math inline">\(\geq p\)</span>” if I win
the corresponding two-and-a-half player zero-sum game (the random
player is counted as half) with probability <span class="math inline">\(\geq p\)</span> under optimal
play.</p>
<p>For example, calling a flip of a fair coin looks like
<span class="math display">\[  \exists k \in \{\textrm{H},\textrm{T}\}. \st x \sim \textrm{uniform}\{\textrm{H},\textrm{T}\}. x = k, \]</span>
which is the game</p>
<ol type="1">
<li>I choose <span class="math inline">\(\textrm{H}\)</span> or <span class="math inline">\(\textrm{T}\)</span></li>
<li>Nature flips a (fair) coin</li>
<li>I win if I chose what Nature flipped.</li>
</ol>
<p>This formula can reasonably be taken as “true with probability 50%”.</p>
<p>In the case of fair coins, it doesn’t matter who calls it. The formula
<span class="math display">\[  \forall k \in \{\textrm{H},\textrm{T}\}. \st x \sim \textrm{uniform}\{\textrm{H},\textrm{T}\}. x = k, \]</span>
which is the game</p>
<ol type="1">
<li>The adversary chooses <span class="math inline">\(\textrm{H}\)</span> or <span class="math inline">\(\textrm{T}\)</span></li>
<li>Nature flips a (fair) coin</li>
<li>I win if the adversary chose what Nature flipped,</li>
</ol>
<p>is also “true with probability 50%”.</p>
<p>Observe that each of these is a very different game from the ones where the
chooser gets to see the result of Nature’s flip before making their
choice:
<span class="math display">\[  \st x \sim \textrm{uniform}\{\textrm{H},\textrm{T}\}. \exists k \in \{\textrm{H},\textrm{T}\}. x = k \]</span>
is the game</p>
<ol type="1">
<li>Nature flips a coin</li>
<li>I choose <span class="math inline">\(\textrm{H}\)</span> or <span class="math inline">\(\textrm{T}\)</span> (knowing the result)</li>
<li>I win if I chose what Nature flipped.</li>
</ol>
<p>I can always win this game, so this formula is true (with probability 100%).</p>
<p>Conversely,
<span class="math display">\[  \st x \sim \textrm{uniform}\{\textrm{H},\textrm{T}\}. \forall k \in \{\textrm{H},\textrm{T}\}. x = k \]</span>
is the game</p>
<ol type="1">
<li>Nature flips a coin</li>
<li>The adversary chooses <span class="math inline">\(\textrm{H}\)</span> or <span class="math inline">\(\textrm{T}\)</span> (knowing the result)</li>
<li>I win if the adversary chose what Nature flipped.</li>
</ol>
<p>The adversary can always make me lose this game, so this formula is
false (i.e., true with probability 0%).</p>
<p>I hope this example has convinced you that when probability occurs in logic, one
must take the same care about scoping quantifiers as one does when
mixing <span class="math inline">\(\exists\)</span> with <span class="math inline">\(\forall\)</span>.</p>
<h3 id="summary">Summary</h3>
<ul>
<li><span class="math inline">\(\forall x \in X\)</span> means the adversary chooses <span class="math inline">\(x\)</span> from the set <span class="math inline">\(X\)</span>.</li>
<li><span class="math inline">\(\exists x \in X\)</span> means I choose <span class="math inline">\(x\)</span> from the set <span class="math inline">\(X\)</span>.</li>
<li><span class="math inline">\(\st x \sim P\)</span> means an impartial player chooses <span class="math inline">\(x\)</span> according to the
probability distribution <span class="math inline">\(P\)</span>.</li>
</ul>
<h2 id="statistics">Statistics</h2>
<p>Now what does this have to do with Bayesian and frequentist
statistics? Bayesian statistics always reasons about probability distributions,
but frequentist statistics makes definitions with foralls in them. In other
words, Bayesians play solitaire against Nature, whereas frequentists
take on strategic adversaries. This means that frequentism is both
harder and more pessimistic than Bayesianism.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>I will illustrate by comparing <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence
intervals</a> and
<a href="https://en.wikipedia.org/wiki/Credible_interval">credible intervals</a>,
the textbook frequentist and Bayesian, respectively, approaches to the
<a href="https://en.wikipedia.org/wiki/Interval_estimation">interval
estimation</a> problem.
Interval estimates are the sort of statistics one sees in the news:
there is some number of interest, such as the proportion of voters
that lean towards one or another political party in an upcoming
election; some evidence about this number is gathered, such as polling
a random fraction of those voters; some computation is done; and an
interval is announced, that is purported to contain the number of
interest with some degree of confidence. As we shall soon see, the
frequentist and Bayesian notions of “confidence” are actually quite
different; but that is the basic set up.</p>
<h3 id="frequentist">Frequentist</h3>
<p>The standard frequentist tool for inverval estimation is the
confidence interval. More generally, one can define confidence regions
for set estimation of parameters that have other structure than single
numbers. To give the construction, we start with a little notation:</p>
<ul>
<li>Suppose the item we are interested in is drawn from a <em>parameter
space</em> <span class="math inline">\(A\)</span>.</li>
</ul>
<p>In a simple rendition of the political example, this would be <span class="math inline">\([0,1]\)</span>,
representing all possible fractions of voters preferring one
particular party over the other.</p>
<ul>
<li>Suppose the experiment we conduct produces results in some
<em>observation space</em> <span class="math inline">\(B\)</span>.</li>
</ul>
<p>In the example, the experiment could be a poll, and <span class="math inline">\(B\)</span> could be the
space of possible results.</p>
<ul>
<li>We model the influence the parameter exerts on the observations as a
function from <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span>, which we take to be random because we
assume the observations are also affected by other influences. This
function is traditionally called the <em>likelihood</em><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, and
can also be seen as a deterministic function from <span class="math inline">\(A\)</span> to probability
distributions over <span class="math inline">\(B\)</span>:
<span class="math display">\[\textrm{likelihood}: A \to \Pr(B).\]</span></li>
</ul>
<p>In the polling example, the randomness of the <span class="math inline">\(\textrm{likelihood}\)</span>
would include, for instance, our choice of whom to poll.</p>
<p>The definition of the likelihood function is the place where our
qualitative modeling assumptions turn into analyzable objects that we
can do mathematics with. Now,</p>
<ul>
<li>a <em>95% confidence interval</em> is a (deterministic) procedure for
going from an observation to a set of possible parameters
<span class="math display">\[\textrm{conf_int}: B \to \mathcal P(A)\]</span>
such that
<span class="math display">\[ \forall a \in A. \st b \sim \textrm{likelihood}(a). a \in \textrm{conf_int}(b) \]</span> with probability <span class="math inline">\(\geq\)</span> 95%,
where the probability is taken over the randomness of the
likelihood.</li>
</ul>
<p>In words, this formula means that for any <span class="math inline">\(a \in A\)</span> (chosen to be as
difficult as possible), at least 95% of the <span class="math inline">\(b\)</span> drawn according to
<span class="math inline">\(\textrm{likelihood}(a)\)</span> are such that the original <span class="math inline">\(a\)</span> is inside the
confidence interval computed from the given <span class="math inline">\(b\)</span>. In the political
example, this translates to the following requirement on the
confidence interval procedure: whatever the true leanings of the
population may be, at least 95% of possible polls conducted according
to our design must lead, via our <span class="math inline">\(\textrm{conf_int}\)</span>, to intervals that
contain those true leanings.</p>
<p>The thing to remember is that <span class="math inline">\(\textrm{conf_int}\)</span> does not depend on <span class="math inline">\(a\)</span>. As a
game, finding confidence intervals for a given problem looks like</p>
<ol type="1">
<li>I choose a function <span class="math inline">\(\textrm{conf_int}: B \to \mathcal P(A)\)</span></li>
<li>The adversary chooses an <span class="math inline">\(a\)</span> (knowing <span class="math inline">\(\textrm{conf_int}\)</span>)</li>
<li>Nature chooses a <span class="math inline">\(b\)</span>, given the adversary’s <span class="math inline">\(a\)</span>, according to the <span class="math inline">\(\textrm{likelihood}\)</span></li>
<li>I win if <span class="math inline">\(a \in \textrm{conf_int}(b)\)</span>.</li>
</ol>
<p>The design task when choosing <span class="math inline">\(\textrm{conf_int}\)</span> is usually to
minimize the cardinality of the sets that it returns, subject to the
above game being won with probability at least 95%.</p>
<h3 id="bayesian">Bayesian</h3>
<p>The standard Bayesian tool for inverval estimation is the credible
interval. In general, a <em>95% credible interval</em> for some probability
distribution <span class="math inline">\(\pi\)</span> on some set <span class="math inline">\(A\)</span> is a
set of <span class="math inline">\(S \subset A\)</span> such that
<span class="math inline">\(\st a \sim \pi. a \in S\)</span>
with probability <span class="math inline">\(\geq\)</span> 95%,
where the probability is taken over the given distribution. The design
task is usually to minimize the cardinality of the set.</p>
<p>This applies to the interval estimation setting as follows:</p>
<ul>
<li>Start with a parameter space <span class="math inline">\(A\)</span>, an observation space <span class="math inline">\(B\)</span>,
and a <span class="math inline">\(\textrm{likelihood}: A \to \Pr(B)\)</span> as before.</li>
</ul>
<p>In fact, the choice of <span class="math inline">\(\textrm{likelihood}\)</span> function for any given
problem is often common between frequent and Bayesian analyses.</p>
<ul>
<li>We model our existing, pre-experiment knowledge about our problem as
a probability distribution <span class="math inline">\(\pi\)</span> on <span class="math inline">\(A\)</span>, which is called the
<em>prior</em>.</li>
</ul>
<p>In the political example, the prior might be the uniform distribution
on the interval <span class="math inline">\([0,1]\)</span> if we modeled the problem assuming relatively
little knowledge about politics; or it might be a Gaussian
distribution with mean 50% and standard deviation 1 percentage point,
if we modeled the problem assuming pretty strong external evidence
that the election was going to be close. Choice of prior is
important—different priors mathematically encode different problems,
so yield different answers.</p>
<ul>
<li>Given a prior and a likelihood, <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ rule</a>
gives the procedure for finding <em>posteriors</em> conditioned on possible observations
<span class="math inline">\(b \in B\)</span>, which are also probability distributions on <span class="math inline">\(A\)</span>.
The application of Bayes’ rule can be viewed as a function,
<span class="math display">\[\textrm{posterior}: B \to \Pr(A),\]</span>
which updates our prior distribution to reflect learning the information <span class="math inline">\(b\)</span>.</li>
</ul>
<p>In the political example, the posterior distribution describes the
state of our knowledge about the coming election after digesting the poll
results. It’s called the posterior (as opposed to the prior) because
it is the distribution post-experiment.</p>
<ul>
<li>To get an interval estimation procedure, we can compose computing
posteriors with choosing 95% credible intervals to get
<span class="math display">\[\textrm{cred_int}: B \to \mathcal P(A).\]</span>
This <span class="math inline">\(\textrm{cred_int}\)</span> then has the property that
<span class="math display">\[\st a \sim \pi. \st b \sim \textrm{likelihood}(a). a \in \textrm{cred_int}(b)\]</span>
with probability <span class="math inline">\(\geq\)</span> 95%, where the probability is taken over
<em>the prior and the likelihood</em>.</li>
</ul>
<p>In words, this formula means that at least 95% of the time, when <span class="math inline">\(a\)</span>
is drawn according to <span class="math inline">\(\pi\)</span> and <span class="math inline">\(b\)</span> is drawn according to
<span class="math inline">\(\textrm{likelihood}(a)\)</span>, it turns out that <span class="math inline">\(a\)</span> is inside the credible
interval computed from <span class="math inline">\(b\)</span>. The translation to the political example
is direct: in 95% of leaning-poll pairs, where the population leanings
are drawn according to the prior and the poll results are drawn
according to the likelihood, the credible interval computed from the
poll result will contain the true leaning of the population.</p>
<p>One way to render selection of credible intervals as a game is</p>
<ol type="1">
<li>I choose a function <span class="math inline">\(\textrm{cred_int}: B \to \mathcal P(A)\)</span></li>
<li>Nature chooses <span class="math inline">\(a\)</span> according to the prior</li>
<li>Nature chooses <span class="math inline">\(b\)</span> given <span class="math inline">\(a\)</span> according to the likelihood</li>
<li>I win if <span class="math inline">\(a \in \textrm{cred_int}(b)\)</span>.</li>
</ol>
<p>The design task when choosing <span class="math inline">\(\textrm{cred_int}\)</span> is usually to
minimize the cardinality of the sets that it returns, subject to the above
game being won with probability at least 95%.</p>
<h3 id="comparison">Comparison</h3>
<p>Look at these formulae side by side. A 95% confidence
interval for a given parameter space <span class="math inline">\(A\)</span> and a given <span class="math inline">\(\textrm{likelihood}: A \to \Pr(B)\)</span>
is a function from <span class="math inline">\(B\)</span> to <span class="math inline">\(\mathcal P(A)\)</span> such that
<span class="math display">\[ \forall a \in A. \st b \sim \textrm{likelihood}(a). a \in \textrm{conf_int}_{A,\textrm{likelihood}}(b) \]</span>
with probability <span class="math inline">\(\geq\)</span> 95%.</p>
<p>A 95% credibility interval for a given prior <span class="math inline">\(\pi\)</span> over
<span class="math inline">\(A\)</span> and a given <span class="math inline">\(\textrm{likelihood}: A \to \Pr(B)\)</span> is a function from <span class="math inline">\(B\)</span> to <span class="math inline">\(\mathcal P(A)\)</span> such that
<span class="math display">\[\st a \sim \pi. \st b \sim \textrm{likelihood}(a). a \in \textrm{cred_int}_{\pi,\textrm{likelihood}}(b)\]</span>
with probability <span class="math inline">\(\geq\)</span> 95%.</p>
<p>The difference between these two formulations is that the frequentist formula
has a <span class="math inline">\(\forall a \in A\)</span> where the Bayesian one has a <span class="math inline">\(\st a \sim \pi\)</span>. In other words, where
the frequentist analysis assumes an adversary, the Bayesian one postulates a fixed
(probabilistic) behavior. This has several consequences:</p>
<ul>
<li><p>Frequentist statistics answer a different kind of question from
Bayesian ones.</p></li>
<li><p>The frequentist question should be <em>askable</em> in situations where the
Bayesian one is not, namely where the information available about possible <span class="math inline">\(a\)</span>s
cannot be captured as a probability distribution.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p></li>
<li><p>The Bayesian question should be <em>answerable</em> in situations where the
frequentist one is not, because having more than one kind of
quantifier always causes trouble. Here I mean mathematically
answerable; empirically there are circumstances where a forall is
computationally more tractable than a probability distribution.</p></li>
<li><p>When a situation is modelable in both styles, one would expect the
answer to the Bayesian style of question to be more optimistic than
the frequentist, because the adversary is assumed to always choose
the worst possible <span class="math inline">\(a\)</span>. Sometimes, when the prior encodes more
information than we are actually justified in assuming, optimism can
lead to incorrect conclusions. Other times, when “all” possibilities
admit arbitrarily extraordinary coincidences, pessimism can lead to
conclusions so weak as to be paralyzing.</p></li>
<li><p>What question, exactly, “the frequentist question” actually is
depends very strongly on the details of the game design: where the
foralls/adversaries actually go, and in what order the choices are
made. For example, it is important that the confidence interval function is
to be chosen before the adversary chooses the <span class="math inline">\(a\)</span> at which to test
it.</p>
<p>In practice, the choice of how to encode a given complex
statistical situation as a frequentist adversarial game can be
just as contentious as the choice of prior in a Bayesian analysis.</p></li>
</ul>
<h2 id="reflection">Reflection</h2>
<p>A quantifier indicates the “quantity” of things
about which something is true. In this sense, all three of the symbols
that appear in this essay are quantifiers—<span class="math inline">\(\exists\)</span> is “at least one”,
<span class="math inline">\(\forall\)</span> is “all”, and <span class="math inline">\(\st\)</span> is “several”. The
game theoretic view, however, exposes a distinction
between <span class="math inline">\(\exists\)</span> and <span class="math inline">\(\forall\)</span> as opposed to <span class="math inline">\(\st\)</span>. The former two
are optimization quantifiers: they define the value they bind by
specifying a set of possibilities and an objective function, and
assuming the optimal value for that objective is somehow found.
<span class="math inline">\(\exists\)</span> and <span class="math inline">\(\forall\)</span> differ only in the objective (to make the
subsequent formula true or false, respectively), so even in the game
theoretic sense they are very similar creatures.</p>
<p>The random choice quantifier <span class="math inline">\(\st x \sim P\)</span> is different. The
<span class="math inline">\(x\)</span> is not an extremum of anything; it is chosen at random. And yet
<span class="math inline">\(\st\)</span> is also the same as the classical quantifiers, in that
it also hides the details of how <span class="math inline">\(x\)</span> is chosen—this time behind the
probability distribution <span class="math inline">\(P\)</span> (which could be very complicated, and
very difficult to actually select a value from computationally).<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>I wonder, then, what other
complex processes would it be worth making symbols for? One
thought is that <span class="math inline">\(\exists\)</span> takes on a slightly different meaning in
intuitionistic rather than classical logic. I’m a bit fuzzy on the
details, but I guess it corresponds to computable optimization (rather
than the absolute optimization of classical <span class="math inline">\(\exists\)</span>). Does it make
sense to ask for computationally limited optimization? Something like
a game where the player is permitted only a polynomial amount of
computation after seeing the last move before having to make theirs?</p>
<h2 id="notes">Notes</h2>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      R: "{\\mathbb{R}}",
      Z: "{\\mathbb{Z}}",
      eps: "\\varepsilon",
      st: "\\unicode{423}",
      sim: "\\propto"
    },
    equationNumbers: { autoNumber: "AMS" },
    noErrors: { disabled: true },
  }
});
</script>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>The embedding can be generalized to formulae that are not in
prenex normal form. Just treat each quantifier as a move by a
player whose goal is to make the expression in that quantifier’s scope
come out true (for <span class="math inline">\(\exists\)</span>) or false (for <span class="math inline">\(\forall\)</span>). The truth is
dependent upon the values already chosen by all quantifiers in scope
at that point (which values the player knows). I will, however, stick
with prenex formulae in the main text, because they are
easier to think about.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>When we say “order of choices”, what we are actually talking about
is the information available to a decision maker about the results of
other decisions in the game. Chronology is a potent metaphor for
capturing one pattern of information flow, namely complete knowledge
about choices made “in the past” and complete absence of knowledge
about choices that remain to be made “in the future”.
Non-chronological information structures are possible, however. For
example, in a three-player game, A might make some move, then B might
make some move knowing what A did, but then C might have to move
knowing what B did but not knowing what A did (except to the extent
that it can be inferred from B’s activities).</p>
<p>Logic generally does not try to encode such patterns, perhaps
because they tend to make the games even more difficult to solve. One
pattern has been recognized by some authors, however: a “branch
quantifier” is when the adversary and I are to make “simultaneous”
choices, each not knowing what the other has chosen.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Colophon: The glyph <span class="math inline">\(\st\)</span> is the capital <a href="https://en.wikipedia.org/wiki/%C6%A7">reversed
S</a>. That letter is called <code>LATIN CAPITAL LETTER TONE TWO</code> in
Unicode (code point 423), and has <code>&amp;#423;</code> for a numeric HTML entity reference.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The reason for the name “frequentist” is that this is the
kind of statistics one is forced into if one subscribes to the
frequentist justification for probability theory. In a nutshell, the
frequentist philosophical view is that probability theory legitimately
describes only situations that correspond in a reasonable way to
repeatable experiments with variable outcomes, where the probabilities
are the frequencies (hence the name) of observed results. Probability
therefore cannot, on this view, be applied to unique situations such
as the true value of some parameter of interest. Given that the
parameter is nonetheless unknown, one resorts to reasoning about what
one can say for all possible values of the parameter.</p>
<p>Bayesian statistics, in contrast, relies on the more permissive
view (now associated with the 18th-century philosopher
Thomas Bayes, hence the name) that probability theory is an extension
of logic to propositions whose truth is not known with certainty.
Under this view, there is nothing wrong with treating things like
fixed but unknown parameters probabilistically, so no foralls are
necessary.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>This function is actually a probability distribution
over <span class="math inline">\(B\)</span> conditioned on a value from <span class="math inline">\(A\)</span>. The reason it’s called a
“likelihood” and not a “probability” is because in this use case we
are interested in its behavior over the space <span class="math inline">\(A\)</span>, with a value in <span class="math inline">\(B\)</span>
held fixed. Holding <span class="math inline">\(b\)</span> fixed, it measures how
good—“likely”—various <span class="math inline">\(a\)</span> look, but it does not give a probability
distribution over <span class="math inline">\(A\)</span>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>I stress that this situation is rarer than one might
think, because many collections of information are capturable as
probability distributions, without requiring appeal to repeated
experiments with known mechanisms. In particular, the Bayesian
statistics community has derived priors for many problems with the goal
to “let the data speak of themselves”—to wit, encode no additional
information at all, beyond the modeling assumptions encoded in the
<span class="math inline">\(\textrm{likelihood}\)</span> function. I encourage the reader to look up
“uninformative priors” or “reference priors”.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Perhaps it should not be surprising that students get
confused by frequentist statistics. That field treads the relatively
unexplored ground of mixing different kinds of quantifiers in a single
theory. Besides game theory, it is the only theory I know about that
does so; and since both of them are pretty new, perhaps we haven’t
worked out good ways to think about such mixtures, or to teach people
to think about such mixtures.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Tue, 13 May 2014 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2014/stochasticity-is-a-quantifier/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Hard Work <em>and</em> Success</title>
    <link>https://alexey.radul.name/ideas/2014/hard-work-and-success/index.html</link>
    <description><![CDATA[<p>This essay is inspired by the assertion that hard work is more
important for success than being smart. There are, of course, many
meta-objections one could have against such an assertion, especially
in the context in which I heard it. For example, one could notice that
the speaker, being American, might be culturally obligated to say
something like this. Or one could observe that speaking to an audience
of people who wish to be told how to behave in order to be successful
forces suggesting something the audience will think they can actually
do. Or one could reflect on who benefits from a general milieu that
encourages working hard. I am not concerned with these meta-objections, though.</p>
<p>In support of this assertion, the particular speaker adduced the empirical
observation that most successful people work hard rather than are smart;
and also that plenty of smart people do not work hard and are not
successful. This sounds compelling on the surface, but is still open
to objection. First, of course, the data being from a survey, the
results will be much biased by whether successful people feel
obligated to downplay their intelligence (and privilege) in favor of
the more culturally acceptable hard work; but there is also the issue
that even if all information transmission mechanisms are perfect,
looking only at successful people cannot provide adequate information
about the factors that matter to success, because it neglects to
account for the prevalence of those factors in failure. This essay
consists of a worked example illustrating this point.</p>
<p>Let us take a somewhat fanciful little model, and
see what features it has. Suppose success is driven by both
talent and hard work, and also chance, in the following way: Let all
individuals be born with some normally-distributed level of talent.
Suppose that there is some threshold of talent, say 3 standard
deviations out, that is required for success—that is, people whose
talent exceeds the mean by 3<span class="math inline">\(\sigma\)</span> or more may be successful, and
others will not. Suppose society calls these people “smart”. But let
there be two wrinkles: suppose also that any individual can choose to
work hard, which imposes some cost but increases their effective
talent by one <span class="math inline">\(\sigma\)</span>; and suppose furthermore that among all the people
whose effective talent exceeds 3<span class="math inline">\(\sigma\)</span>, 10% succeed at random and the
rest fail. In equations:</p>
<p><span class="math display">\[\begin{eqnarray*}
  talent &amp; = &amp; N(0, \sigma)\\
  talent_{eff} &amp; = &amp; 
    \begin{cases}
      talent &amp;           \mbox{if slacking off}\\
      talent + \sigma &amp;  \mbox{if working hard} \end{cases} \\
  P(\mbox{success}) &amp; = &amp;
    \begin{cases}
      0.1 &amp;  \mbox{if } talent_{eff} &gt; 3\sigma\\
      0   &amp;  \mbox{otherwise}\end{cases}\\
\end{eqnarray*}\]</span></p>
<p>If we furthermore suppose that i) everyone wants to succeed, ii) no
one wants to work hard if it will have no effect on their success, and
iii) everyone knows their endowment (and all the rules), then the
following things will happen:</p>
<ul>
<li>0.13% of people will be called “smart” (talent &gt; 3<span class="math inline">\(\sigma\)</span>), but
none of them will work hard (why bother?)</li>
<li>2.15% of people will have 2<span class="math inline">\(\sigma\)</span> &lt; talent &lt; 3<span class="math inline">\(\sigma\)</span>,
so will work hard and be eligible to succeed.</li>
<li>The remaining 97.72% of people will not work hard and will not
succeed.</li>
</ul>
<p>After the randomness of success is taken into account, the
distribution of results will look like this (as percentages of the
overall population):</p>
<table style="width:69%;">
<caption>Outcomes for our hypothetical population</caption>
<colgroup>
<col style="width: 41%" />
<col style="width: 15%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr>
<th></th>
<th>Succeeded</th>
<th>Failed</th>
</tr>
</thead>
<tbody>
<tr>
<td>Smart and worked hard</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Smart and didn’t work hard</td>
<td>0.013%</td>
<td>0.117%</td>
</tr>
<tr>
<td>Not smart and worked hard</td>
<td>0.215%</td>
<td>1.935%</td>
</tr>
<tr>
<td>Not smart and didn’t work hard</td>
<td>0</td>
<td>97.720%</td>
</tr>
<tr>
<td>Total</td>
<td>0.228%</td>
<td>99.772%</td>
</tr>
</tbody>
</table>
<p>But we are tempted to examine the data selectively. If we
look just at people who succeeded, it looks like this:</p>
<table style="width:50%;">
<caption>Concomitants of success in our model</caption>
<colgroup>
<col style="width: 35%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr>
<th></th>
<th>Succeeded</th>
</tr>
</thead>
<tbody>
<tr>
<td>Smart</td>
<td>5.7%</td>
</tr>
<tr>
<td>Not smart but worked hard</td>
<td>94.3%</td>
</tr>
<tr>
<td>Total</td>
<td>100%</td>
</tr>
</tbody>
</table>
<p>That’s a pretty compelling-looking case for hard work. What if we do
some homework and include people who obviously could have succeeded
but didn’t (that is, those who were smart to begin with)?</p>
<table style="width:62%;">
<caption>Concomitants of potential to succeed in our model</caption>
<colgroup>
<col style="width: 35%" />
<col style="width: 15%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr>
<th></th>
<th>Succeeded</th>
<th>Failed</th>
</tr>
</thead>
<tbody>
<tr>
<td>Smart</td>
<td>3.8%</td>
<td>33.9%</td>
</tr>
<tr>
<td>Not smart but worked hard</td>
<td>62.3%</td>
<td>no data</td>
</tr>
<tr>
<td>Total</td>
<td>66.1%</td>
<td>33.9%</td>
</tr>
</tbody>
</table>
<p>If anything, that makes the case for hard work seem even stronger. But
what’s the real story? If you weren’t born into this world yet, would
you rather be born smart or commit to working hard? <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p><span class="math display">\[\begin{eqnarray*}
  P(\mbox{success}|\mbox{smart}) &amp; = &amp; P(\mbox{success}|talent_{eff} &gt; 3\sigma) \\
  &amp; = &amp; 0.1 = 10\% \\
  P(\mbox{success}|\mbox{work}) &amp; = &amp;
    P(talent &gt; 2\sigma) \cdot P(\mbox{success}|talent_{eff} &gt; 3\sigma) \\
  &amp; = &amp; .0228 \cdot 0.1 = 0.228\%
\end{eqnarray*}\]</span></p>
<p>I know which of those chances I would rather take. <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>How did our examination of the data lead to such poor intuitions about
what actually happens in this world? Because those examinations were
constrained by a pretty harsh filter: looking at only the successful,
or only those who were smart or successful, shows us only 0.228% or
0.345% of this population, respectively. Since the factors we are trying to
study affect individuals’ membership in the group we observe, the selection
grossly distorts our statistics.</p>
<p>This is not to say that I am encouraging my dear readers to slack
off—on the contrary, working hard at the proper kind of work can be
<a href="http://www.slate.com/articles/technology/technology/2014/01/do_what_you_love_love_what_you_do_an_omnipresent_mantra_that_s_bad_for_work.html">its
own reward</a>. This is just a cautionary tale about statistical
arguments, and when one ought to find them convincing.</p>
<h2 id="notes">Notes</h2>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      R: "{\\mathbb{R}}",
      eps: "\\varepsilon"
    },
    equationNumbers: { autoNumber: "AMS" },
    noErrors: { disabled: true },
  }
});
</script>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Note that there
is a fine point in the last question: in this model, the only people
who actually do work hard are the ones for whom it makes a difference,
so working hard is not independent of being at least a little smart.
But we need to artificially separate these two variables to judge the
value of hard work as such, so we need to assume a hypothetical person
who always works hard regardless of their endowment.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Note that even though in the abstract, smarts are much more
important to success in this model than hard work, hard work is still the
only thing an individual can affect, so “work hard” is still good
advice—only the justification “it matters more than talent” is
wrong.</p>
<p>Actually, whether it’s good advice or not can be a mathematical
question too, if we specify how obnoxious hard work is. If I am
giving advice to a completely random individual from this world, then
as far as I know, P(2<span class="math inline">\(\sigma\)</span> &lt; their talent &lt; 3<span class="math inline">\(\sigma\)</span>) is 2.15%; so if
succeeding is at least 500 times better than working hard is bad, my
expectation of their well-being increases if they decide to work hard.
That ratio goes down significantly if we consider that maybe they
wouldn’t have asked me for advice if they didn’t sense that they at
least had some chance of becoming successful; etc.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>What about being born lucky? That is, imagine an individual
to whom success will come automatically if they manage to get their
effective talent above 3<span class="math inline">\(\sigma\)</span>. For them, if they work hard when
appropriate instead of leaning exclusively on their luck,</p>
<p><span class="math display">\[ P(\mbox{success}|\mbox{luck}) = P(talent &gt; 2\sigma) = 2.28\% \]</span>
so in this particular world, it’s better to be smart than lucky too.
But that’s very dependent on the choice of parameter values.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Sat, 29 Mar 2014 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2014/hard-work-and-success/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Testing Revisited</title>
    <link>https://alexey.radul.name/ideas/2014/testing-revisited/index.html</link>
    <description><![CDATA[<p>Test Driven Development, as it is now called, is something I have
<a href="http://web.mit.edu/~axch/www/programming_habits.html">advocated</a>
for a large part of my career. My more recent experience, however,
has brought more nuance to my view of the proper level and kind of
software testing, which now feels ripe for setting down.</p>
<p>The driving observation is that automated tests bear a cost. Tests
are code, and like all code they require maintenance. In the case of
tests, this cost comes in the form of making it harder to change the
interfaces against which the tests are written. When such a change is
made intentionally, the tests need to be updated; and too many times
I have seen situations were doing so was even more difficult than
updating the production code, because it was not clear what, exactly,
the test was actually testing.</p>
<p>Don’t get me wrong—testing software is by and large done too little
rather than too much. If the interfaces you test against are
sufficiently obvious that you know you got them right, then the cost
of test maintenance will be minimal. And aggressive testing improves
the shape of the underlying program, because it forces there to <em>be</em>
interfaces around parts that are sufficiently complex to be tested in
isolation.</p>
<p>That said, I think it is too easy for a carelessly cultivated test
suite to grow weeds—too many tests that each test too little. In
particular, if a project is only tested with examples (to wit, in
exact situation X, the system should do this, that and the other), the
developers can easily lose track of what exactly each of those
detailed situations were supposed to stress; and when an interface
changes, get caught in a bind between fearing to throw a potentially
valuable test out, and not knowing how to adapt it to the new order.</p>
<p>I find that property-based testing goes a long way to alleviate this
potential problem. The idea with
property-based testing is that you specify, as code, properties that
the system under test should satisfy under all (or a wide range of)
circumstances, and the test framework takes it upon itself to generate
circumstances, check the properties, and let you know if any of them
ever failed. Typically, “circumstances” are different inputs to some
function under test, and are typically generated randomly; Haskell’s
<a href="https://hackage.haskell.org/package/QuickCheck">QuickCheck</a>
and its <a href="https://en.wikipedia.org/wiki/QuickCheck">many ports</a> are the prototypical examples of this
style. Randomized property testing covers many testing situations,
but in principle it is also possible to use property tests with model
checkers (that systematically test the system against all “small”
inputs, a la <a href="https://hackage.haskell.org/package/smallcheck">SmallCheck</a>), or white-box input generators (that
instrument the tested code and try to poke it in ways that elicit
different code paths), or even theorem provers (that prove that (a
model of) the system cannot violate the stated property).</p>
<p>For example, if one is developing a web application, one might wish to
ensure that users interacting with it do not cause the server to
crash. One way to do so is to monitor the live server, and every time
it crashes, investigate what happened, and encode that situation in a
regression test that does that thing and checks that the server didn’t
crash. Then the server will stop crashing for that reason as long as
tests are kept passing. This is a fine strategy, but it can be
greatly augmented by adding a randomized test that simulates random
user actions (including putting random data into web forms in the
application, etc) and reports any such sequences that cause the
server to crash.</p>
<p>Property-based
tests strike me as much easier to maintain than large collections of
example-based tests. First off, one property test usually does the
work of very many example tests, because the actual examples are
generated automatically. Second, even more important, the content of
a property test is clearer, because it is separated from the
specification of the circumstance in which that content is being
checked.</p>
<p>On the other hand, property-based testing alone is also unlikely to
fully serve a project’s needs, because “do the right thing” can be
very hard to capture as an executable specification. So some examples
are invaluable, because they can check that everything went right in a
particular situation, even things that are difficult to pin down as
being right until they go wrong. Also, I expect it to be quite rare in
practice for a random process to generate really thorough coverage of
all circumstances that will occur in real use; so when something bad
happens to the live system, encoding that particular situation as a
regression test can be a good idea.</p>
<p>So how should one combine these styles? Judgement is of course called
for. If you have a user story that you want to make sure works,
that’s probably an example-based test. If you are chasing some bug
and the investigation makes you go “X shouldn’t have happened”, then
perhaps “X never happens” is a good property-based test. If you find
yourself writing a second test whose purpose is to check the same
thing as an existing test checks, but under different circumstances,
then maybe those should be replaced with a property. And, of course,
the line between the two can be pretty blurry: your user story can
become a suite of property tests where you check that any reordering
and variation of some collection of user actions all achieve some
aspect of the goals in that user story (and do not crash the
application).</p>
<p>As generic starting point, though, I suggest really taking seriously
the idea of a test suite as executable documentation:</p>
<ol type="1">
<li>Good documentation should contain enough usage examples to teach
how the software is to be used. The test suite should contain
those same examples as unit tests, checking that they do what the
documentation says they do.</li>
<li>Good documentation should also describe general properties of the
system, such as the space of valid inputs. These should turn into
property-based tests.</li>
</ol>
<p>Ideally, between the above two types of tests, the project will
achieve 100% documentation coverage—every assertion made in the
documentation accurately reflected in the testing process.</p>
<ol start="3" type="1">
<li>Finally, if the system lives long enough to experience nontrivial
bugs, it makes sense to have a collection of regression tests,
where each test is a reproduction of a circumstance that led to a
bug and an assertion of the correct behavior in that circumstance
(which could be implemented in terms of checking properties).</li>
</ol>
<p>It seems reasonable to me to keep these three kinds of tests separate
from each other. Doing so would be a way to approach the real goal,
which is that looking at any given test should make it obvious what
that test was trying to test (for example, which sentence in the
documentation is being verified by a given test). That way, tests
become easy to adapt to intentional changes in the system they are
testing.</p>
<p>In particular, I think that as the underlying software evolves, the
regression suite should be treated lightly (in the <a href="https://senseis.xmp.net/?Light">Go player’s sense
of that word</a>): if an intentional interface change breaks a regression
test against that interface, then it’s quite possible that the old bug
is nonsensical now, in which case the regression test should just be
thrown out. The other tests, in contrast, should almost certainly be
updated in response to changes to the interfaces they are testing.</p>]]></description>
    <pubDate>Sat, 18 Jan 2014 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2014/testing-revisited/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Consider <em>the</em> Cheeseburger</title>
    <link>https://alexey.radul.name/ideas/2013/consider-the-cheeseburger/index.html</link>
    <description><![CDATA[<p>Advertising. People in my social circle are used to thinking of it as being an
awful institution: greedy, faceless, unscrupulous corporations
brainwashing the susceptible masses into wasting their hard-earned
money on cheap, low quality, superfluous devices for solving problems
no one actually has and that don’t even work. An irritation at best,
sheer deceit and manipulation at worst. But in fact, what evils
afflict the institution of advertising are reflections of its poor
implementation, not of its superfluity. The reason advertising exists is
that it is chewing on one of the most important problems in an economy
as large as ours: You have 300,000,000 people in the country, which have
needs, desires, and abilities. How do you arrange to take the best
advantage of their abilities, do the best job of meeting their needs,
and turn a reasonable eye towards their desires, too?</p>
<p>Consider the cheeseburger: someone raised a cow, someone slaughtered
it, and someone shipped its meat to various places; someone grew some
corn, someone made it into oil, and someone transported the oil hither
and yon; someone mined some iron ore, someone smelted it and made
iron, someone mixed it in a furnace with carbon, someone crafted it
(who knows how) into shape, someone made frying pans out of some of
it, someone made a stove out of some, and someone transported the
frying pans and the stove; someone else grew some wheat, someone
ground it into meal, someone cultivated some yeast, someone cooked it
into buns, and someone shipped all this to and fro and eventually to
the same place as the stove, the pans, the oil, and the meat; someone
grew tomatoes, someone else grew lettuce, someone else grew onions,
and these too were all brought there; someone grew cucumbers, someone
mined salt, someone brined the cucumbers in the salt to make pickles,
and someone brought them; someone raised a milk cow, someone milked
it, someone extracted rennet out of another cow, someone developed a
good strain of cheese microbes, someone cultured the milk into cheese,
and someone brought the cheese; someone mined coal, someone built a
steam turbine, someone bruned the coal, someone built electric
transmission lines, and someone wired up that stove to those
transmission lines in that place; and finally someone cooked the meat
in the oil on the pan on the stove, melted the cheese onto it, cut the
tomatoes, onions, lettuce and pickles, put them all between the buns,
and gave them to you. Never mind the copper for the transmission
lines, the vehicles for transportation, the fuel to run them, the
materials and construction for the roads for this transportation and
for the building in which you were given this cheeseburger, and
everything that goes into making enough cheeseburgers to feed all
these people. And why did the world do this herculean thing? Because
it guessed, more or less right, that you would have a hankering for a
cheeseburger just then, and that it was worth mobilizing all that
mechanism to satisfy you.</p>
<p>Of course, none of this would be possible at all if not for the fact
that raising one cow for meat contributes to a vast number of
cheeseburgers; and indeed, everything I mentioned has plenty of uses
besides the one cheeseburger that touched it in this little vignette.
But I hope I convinced you of the fundamental immensity of the
coordination problem. The cooperation problem, namely getting all the
people involved in this story to want to help you get your
cheeseburger, is more or less solved by money: each of them will get
paid some small fraction of the price of your cheeseburger, and given
the number of cheeseburgers they helped with their respective actions,
those small fractions will add up to a more or less decent salary;
which in turn will allow them to command similar help from similar networks
to satisfy needs or wishes of their own. But what about the
coordination problem? How does the guy with the cows know how many to
raise and how many to slaughter? How does the guy in the truck know
where to go to pick things up and where to go to drop them off? And
how do you know that you could come to this place and receive this
cheeseburger? This is a tremendous, mammoth problem of communication,
historically approached by guesses, small meetings, and public
displays (read: store fronts for centuries, also bill boards now).
Now that we have invented technology to make communication easier and
faster, what is the biggest communication-related economic problem
there is to solve? Are you surprised that advertising online is huge
business?</p>
<p>That said, the current model is primitive. The effort of doing the
communication is funded exclusively by the sellers of goods and
services, and takes the form of a more or less targeted broadcast,
which it is up to the buyers to filter for what they want. Being
seller-dominated, these broadcasts are, from the perspective of the
buyers, biased, unreliable, and far too numerous. The problem that
this system is trying to address, however poorly it addresses it
relative to potential, is nonetheless one of the vital underpinnings
of our society.</p>]]></description>
    <pubDate>Thu, 31 Oct 2013 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2013/consider-the-cheeseburger/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title><em>How to</em> Take CSS <em>to the</em> Next Level</title>
    <link>https://alexey.radul.name/ideas/2013/taking-css-to-the-next-level/index.html</link>
    <description><![CDATA[<p>Computationally generating the semantics of a web page—the
HTML—has become the standard modus operandi on the web. No serious
web developer writes by hand the HTML that their server sends to the
browser anymore; invariably, some template engine or other generator
intermediates, removing tedium and adding flexibility and power. The
same thing is slowly happening to the presentation layer as well—the
CSS. This article discusses the high technology that can be brought
to bear on computed web layout and styling, both as an advertisement
for some impressive existing tools and as a call to action for
toolsmiths to fill the gaps I observe.</p>
<h2 id="contents">Contents</h2>
<ul>
<li><a href="#compute-with-variables-and-functions">Compute with Variables and Functions</a></li>
<li><a href="#read-the-dom">Read the DOM</a>
<ul>
<li><a href="#a-javascript-runtime-can-do-anything">A JavaScript Runtime can do Anything</a></li>
<li><a href="#take-functional-reactive-programming-to-the-web">Take Functional Reactive Programming to the Web</a></li>
<li><a href="#compile-magic-variables-to-media-queries">Compile Magic Variables to Media Queries</a></li>
<li><a href="#compile-more-magic-to-many-queries">Compile More Magic to Many Queries</a></li>
</ul></li>
<li><a href="#solve-linear-constraints">Solve Linear Constraints</a>
<ul>
<li><a href="#a-javascript-runtime-can-do-that-too">A JavaScript Runtime Can Do That, Too</a></li>
<li><a href="#intermezzo-constraint-programs-always-guessright">Intermezzo: Constraint Programs Always Guess Right</a></li>
<li><a href="#solve-static-constraints-at-compile-time">Solve Static Constraints at Compile Time</a></li>
</ul></li>
<li><a href="#solve-discrete-constraints-too">Solve Discrete Constraints, Too</a></li>
<li><a href="#explain-solutions-of-constraint-systems">Explain Solutions of Constraint Systems</a></li>
<li><a href="#take-css-to-the-next-level">Take CSS to the Next Level</a></li>
</ul>
<h2 id="compute-with-variables-and-functions">Compute <em>with</em> Variables <em>and</em> Functions</h2>
<p>Variables and functions are essential tools for writing anything in
any language, and it is high time CSS acquired them. The world has
known how to implement subroutines for over fifty years, and lexical
scope for nearly forty, so it is with great pleasure that I refer the
gentle reader to <a href="http://sass-lang.com/">SASS</a> and
<a href="https://learnboost.github.io/stylus/">Stylus</a>. Each is a polished,
productized tool that finally makes CSS an actual language, and all
you have to do is insert a code generator into your workflow.</p>
<p>I haven’t actually used SASS or Stylus much myself, but I’ve looked
over the documentation, and I am impressed with how careful they both
are to just drop in to a develper’s existing workflow. Standard CSS
is valid input for either of these tools and compiles to itself.
Additional constructs like variables, functions, arithemtic, and so on
are all resolved completely during compilation, producing normal,
standards-compliant CSS. Even your web server doesn’t get any
additional load from your using one of these tools, and the user’s
browser is certainly none the wiser.</p>
<p>I salute the work that Stylus and SASS represent and the benefits they
bring. I have, however, nothing to say about the technology that
makes them possible—as far as I can tell, everything there is pretty
standard, well-tested programming language stuff. Buckle your
seatbelt: the ride is about to get wilder.</p>
<h2 id="read-the-dom">Read <em>the</em> DOM</h2>
<p>Every web developer is painfully aware of the problems caused by their
audiences viewing their web pages on screens of wildly different size
and resolution, and now even different precision of interaction (mice
are way easier to aim with than touch screens). Every web developer
is also painfully aware of the impracticality of developing completely
separate versions of a web page for each possible device—it’s tons
of work, and keeping them in sync is a nightmare.</p>
<p>Responsive web design is a philosophy of trying to build single web
designs that, by incorporating appropriate points of flexibility,
automatically and gracefully adapt to being viewed on various devices.
If styling is a real language, why not introduce magic variables whose
values depend on the user’s context? It would make responsive design
so much easier!</p>
<h3 id="a-javascript-runtime-can-do-anything"><em>A</em> JavaScript Runtime <em>can do</em> Anything</h3>
<p>If one goes the JavaScript route, it becomes possible for responsive
design to respond not just to gross features of the user’s device,
such as the width of their screen, but to subtle consequences of the
user’s choices. For example, suppose you want to adjust the style of
some piece of text depending on whether it fits on at most two lines:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode css"><code class="sourceCode css"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>span<span class="pp">#do-not-be-too-tall</span> {</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">/* Measure the element&#39;s height and line-height with -&gt; syntax */</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  if -<span class="op">&gt;</span>height <span class="op">&gt;</span>= 3 <span class="op">*</span> -<span class="op">&gt;</span>line-height</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    font-size<span class="in">:</span> 90%; <span class="co">/* and try to fit in two lines */</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>That depends not only on the width of the screen, but also on the font
size (which the user can change), the actual font (which will depend
on the fonts available on the user’s system), and the decisions made
by the browser’s hyphenation, justification, and line breaking
algorithms. No pure-CSS design can be that responsive—there aren’t
enough media queries for your CSS to be able to discern all these
relevant variables, even if you could simulate the browser’s rendering
engine well enough to decide at compile time what to do in each
configuration.</p>
<p>Such responsiveness can, however, be implemented with a JavaScript
runtime system that, on window load and other relevant events, examines the
rendered metrics of appropriate DOM elements and adds or removes
appropriate styles in response. There are nice efficiency problems
here, for intance to decide which properties of which elements need to be
inspected under which circumstances, and which computations ought to
be redone in order to decide which properties to adjust.</p>
<h3 id="take-functional-reactive-programming-to-the-web"><em>Take</em> Functional Reactive <em>Programming to the</em> Web</h3>
<p>One direction in which the JavaScript path can lead is full-blown <a href="https://en.wikipedia.org/wiki/Functional_reactive_programming">functional
reactive programming</a>
on the web. In brief, functional reactive programming (FRP) starts
with viewing user inputs as either time-varying signals (like the
position of the mouse) or chronological event streams (like mouse
clicks). The big idea is that functions written as though they
operate at a single instant in time can be lifted to operate sensibly
on these signals, and the job of updating state (which would normally
end up being a mess of callbacks) can be handled smoothly and
automatically by the runtime system. A <a href="http://www.flapjax-lang.org/demos/index.html#follow">classic
example</a> would be
writing code that looks like</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode css"><code class="sourceCode css"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#mouse-follower</span> {</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">position</span><span class="ch">:</span> <span class="dv">absolute</span><span class="op">;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">left</span><span class="ch">:</span> %<span class="dv">mouse</span>%<span class="dv">-</span>&gt;<span class="dv">left</span><span class="op">;</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">top</span><span class="ch">:</span> %<span class="dv">mouse</span>%<span class="dv">-</span>&gt;<span class="dv">top</span><span class="op">;</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>and having that compile to some JavaScript that causes the item with
id <code>mouse-follower</code> to follow the mouse around on the screen—without
the developer writing any callbacks or mouse event handlers.</p>
<p>I know about two tools that implement FRP for the web, which I highly
recommend you check out: <a href="http://www.flapjax-lang.org/">Flapjax</a> and
<a href="http://elm-lang.org/">Elm</a>. I haven’t had a chance to use them
myself, but the backing technology is really cool. On the other hand,
I expect them not to be as polished as a SASS or a Stylus—functional
reactive programming is a more recent academic development than
something as basic as subroutines, and I know that Flapjax at least
was born as a research project rather than explicitly a product
(though it may have evolved since).</p>
<p>In a sense, FRP-on-the-web subsumes reactive web design. A toolkit that
makes it really easy to write rich user interfaces that smoothly
update themselves in response to mouse movements, keystrokes,
gestures, and clicks should be great for the task of writing user
interfaces that smoothly update themselves when the user changes the
width of their screen or the size of their font—just make those
things be variables also, instead of hardcoding your expectations for
them.</p>
<p><a id="overkill"></a>In a different sense, the needs and constraints
of reactive web design may make web-FRP overkill. FRP worries about
high-frequency events like the whereabouts of the user’s mouse, which
require all sorts of fancy technology to react to efficiently, both at
compile time and at runtime (no way to do FRP on the web without
JavaScript, and plenty of it!). Reactive web design, on the other
hand, is interested in things that change rarely or even not at all
during any given page view—screen width, font size, pointer
precision—but expects extreme performance and portability. Being
able to run on lots of devices is the point, so why rule out devices
that can’t or won’t execute JavaScript fast enough, or even at all?</p>
<h3 id="compile-magic-variables-to-media-queries"><em>Compile</em> Magic Variables <em>to</em> Media Queries</h3>
<p>It seems to me like there is a technology gap between Stylus, which
emits pure CSS but offers no support for adjusting the presentation
based on the user’s context, and Flapjax, which can react to anything
at all, but needs a JavaScript runtime system to even work.</p>
<p>Perhaps one could augment Stylus (or another tool in the
same niche) to permit a limited set of variables whose values are
semantically determined by the user’s browser. If such variables only
affect the actual layout through conditionals, then it should be
possible to compile them to media queries. For example, a variable
called <code>%width%</code> could, semantically, always be set to the width of
the window viewing the page. Then the developer can write something
like</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode css"><code class="sourceCode css"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#sidebar</span> {</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">width</span><span class="ch">:</span> <span class="dv">100</span><span class="dt">px</span><span class="op">;</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  if %width% &lt; 100px</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">display</span><span class="ch">:</span> <span class="dv">none</span><span class="op">;</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>to suppress a 100px wide element if the user’s window is too narrow
for it. The media query implementation of that segment could look
like</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode css"><code class="sourceCode css"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#sidebar</span> {</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">width</span><span class="ch">:</span> <span class="dv">100</span><span class="dt">px</span><span class="op">;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">@media</span> <span class="an">all</span> <span class="kw">and</span> <span class="fu">(</span><span class="kw">max-width</span><span class="ch">:</span><span class="dv">100</span><span class="dt">px</span><span class="fu">)</span> {</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="pp">#sidebar</span> {</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">display</span><span class="ch">:</span> <span class="dv">none</span><span class="op">;</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Note that even this feature needs more than a completely brain-dead syntax translation
because the media query appears outside the selector, and part of the
style does not depend on the media query. Of course, proper
variables should also be able to participate in functions and arithmetic
and such, so the following ought to work for disappearing an element
that should be 100px wide but should occupy no more than half the
screen:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode css"><code class="sourceCode css"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>sidebar_width = 100px</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#sidebar</span> {</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">width</span><span class="ch">:</span> <span class="dv">sidebar_width</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  <span class="dv">if </span>%<span class="dv">width</span>%/<span class="dv">2</span> &lt; <span class="dv">sidebar_width</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="dv">display</span>: <span class="dv">none</span><span class="op">;</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Such a thing ought to turn into</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode css"><code class="sourceCode css"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#sidebar</span> {</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">width</span><span class="ch">:</span> <span class="dv">100</span><span class="dt">px</span><span class="op">;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co">/* System needs to find the inequality %width%/2 &lt; 100px</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co">   and solve it for %width% */</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="im">@media</span> <span class="an">all</span> <span class="kw">and</span> <span class="fu">(</span><span class="kw">max-width</span><span class="ch">:</span><span class="dv">200</span><span class="dt">px</span><span class="fu">)</span> {</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  <span class="pp">#sidebar</span> {</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">display</span><span class="ch">:</span> <span class="dv">none</span><span class="op">;</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><a id="symbolic"></a>Implementing such magic variables properly seems
to me to call first for <a href="https://en.wikipedia.org/wiki/Symbolic_execution">symbolically evaluating</a>
the stylesheet with respect to unknown values of the magic variables
(in order to detect the conditionals they participate in), and second
for solving the inequalities those conditionals represent to discover
the points where the stylesheet’s behavior changes. Those points can
then become the values that the media queries compare against. I
imagine that constant-coefficient linear inequalities ought to be
enough for the needs of web UIs, so this is eminently doable.</p>
<h3 id="compile-more-magic-to-many-queries"><em>Compile</em> More Magic <em>to</em> Many Queries</h3>
<p>A more sophisticated thing would be to allow the magic variables to
influence the actual values in the stylesheet. For example,</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode css"><code class="sourceCode css"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#screen</span> {</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">width</span><span class="ch">:</span> %<span class="dv">width</span>%<span class="op">;</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">height</span><span class="ch">:</span> <span class="dv">9</span>*%<span class="dv">width</span>%/<span class="dv">16</span><span class="op">;</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>would specify a full-width viewport with a 9x16 aspect ratio; which I
do not think is possible with standard CSS, because a height cannot be
specified as a percentage of a width.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>I doubt such a feature as this can be implemented perfectly with media
queries or any other pure-CSS mechanism, but a jaggy version could be
done with no JavaScript. To do it, compute (at compile time) what the style
should be for every value of <code>%width%</code> between 0 and 3000px in 20px
increments, and use media queries to select the one of those that best
matches the user’s window. Choosing those constants ought to be
manual, because it exposes a trade-off between resolution (how close
to the user’s actual window width the layout gets, and how jumpy the
transition is as the window is resized) and the size and rendering
speed of the resulting stylesheet. It would be appropriate to use
something like symbolic evaluation to detect which variables do not,
in fact, affect the layout in a continuous way, so that effort is not
wasted on sampling them like this.</p>
<h2 id="solve-linear-constraints">Solve <em>Linear</em> Constraints</h2>
<p>The things that CSS allows a designer to control conveniently aren’t
actually the things that are important about a layout. It doesn’t
really matter that some <code>div</code> is 397px wide; what matters is that the
layout’s two columns should be the same width, and they should fit
into their container side-by-side, with a gap between them.</p>
<p>A general way to say that is that we want the elements of our
presentation to stand in some relationships with each other (such as
being the same width); and much of the thankless part of web design is about
achieving those relationships with the available tools (such as
specifying exact widths for things). Wouldn’t it be better to just
spell out the desired relationships, and let the computer figure out
how to arrange things to satisfy them?</p>
<p>This phenomenon is not specific to the web, and there has been a
steady trickle of research on so-called constraint-based interfaces
over the years. One system of note is
<a href="https://constraints.cs.washington.edu/cassowary/">Cassowary</a>,
a <a href="https://en.wikipedia.org/wiki/Linear_programming">linear program</a>
solver designed with the needs of user interfaces in mind.
Specifically, the solver is incremental, in the sense that it has
algorithms for very quickly updating the solution to an already-solved
a constraint system under small changes (including adding and removing
constraints). Such re-solving corresponds to updating the UI in response to user actions.
The solver also supports constraints of varying priorities, where the
low-priority constraints get violated if the system as a whole is
otherwise unsatisfiable. Graceful degradation like that is critical for smoothly handling
unexpected or extreme situations.</p>
<p>Constraint-based interfaces seem like a really good idea to me, though
as far as I know, they have yet to really catch on. Maybe the block
was that putting even a little constraint into your design used to
require shifting your whole system to use one of the constraint-based
UI packages, I don’t know. Apple <a href="https://developer.apple.com/library/mac/documentation/UserExperience/Conceptual/AutolayoutPG/Introduction/Introduction.html">seems to be incorporating this
technology</a>,
though.</p>
<h3 id="a-javascript-runtime-can-do-that-too">A JavaScript Runtime can do That, Too</h3>
<p>What might <a href="https://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.4819">constraint-based layout with
CSS</a>
look like? There would of course need to be some syntax for
specifying constraints and preferences; the example of two columns and
a separator might look like this:</p>
<pre><code>require(#left-&gt;width = #right-&gt;width)
require(#left-&gt;width + #right-&gt;width + #gap-&gt;width &lt; #container-&gt;width)
maximize(#left-&gt;width)</code></pre>
<p>And then there would need to be a constraint solver such as Cassowary
somewhere that computes solutions.</p>
<p>In fact, there are projects focused on <a href="https://github.com/slightlyoff/cassowary.js">reimplementing Cassowary in
JavaScript</a> and <a href="http://gridstylesheets.org/">using it
for web layout</a>. Permit me to augment my
earlier recommendation: Check out <a href="http://gridstylesheets.org/">Grid Style
Sheets</a> too. It’s a compiler that
transforms constraint-CSS to instructions for a JavaScript runtime
system to measure the relevant aspects of the DOM and solve the
constraint system. The runtime uses Cassowary’s algorithms to
incrementally update the solution as DOM variables change. As of this
writing, it looks like a very new project, so you would get “all the
rights and privileges” of being an early adopter.</p>
<p>Note that the expressive power of Flapjax (and Elm) is probably not
comparable to that of GSS. GSS allows the UI to be specified with
declarative, potentially cyclic and over-specified constraints that
are automatically solved, whereas Flapjax and Elm require the developer to
decide on the order in which values are computed and write down
functions defining each variable in terms of earlier ones. On the
other hand, Flapjax and Elm can handle variables of all types, including
discrete values and event streams, whereas the Cassowary algorithms
that GSS is based on only solve linear programs, so cannot operate on
discrete variables. This divergence is not an accident: solving acyclic systems
like Flapjax or linear systems like Cassowary is asymptotically fast,
whereas cyclic systems of discrete equations are in general
<a href="https://en.wikipedia.org/wiki/P_versus_NP_problem">NP-hard</a>. I will <a href="#solve-discrete-constraints-too">get back to
this</a>.</p>
<p>Aside: I imagine that Grid Style Sheets does not do either of the
following optimizations, so there may be a good opportunity to improve
its performance, perhaps considerably. First, the browsers’ existing
layout engines already solve certain kinds of constraints natively.
For example, <code>position: absolute; top: 5px</code>
constrains a UI element relative to its enclosing element.
Presumably everything would be more efficient if the compiled
output represented whatever constraints it could in terms of ones the
browsers can solve natively, and relied on JavaScript only for the
unavoidable cases.</p>
<p>Second, given a fixed DOM and stylesheet, it should be possible to
<a href="https://en.wikipedia.org/wiki/Partial_evaluation">partial-evaluate</a>
the JavaScript constraint solving engine with respect
to the actual constraints it is to solve. If this works, the residual
ought to be pretty efficient (especially since the optimization loop should
be implementable as an <a href="http://asmjs.org/">asm.js</a> module). Then
again, this may not be so easy, because JavaScript elsewhere on the
page may do things like add or remove elements that match the given
selectors, changing the constraint set that needs to be solved.</p>
<h3 id="intermezzo-constraint-programs-always-guess-right"><em>Intermezzo:</em> Constraint Programs <em>always</em> Guess Right</h3>
<p>The presence of constraint solving machinery enables a very powerful
and liberating programming style. A tiny amount of linguistic support
suffices to allow the developer to call forth unknown quantities (say,
lengths) whose values will be determined by solving the constraints:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode css"><code class="sourceCode css"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>mylen = a-length<span class="in">()</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#foo</span> {</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  ... use mylen</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="pp">#bar</span> {</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>  ... use mylen some more</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>... require something that constrains mylen</span></code></pre></div>
<p>With this feature, coding feels like you can ask your program to
guess, and depend upon it always guessing right (to wit, in a way that
satisfies the constraints, which can appear later).</p>
<blockquote class="pullquote-display">
<p>
Standard programming requires the developer to diagonalize the
system of constraints in their head before writing the program.
</p>
</blockquote>
<p>Guessing and constraining is important as a programming style in part because
it allows a new form of modularity: the unkown values can be created
and used in one part of a program (module, if you will) and
constrained in another. In other words, some of the relationships
that some variable participates in can be spelled out before the value
of that variable could possibly be known. Standard (read: imperative,
functional, or object-oriented) programming styles, in contrast,
essentially require the developer to diagonalize the system of
constraints in their head before writing the program, so as to be able
to write code that computes each value before it is used. This is bad
not only because it is work, but because such diagonalization impedes
modularity by having to span what could otherwise be module
boundaries.</p>
<h3 id="solve-static-constraints-at-compile-time"><em>Solve</em> Static <em>Constraints at</em> Compile Time</h3>
<p>Grid Style Sheets shares with Flapjax and Elm the potentially
undesirable feature of requiring a JavaScript runtime system to work.
As discussed <a href="#overkill">above</a>, this imposes weight—on the
developer’s toolchain, on portability, and on the user’s ultimate
experience with the page.</p>
<p>There is, however, no reason to solve all the constraints
that may appear in a stylesheet at runtime. If we return to
our example with the two columns,</p>
<pre><code>require(#left-&gt;width = #right-&gt;width)
require(#left-&gt;width + #right-&gt;width + #gap-&gt;width &lt; #container-&gt;width)
maximize(#left-&gt;width)</code></pre>
<p>this system could be solvable completely statically if the gap width
and container width are specified elsewhere in the stylesheet. In
this case, a CSS preprocessor occupying the Stylus niche could just
solve that constraint system at compile time, and emit pure,
JavaScript-free CSS with the solution embedded in it. It could even
be done with an off-the-shelf external linear program solver (like
Cassowary!) running on the developer’s desktop.</p>
<p>Some degree of responsive design could be injected into this with the
<a href="#compile-more-magic-to-many-queries">many queries trick</a>: if you have
a constraint system whose solution depends on the width of the user’s
window but not on anything else about the user’s environment, you
could solve it at compile time for each possible value of that width
(up to some resolution), and emit media queries that choose among
stylesheets embodying the various solutions depending on which point
the true value is closest to. An incremental linear program solver
would offer a substantial benefit to compilation times here.</p>
<h2 id="solve-discrete-constraints-too"><em>Solve</em> Discrete Constraints, <em>Too</em></h2>
<p>My feeling is that linear constraint satisfaction is great, but for
serious styling and layout design, developers really want constraints
on discrete variables too. With such technology, making a navigation
bar appear across the top or down the side depending on where it fits
could look like this:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode css"><code class="sourceCode css"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#navbar</span> {</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  orientation = a-value(above<span class="op">,</span> beside);</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  if orientation = above</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  if orientation = beside</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>... require the navbar to fit ...</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">/* might be automatic with min-width properties and such */</span></span></code></pre></div>
<p>Solving systems of guesses like this translates into an instance of
the <a href="https://en.wikipedia.org/wiki/Boolean_satisfiability_problem">boolean satisfiability
problem</a>.
If such discrete variables interact seriously enough with the linear
variables (for instance, turning linear constraints on or off, or
doing something depending on some inequality over linearly-constrained
variables), then it’s a case of <a href="https://en.wikipedia.org/wiki/Integer_programming">mixed integer-linear
programming</a>.</p>
<p>Even though in general both of these problems are NP-hard, in practice
I cannot imagine any sane layout having enough embedded guesses to
stymie any self-respecting SAT or MILP algorithm. An adequate
algorithm for batch-mode solution of one layout-sized system of linear
and discrete guesses would not be at all difficult to implement.</p>
<p>If the constraints on the guesses do not reference any client-side
parameters (in any way that may affect the solution), the compiler
should just solve the system (maybe with an off-the-shelf solver,
even) and emit CSS specialized to the answers. If the validity of the
guesses depends on client-side values only in a discrete way, it may
be feasible to compute all the relevant regions of parameter space by
a <a href="#symbolic">symbolic evaluation mechanism</a> and emit CSS that checks
what region it’s in using media queries. For systems where that
wouldn’t work, but for only a few client-side parameters, it may also
be possible to use the <a href="#compile-more-magic-to-many-queries">many queries
trick</a> to produce jaggy but
JavaScript-free CSS output.</p>
<p>It is not clear to me that the other implementation strategy, namely
putting a full mixed integer-linear program solver into a JavaScript
runtime system, would be a very good idea. There are no fast
algorithms for solving MILPs. Worse, I don’t know of any incremental
algorithms, namely ones that can quickly adapt the solution of one
MILP to solve a similar one; and re-solving the whole layout every
time anything changes could be prohibitive.</p>
<p>Then again, it could also be just fine. Flapjax, Elm, and Cassowary
uniformly do not do this partly because they were all intended to
support very interactive user interfaces, where computing the layout
had to keep up with the user typing and moving their mouse. A tool
whose goal is just responsive web design and constraint-based layout
may well afford to solve a modest mixed integer-linear program once on
page load plus every time the width of the user’s window changes.</p>
<h2 id="explain-solutions-of-constraint-systems">Explain Solutions <em>of Constraint Systems</em></h2>
<p>With power comes responsiblity. A constraint-based version of CSS
such as I have suggested brings the possibility for a whole new kind
of bug: systems of constraints that are accidentally unsolvable, or
ones that cause the compiler or the runtime to choose a solution that
was not what the developer intended. Debugging such sitations, even
with just a couple dozen constraints, could easily turn into a
nightmare.</p>
<p>Therefore, I think it would be quite important for any CSS tool
along these lines to provide debugging support, specifically in the form of
explanations. There should be some appropriate interface
(perhaps accessible only in code generated with a “debug” flag or
something) that lets the developer select some element and ask “why is
this here?”; or indicate some variable and ask “why does this have
this value?”. The answer to this kind of question consists of the
relevant constraint(s) that forced the variable in question to its
value. Often, constraints become forcing only because of the values
of other variables, so a recursive explanation may be necessary. Such
explanations also apply to the question “why is this constraint not
satisfiable?”</p>
<blockquote class="pullquote-display">
<p>
Implementing linear or integer solvers that actually pay
attention to the sparse structure of the problem and use it to explain
the solution would be both nontrivial and an advance.
</p>
</blockquote>
<p>Making such explanations would actually be some amount of work.
Formalisms like linear programming and integer programming culturally
assume the problem is dense: there are variables, there are
constraints, and both the formalisms and the usual algorithms assume
that every variable might as well appear in every constraint. In
practice, though, actual problems tend to be pretty sparse: usually
any given constraint will only touch a handful of variables, and any
given variable will only participate in a handful of constraints. So
the question “why did this variable get this value” actually has a
richly structured answer: only a handful of constraints were
immediately relevant, and an only somewhat larger handful of variables
were relevant to those constraints, etc.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>It may be possible to recover the sparse structure of the problem by
augmenting the solution algorithm to record, in some appropriate
datastructure, the pieces of information each of its solution actions
depended upon. Then, reading the relevant portion of such a
dependency trace backwards may constitute a reasonable explanation for
a given part of a solution. Then again, it may even be possible to
construct explanations just by examining the solution and the
problem—the value of <code>x</code> in the optimal solution is this because
trying to increase <code>x</code> would voliate constraint foo, given the values
of <code>y</code> and <code>z</code>; the value of <code>y</code> in the optimal solution is that
because, etc. It may also turn out that compelling explanations
require a mixture of both techniques, or that one kind of technique
works well for discrete systems and another for linear ones. In any
case, implementing linear or integer solvers that actually pay
attention to the sparse structure of the problem and use it to explain
the solution would be both nontrivial and an advance.</p>
<h2 id="take-css-to-the-next-level">Take CSS <em>to the</em> Next Level</h2>
<p>If you are a practicing web developer, you can take your own web
designs to the next level by trying one of the tools I have been
talking about. In the case of Flapjax, Elm, or GSS, it may take some
time to get used to the design possibilities they offer, but once your
mind has been stretched by that new idea, I do not expect it to ever
regain its original dimensions.</p>
<p>As a recap, here is the space of tools as I see it:</p>
<table style="width:70%;">
<caption>Feature space of existing next-level CSS tools.</caption>
<colgroup>
<col style="width: 32%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr>
<th>Feature</th>
<th>Stylus</th>
<th>Flapjax</th>
<th>GSS</th>
</tr>
</thead>
<tbody>
<tr>
<td>JS-free usage</td>
<td><strong>yes</strong></td>
<td>no</td>
<td>no</td>
</tr>
<tr>
<td>Continuous Computations</td>
<td><strong>yes</strong></td>
<td><strong>yes</strong></td>
<td><strong>yes</strong></td>
</tr>
<tr>
<td>Discrete Computations</td>
<td><strong>yes</strong></td>
<td><strong>yes</strong></td>
<td>no</td>
</tr>
<tr>
<td>User Functions</td>
<td><strong>yes</strong></td>
<td><strong>yes</strong></td>
<td>no?</td>
</tr>
<tr>
<td>Responsiveness</td>
<td>no</td>
<td><strong>yes</strong></td>
<td><strong>yes</strong></td>
</tr>
<tr>
<td>Reactivity</td>
<td>no</td>
<td><strong>yes</strong></td>
<td>no?</td>
</tr>
<tr>
<td>Linear Constraints</td>
<td>no</td>
<td>no</td>
<td><strong>yes</strong></td>
</tr>
<tr>
<td>Discrete Constraints</td>
<td>no</td>
<td>no</td>
<td>no</td>
</tr>
<tr>
<td>Explanations of Results</td>
<td>no</td>
<td>no</td>
<td>no</td>
</tr>
</tbody>
</table>
<p>The Stylus column goes for SASS too. The Flapjax column goes for Elm
too. Question marks mean that I am not sure because the documentation
isn’t clear enough to definitely rule a capability out.
“Responsiveness” means taking into account low-frequency client-side
values like the user’s window width, whereas “reactivity” means also
taking into account high-frequency ones like mouse position.</p>
<p>If you are into making tools as well as web design, one way you can
help take everyone else’s CSS to the next level is to take one of
these thoughts and implement it, either in an appropriate existing
tool, or, if that doesn’t fit, a new one. As a recap, here’s the
project list as I see it:</p>
<ul>
<li><p>Augment the compiler in any tool to <strong>symbolically evaluate</strong> style
rules with respect to media-queryable variables. From this, detect
regions where behavior changes, and extract the boundaries of those
regions to media queries.</p>
<ul>
<li><p>For SASS or Stylus this would provide support for responsive web
design.</p></li>
<li><p>For Flapjax, Elm, or GSS, this could improve performance, by
offloading some of the work the JavaScript runtime has to do to
the browser’s native rendering engine.</p></li>
</ul></li>
<li><p>Augment the compiler in any tool to <strong>numerically evaluate</strong> style
rules with respect to media-queryable variables. Do this covering
some value region at some resolution, and emit media queries that
select a rendition of the style sheet based on the nearest sample.</p>
<ul>
<li><p>This goes anywhere that symbolic evaluation goes, to handle
situations where the variable influences the answer in a
continuous (rather than discrete) way.</p></li>
<li><p>Down sides: the resulting experience will be jaggy, and the
process may emit large stylesheets with many media queries.</p></li>
</ul></li>
<li><p>Augment the compiler of GSS, Flapjax, or Elm to <strong>partial-evaluate</strong>
the constraint system or the functional reactive network into the
runtime system. If this works, it should improve the runtime
performance, perhaps considerably. It may not work, however,
because the constraint system or the functional reactive network may
not be static enough.</p></li>
<li><p>Incorporate (incremental) <strong>linear constraint solving</strong> from
Cassowary into Stylus, SASS, Flapjax, or Elm, to handle linear
constraint layouts in the context of any of those tools.</p></li>
<li><p>Incorporate a <strong>functional reactive engine</strong> like Flapjax or Elm
into GSS to handle reactivity to discrete variables.</p></li>
<li><p>Incorporate <strong>satisfiability solving</strong> or <strong>mixed integer-linear
programming</strong> into any of the above to extend their domains to
handle linear and discrete constraint layouts.</p>
<ul>
<li>It may be appropriate to limit the scope of discrete solving to
constraints that depend only upon low-frequency variables. Full
mixed-integer linear programming may be too heavy-weight for
interactive interfaces, but may be lightweight enough for
responsive constraint-based layouts.</li>
</ul></li>
<li><p>Implement generation of <strong>on-demand explanations</strong> into anything
that solves constraints. Such explanations should be a godsend for
debugging constraint layouts.</p></li>
</ul>
<h2 id="notes">Notes</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>It may be possible to implement this particular example
using the <code>calc</code> and <code>attr</code> functions from CSS3, if <code>attr</code> is willing
to return the value of a computed attribute like the element’s height.
I did not manage to make it work in Firefox 23, however, so there is
scope for doing this in a preprocessor. When <code>attr</code> becomes stable,
it could perhaps be retrofit as a backing technology.</p>
<p>Also, the semantics of this particular example is that this
<code>#screen</code> should be the full width of the user’s window regardless of
the width of its containing element; but CSS percentages don’t do
that. So, to make this happen without constraining the markup, <code>attr</code>
would need to be able to capture the value of the width of one element
and use it in a <code>calc</code>ulation in another element. There may be a good
reason for <code>attr</code> not to be able to behave like that in any near
future at all: with such flexibility in <code>attr</code>, it becomes easy to ask the
browser to solve systems of equations, and retrofitting that ability
into existing layout engines may be quite difficult. More on systems
of equations later.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In fact, it is quite possible that in the layout
setting, groups of constraints will even turn out to be completely
disconnected, so that even a complete explanation of what’s going on
with a given variable may not need to mention the entire layout.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Sat, 19 Oct 2013 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2013/taking-css-to-the-next-level/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Cleverness <em>of</em> Compilers <em>2:</em> How</title>
    <link>https://alexey.radul.name/ideas/2013/cleverness-of-compilers-2-how/index.html</link>
    <description><![CDATA[<p>The <a href="/ideas/2013/cleverness-of-compilers/">Cleverness of Compilers</a>
essay described the name of the hyperaggressive compilation game in
broad, philosophical strokes. Here, I would like to walk through the
Mandelbrot example in some detail, so that the interested reader may
see one particular way to actually accomplish that level of
optimization. There are of course other approaches to the
same goal, and the present approach (as well as its implementation)
leaves plenty of desiderata unfilled. It is, nevertheless, one way to
do it.</p>
<p>As a reminder, the task is to turn this modular Mandelbrot program</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode scheme"><code class="sourceCode scheme"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">;;; Complex arithmetic library</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> </span>(c:+ z1 z2)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  (<span class="kw">cons</span> (<span class="op">+</span> (<span class="kw">car</span> z1) (<span class="kw">car</span> z2))</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        (<span class="op">+</span> (<span class="kw">cdr</span> z1) (<span class="kw">cdr</span> z2))))</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> </span>(c:* z1 z2)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  (<span class="kw">cons</span> (<span class="op">-</span> (<span class="op">*</span> (<span class="kw">car</span> z1) (<span class="kw">car</span> z2))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>           (<span class="op">*</span> (<span class="kw">cdr</span> z1) (<span class="kw">cdr</span> z2)))</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        (<span class="op">+</span> (<span class="op">*</span> (<span class="kw">car</span> z1) (<span class="kw">cdr</span> z2))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>           (<span class="op">*</span> (<span class="kw">cdr</span> z1) (<span class="kw">car</span> z2)))))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> c:0 </span>(<span class="kw">cons</span> (real <span class="dv">0</span>) (real <span class="dv">0</span>)))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> </span>(<span class="kw">magnitude</span> z)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  (<span class="kw">sqrt</span> (<span class="op">+</span> (<span class="op">*</span> (<span class="kw">car</span> z) (<span class="kw">car</span> z))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>           (<span class="op">*</span> (<span class="kw">cdr</span> z) (<span class="kw">cdr</span> z)))))</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co">;;; Iteration library</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> </span>(iterate count f x)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  (<span class="kw">if</span> (<span class="op">&lt;=</span> count <span class="dv">0</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>      x</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>      (iterate (<span class="op">-</span> count <span class="dv">1</span>) f (f x))))</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co">;;; Mandelbrot set membership test</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> </span>((step c) z)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>  (c:+ (c:* z z) c))</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> </span>(mandelbrot? c)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>  (<span class="op">&lt;</span> (<span class="kw">magnitude</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>      (iterate (real <span class="dv">400</span>) (step c) c:0))</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>     <span class="dv">2</span>))</span></code></pre></div>
<p>into this efficient, browser-executable one</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">fol_program</span>(stdlib<span class="op">,</span> foreign<span class="op">,</span> heap) {</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;use asm&quot;</span><span class="op">;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">var</span> heap_view <span class="op">=</span> <span class="kw">new</span> stdlib<span class="op">.</span><span class="fu">Float32Array</span>(heap)<span class="op">;</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">var</span> sqrt <span class="op">=</span> stdlib<span class="op">.</span><span class="at">Math</span><span class="op">.</span><span class="at">sqrt</span><span class="op">;</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">function</span> <span class="fu">operation_231</span>(count<span class="op">,</span> f_env_1<span class="op">,</span> f_env_2<span class="op">,</span> x_1<span class="op">,</span> x_2) {</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    count <span class="op">=</span> <span class="op">+</span>count<span class="op">;</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    f_env_1 <span class="op">=</span> <span class="op">+</span>f_env_1<span class="op">;</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    f_env_2 <span class="op">=</span> <span class="op">+</span>f_env_2<span class="op">;</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    x_1 <span class="op">=</span> <span class="op">+</span>x_1<span class="op">;</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    x_2 <span class="op">=</span> <span class="op">+</span>x_2<span class="op">;</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (count <span class="op">&lt;=</span> <span class="fl">0.0</span>) {</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>      heap_view[<span class="dv">0</span>] <span class="op">=</span> x_1<span class="op">;</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>      heap_view[<span class="dv">1</span>] <span class="op">=</span> x_2<span class="op">;</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span><span class="op">;</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> <span class="fu">operation_231</span>(count <span class="op">-</span> <span class="fl">1.0</span><span class="op">,</span> f_env_1<span class="op">,</span> f_env_2<span class="op">,</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>                           ((x_1<span class="op">*</span>x_1 <span class="op">-</span> x_2<span class="op">*</span>x_2) <span class="op">+</span> f_env_1)<span class="op">,</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>                           ((x_1<span class="op">*</span>x_2 <span class="op">+</span> x_2<span class="op">*</span>x_1) <span class="op">+</span> f_env_2))<span class="op">;</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>  <span class="kw">function</span> <span class="fu">__main__</span>(c_1<span class="op">,</span> c_2) {</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    c_1 <span class="op">=</span> <span class="op">+</span>c_1<span class="op">;</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    c_2 <span class="op">=</span> <span class="op">+</span>c_2<span class="op">;</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">var</span> ret_x <span class="op">=</span> <span class="fl">0.0</span><span class="op">;</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">var</span> ret_y <span class="op">=</span> <span class="fl">0.0</span><span class="op">;</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">operation_231</span>(<span class="fl">400.0</span><span class="op">,</span> c_1<span class="op">,</span> c_2<span class="op">,</span> <span class="fl">0.0</span><span class="op">,</span> <span class="fl">0.0</span>)<span class="op">;</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    ret_x <span class="op">=</span> <span class="op">+</span>heap_view[<span class="dv">0</span>]<span class="op">;</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    ret_y <span class="op">=</span> <span class="op">+</span>heap_view[<span class="dv">1</span>]<span class="op">;</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="op">+</span><span class="fu">sqrt</span>(ret_x<span class="op">*</span>ret_x <span class="op">+</span> ret_y<span class="op">*</span>ret_y) <span class="op">&lt;</span> <span class="fl">2.0</span>)<span class="op">|</span><span class="dv">0</span><span class="op">;</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> __main__<span class="op">;</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>In the <a href="https://github.com/axch/dysvunctional-language">DysVunctional Language
compiler</a>, this task
happens in four steps:</p>
<dl>
<dt><a href="#flow-analysis">Flow analysis</a></dt>
<dd>
<p>converts the input program into a form where all the
specialization is explicit, and the contextual information
relevant to each specialized code point is immediately available.
In keeping with an unfortunate tradition, both the process and the
result is called an “analysis”.</p>
</dd>
<dt><a href="#code-generation">Code generation</a></dt>
<dd>
<p>converts the result of flow analysis (without reference to the
original program) into code in FOL, a First-Order intermediate
Language, where all types and control flow are explicit (all
function calls are to statically apparent targets).</p>
</dd>
<dt><a href="#further-optimization">Further optimization</a></dt>
<dd>
<p>optimizes the intermediate FOL program with (aggressive versions
of) standard methods, to which FOL is well suited because of its
restricted nature.</p>
</dd>
<dt><a href="#backend-translation">Backend translation</a></dt>
<dd>
<p>translates the optimized FOL program to asm.js syntax. FOL is
sufficiently low-level for this not to be too complicated.</p>
</dd>
</dl>
<p>The plan for this essay is to walk through all four of these
operations as they apply to this example program.</p>
<h2 id="flow-analysis">Flow Analysis</h2>
<p>The flow analysis in DVL is an abstract interpretation of the input
program.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> An <em>abstract interpretation</em> of a program consists of
executing (<em>interpreting</em>) the program, except that instead of the
variables and data structures holding the values the program actually
computes, they hold <em>abstract values</em>—representations of sets of
possible values that could occur during various runs of the program.
The idea is that one abstract value or abstract program state captures
some commonality of many possible actual values or program states
but ignores (<em>abstracts from</em>) the residual variation among them.</p>
<blockquote class="pullquote-display">
<p>
Reusable code is written to be able to consume huge
varieties of stuff, but any given use will only feed it some
restricted diet.
</p>
</blockquote>
<p>If we do a good job of picking what commonality to represent and what
variation to ignore, it is possible to compute a consistent model of
all possible executions of the source program. This
model then gives information about what values each part of the program
may produce, and therefore what the consumers of that part may
have to consume. Figuring that out is extremely valuable for
optimization because general-purpose, reusable code is written to be
able to consume huge varieties of stuff (that’s why it’s called
“general-purpose”), but any given use will only feed it some
restricted diet. For instance, the <code>iterate</code> in our Mandelbrot
program is only ever told to iterate one specific type of function,
namely <code>(step c)</code> for various complex numbers <code>c</code>, even though it was
written to be able to iterate absolutely anything. Discovering this use pattern
opens the door to a plethora of optimizations.</p>
<p>How do we do abstract interpretation on DVL? Since the DVL input
language is functional,<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> the state of execution of any
program is completely determined by the expression being evaluated and
the environment in which to evaluate it, or, alternately, by the
procedure being invoked and the arguments with which to invoke it. We
can represent a whole class of executions of a DVL program by allowing
the values in such a state to be abstract; and we can simulate it with
a suitably modified interpreter that respects and appropriately
propagates the variation the abstract state permits, while deducing as
much as possible from the commonalities among states that the
abstraction exposes.</p>
<p>Without further ado, the following slideshow illustrates how flow
analysis by abstract interpretation proceeds on (an interesting part
of) our example <code>mandelbrot?</code> program.</p>
<center>
<iframe class="frame" width="540" height="590" src="embedder.html#animated-analysis.html">
<a href="/animated-analysis.html">the analysis as a standalone
page</a>
</iframe>
</center>
<p>
</p>
<p>At the end of the flow analysis process (it ends on the Mandelbrot
example, but in general DVL relies on correct use of the <code>real</code>
primitive to ensure that abstract evaluation terminates), we have an
analysis data structure holding a bunch of information like this:</p>
<p><span class="math display">\[ \begin{eqnarray*}
\exp{(iterate (real 400) (step c) c:0)}, \env{1} &amp; \mapsto &amp; \RR \\
\obj{iterate}, \R, \obj{closure1}, \RR &amp; \mapsto &amp; \RR \\
\exp{(&lt;= count 0)}, \env{2} &amp; \mapsto &amp; \B \\
\obj{closure1}, \RR &amp; \mapsto &amp; \RR \\
\textrm{etc} &amp; &amp;
\end{eqnarray*} \]</span>
These entries tell us that</p>
<ul>
<li><p>The expression <span class="math inline">\(\exp{(iterate ...)}\)</span>, evaluated in any environment
of the same shape<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> as <span class="math inline">\(\env{1}\)</span>, should be treated as though it
returns a pair of arbitrary real numbers;</p></li>
<li><p>The procedure <span class="math inline">\(\obj{iterate}\)</span>, when applied to any triple of
arguments consisting of any real number, any procedure of the same
shape as <span class="math inline">\(\obj{closure1}\)</span>, and any pair holding any two real
numbers, should be treated as though it returns a pair of arbitrary
real numbers;</p></li>
<li><p>The expression <span class="math inline">\(\exp{(&lt;= count 0)}\)</span>, evaluated in any environment
of the same shape as <span class="math inline">\(\env{2}\)</span>, should be treated as though it
returns an arbitrary boolean;</p></li>
<li><p>etc.</p></li>
</ul>
<p>Moreover, the entries tell us exactly the shapes of program points
through which execution will pass, so they carry sufficient
information to generate code that will do the same thing but with
explicit data types and control flow, which will then be
optimizable by standard methods.</p>
<h2 id="code-generation">Code Generation</h2>
<p>Now that we have a flow analysis in hand, what do we do with it?
The entries in the analysis cover all the shapes of all the program
points that will be encountered during program execution. In
particular, the entries like</p>
<p><span class="math display">\[ \obj{iterate}, \R, \obj{closure1}, \RR \mapsto \RR \]</span>
correspond to procedure applications, and cover all the function calls
(in all the variations on the shapes of their arguments) that will
occur at runtime.</p>
<p>We generate a procedure corresponding to each such entry. Since a given
source procedure may lead to many entries if it is called with arguments of
many different shapes, this is the place where we materialize the
specialization that the flow analysis gave us. The body of
each generated procedure is bascially a direct copy of the body of the
closure being applied, with three important exceptions:</p>
<ul>
<li><p>Any subexpressions whose concrete values are completely determined
by what is known about the argument shapes get replaced by constants
(in particular, a lot of the type tests that happen in the source
language go away at this point).</p></li>
<li><p>To make sure the result is first-order, the code generator performs
<a href="http://matt.might.net/articles/closure-conversion/">closure
conversion</a>;
that is, every <code>lambda</code> form becomes a constructor for an explicit
data structure, and every procedure accepts and deconstructs this
structure to find the variables it was closed over. Note that we
don’t need to keep track of the code pointer, because the abstract
evaluation already computed where they flow.</p></li>
<li><p>Each application in the body becomes a call to the appropriate
generated procedure. The resulting static control flow information
is very precise because we always call exactly the specialization
determined by the shapes of the arguments passed at that particular
call site.</p></li>
</ul>
<p>For example, the entry</p>
<p><span class="math display">\[ \obj{iterate}, \R, \obj{closure1}, \RR \mapsto \RR \]</span>
becomes the following code (comments added):<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode scheme"><code class="sourceCode scheme"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> </span>(operation-231  <span class="co">; iterate-R-cl1-R.R would be a better name</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>         the-env        <span class="co">; arg introduced by closure conversion</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>         count f-env x) <span class="co">; original args, but proc now just env</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">;; The intermediate language has explicit type signatures</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  (argument-types</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>   env-226                    <span class="co">; environment data type</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>   real env-235 (real <span class="op">.</span> real) <span class="co">; types of formal parameters</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>   (real <span class="op">.</span> real))             <span class="co">; return type</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  (<span class="kw">if</span> (operation-234 <span class="co">; application of &lt;= to R and 0</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>       (env-226-g:&lt;= the-env) <span class="co">; the &lt;= procedure is in my closure</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>       count <span class="dv">0</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>      x</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>      (operation-231 <span class="co">; recursive application (same arg shapes)</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>       the-env       <span class="co">; recursion means same closure</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>       (operation-24 <span class="co">; application of - to R and 1</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        (env-226-g:- the-env)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        count <span class="dv">1</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>       f-env</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>       (operation-23 <span class="co">; the call (f x) from the source</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>                     <span class="co">; the target is static, and itself specialized</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        f-env x))))</span></code></pre></div>
<p>Are we fast yet? No, I expect not. But though this may not be
apparent by looking at it, this intermediate code is much more
amenable to optimization by well-known methods than the</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode scheme"><code class="sourceCode scheme"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> </span>(iterate count f x)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  (<span class="kw">if</span> (<span class="op">&lt;=</span> count <span class="dv">0</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>      x</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>      (iterate (<span class="op">-</span> count <span class="dv">1</span>) f (f x))))</span></code></pre></div>
<p>we started with. For instance, the minus procedure (spelled <code>-</code>) is not actually
primitive in DVL, but defined in the standard library as a generic
procedure that operates on arbitrary “vector-like” structures,
including trees of dual numbers for <a href="../introduction-to-automatic-differentiation/">automatic
differentiation</a>.
But the generated procedure <code>operation-24</code> is specialized to the case
of applying minus to some real number and the constant 1. With further
optimization, <code>operation-24</code> will turn into a single machine
instruction, even though minus was capable of an arbitrarily deep
recursive computation. But perhaps even more important in this case
is that the <code>(f x)</code> call from the source, which called a syntactically
unknown function, now becomes a static call to a known procedure
(namely, <code>operation-23</code>), which is itself specialized to its calling
context.</p>
<blockquote class="pullquote-display">
<p>
Flow analysis
removes obstacles to optimization by detecting which
parts of the program don’t actually communicate with each other,
so their interfaces need not remain compatible.
</p>
</blockquote>
<p>Why will this help us be fast later? The primary obstacle to
optimization is the inability to
alter interfaces, because to alter an interface one needs to adjust
all the producers and consumers of that interface. Flow analysis
reduces this obstacle by detecting which
parts of the program don’t actually communicate with each other,
so that their interfaces need not remain compatible.
The generated FOL program captures this more accurate model of the
source program’s communication flow, and is consequently very amenable
to optimization by standard methods.</p>
<h2 id="further-optimization">Further Optimization</h2>
<p>Here is not the place to discuss in detail all of the optimization
passes that operate on intermediate FOL programs. Suffice it to say
that aggressive application of the following standard techniques
produces quite pleasant results:</p>
<dl>
<dt><a href="https://en.wikipedia.org/wiki/Inline_expansion">Procedure inlining</a></dt>
<dd>
<p>inserts (many) definitions of called procedures inline into their
callers. Inlining is less critical for FOL than for other languages
because those called procedures are already quite specialized, so
copying them yields less benefit. Some later optimizations are
still easier to spot after inlining than before, though.</p>
</dd>
<dt><a href="https://en.wikipedia.org/wiki/Common_subexpression_elimination">Common subexpression elimination</a></dt>
<dd>
<p>replaces repeated computations by reuses of past results. This
pass also applies algebraic simplifications like <code>(car (cons x _)) =&gt; x</code> and <code>(* z 0) =&gt; 0</code>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> These activities are
intermeshed in one pass because these optimizations
cascade with each other.</p>
</dd>
</dl>
<dl>
<dt><a href="https://en.wikipedia.org/wiki/Dead_code_elimination">Dead code elimination</a></dt>
<dd>
<p>deletes computations of values for unused variables. The
intraprocedural version is simple and effective. The DVL compiler
also implements an interprocedural version, which is mostly good
for flushing stuff carried around but not used in recursive loops.</p>
</dd>
<dt>Scalar replacement of aggregates</dt>
<dd>
<p>converts compound data structures into sets of scalar variables
when possible, reducing allocation and referencing.</p>
</dd>
<dt><a href="https://en.wikipedia.org/wiki/Constant_folding">Constant folding</a></dt>
<dd>
<p>is not necessary because the code generator already replaced
computations of statically known values with those constants.</p>
</dd>
</dl>
<p>In the case of our running example, the result after applying these
passes is the cleanest iteration for which one could wish:<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode scheme"><code class="sourceCode scheme"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> </span>(operation-231</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>         count</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>         f-env-1 <span class="co">; The only variable content of the closure</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>         f-env-2 <span class="co">; of f was two real numbers</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>         x-1</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>         x-2)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  (argument-types</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>   real real real real real</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>   (<span class="kw">values</span> real real)) <span class="co">; Returns two values to avoid making a pair</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  (<span class="kw">if</span> (<span class="op">&lt;=</span> count <span class="dv">0</span>)     <span class="co">; Primitive numeric &lt;= now</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>      (<span class="kw">values</span> x-1 x-2)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>      (operation-231   <span class="co">; Loops still rely on tail recursion</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>       (<span class="op">+</span> count -<span class="dv">1</span>)    <span class="co">; Primitive numeric +</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>       f-env-1</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>       f-env-2</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>       <span class="co">;; Here is our elaborate arithmetic specialized to complex</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>       <span class="co">;; numbers and inlined.  No allocation or generics here;</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>       <span class="co">;; just floating adds and multiplies.</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>       (<span class="op">+</span> (<span class="op">-</span> (<span class="op">*</span> x-1 x-1) (<span class="op">*</span> x-2 x-2))</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>          f-env-1)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>       (<span class="op">+</span> (<span class="op">+</span> (<span class="op">*</span> x-1 x-2) (<span class="op">*</span> x-2 x-1))</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>          f-env-2))))</span></code></pre></div>
<p>All that remains is to print it out in the backend’s preferred syntax.</p>
<h2 id="backend-translation">Backend Translation</h2>
<p>Translation from optimized intermediate code to the syntax of the
desired backend is not actually completely trivial, because backends
tend not to have exactly the same semantic model as the intermediate
language. For example, the asm.js backend implements multiple value
returns by writing to the global array that serves as heap (I am
fortunate that I do not need it for anything else right now). That
said, there isn’t that much enlightening content in backend
translation as it currently exists in the DVL system. Getting from
the optimized FOL in the previous section to the following asm.js is
really pretty straightforward.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">operation_231</span>(count<span class="op">,</span> f_env_1<span class="op">,</span> f_env_2<span class="op">,</span> x_1<span class="op">,</span> x_2) {</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  count <span class="op">=</span> <span class="op">+</span>count<span class="op">;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  f_env_1 <span class="op">=</span> <span class="op">+</span>f_env_1<span class="op">;</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  f_env_2 <span class="op">=</span> <span class="op">+</span>f_env_2<span class="op">;</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  x_1 <span class="op">=</span> <span class="op">+</span>x_1<span class="op">;</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  x_2 <span class="op">=</span> <span class="op">+</span>x_2<span class="op">;</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (count <span class="op">&lt;=</span> <span class="fl">0.0</span>) {</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    heap_view[<span class="dv">0</span>] <span class="op">=</span> x_1<span class="op">;</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    heap_view[<span class="dv">1</span>] <span class="op">=</span> x_2<span class="op">;</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span><span class="op">;</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fu">operation_231</span>(count <span class="op">-</span> <span class="fl">1.0</span><span class="op">,</span> f_env_1<span class="op">,</span> f_env_2<span class="op">,</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>                         ((x_1<span class="op">*</span>x_1 <span class="op">-</span> x_2<span class="op">*</span>x_2) <span class="op">+</span> f_env_1)<span class="op">,</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>                         ((x_1<span class="op">*</span>x_2 <span class="op">+</span> x_2<span class="op">*</span>x_1) <span class="op">+</span> f_env_2))<span class="op">;</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>And that’s it. There you have it, folks: the anatomy of one compiler
that’s too clever for its own good.</p>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      R: "{\\mathbb{R}}",
      B: "{\\mathbb{B}}",
      RR: "(\\R\\ . \\R)",
      eps: "\\varepsilon",
      exp: ["{\\textrm{#1}}",1],
      env: ["{\\textrm{env#1}}",1],
      obj: ["{\\left<\\textrm{#1}\\right>}",1],
    },
    equationNumbers: { autoNumber: "AMS" },
    noErrors: { disabled: true },
  }
});
</script>
<h2 id="notes">Notes</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>For those who know the jargon, I can elaborate a bit.
The trick in DVL is that the abstract value space is extremely
fine-grained: the only actual abstraction is over unknown real numbers
and unknown booleans (general sum types are on the roadmap but not
implemented yet). In particular, the abstraction of a procedure is a
closure with a definite code body, and only the contents of the
environment are abstract. The second trick is that the analysis is
polyvariant, and indeed there are no built-in bounds on the
polyvariance—all imprecision is introduced manually. Eschewing
polyvariance bounds buys a certain predictability, namely the
generated code being exactly as specialized as the programmer intends.
The cost this choice pays, of course, is possible overspecialization,
including possible nontermination of the analysis.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Actually, DVL includes a primitive nullary procedure named
<code>gensym</code>, different dynamic invocations of which make distinct
objects. This little bit of impurity is what puts the Dys- into
DysVunctional Language. Some impurity is necessary to correctly
implement <a href="../introduction-to-automatic-differentiation/">automatic
differentiation</a>, and I
chose generating unique objects because they can still be accommodated
within the framework of flow analysis. How to actually accommodate them is
beyond the scope of this footnote; but the program being described in this
essay is preserved in the DVL source repository as <a href="https://github.com/axch/dysvunctional-language/tree/master/vl/">Vunctional
Language (VL)</a>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>By “shape” I mean that <span class="math inline">\(\env1\)</span> or <span class="math inline">\(\obj{closure1}\)</span> are
actually abstract, and may, for example, declare some variables bound
to arbitrary real numbers rather than to specific values, allowing
variation at those points. Note that in this system, abstract
procedures like <span class="math inline">\(\obj{closure1}\)</span> still have just one concrete
procedure body, and admit variation only in the values of closed-over
variables. This fine granularity is important, because application of an abstract
procedure means branching to a specific piece of code: all variation
in control flow caused by higher-order functions is at this point
solved.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Actually, if you look in <a href="https://github.com/axch/dysvunctional-language/blob/master/vl/code-generator.scm">the
source</a>,
this output is somewhat simplified from what code generation produces, but
not in any important way. I have post-processed the output to
eliminate a few confusing and uninteresting artifacts of the choice of
minimum primitive basis in the DVL language, and of the details of DVL
macroexpansion into said minimum primitive basis. In particular,
recursion is actually handled with a variant of the <a href="https://en.wikipedia.org/wiki/Fixed-point_combinator#Y_combinator">Y
combinator</a>
rather than explicit implementation as this example would suggest.
But that’s beside the point.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Yes, yes, I know that converting <code>(* z 0)</code> to <code>0</code> does not
preserve semantics strictly in the presence of floating point
infinities and NaNs. This optimization is present in the prototype
because it chains, and because the implementation of automatic
differentiation in DVL introduces lots of multiplications by
statically apparent zeroes. In the limit as DVL approaches actual
production use, thought will need to be directed to the distinction
between code expressing formulas over nice mathematical structures
(where multiplying by zero always does produce zero) and code
carefully written with floating point semantics in mind. This
actually strikes me as a very deep issue, because it is the difference
between specification and approximation.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>FOL currently has no explicit syntax for loops and
represents them as tail recursions. This makes it difficult to
implement loop optimizations over FOL; but my philosophy is to let the
compiler targeted by the backend deal with that for now. FOL’s other
optimizations are quite useful for making sure the code generated from
FOL doesn’t violate any hidden assumptions of its targets, but
optimizing loops inside FOL does not seem necessary for that. On the
other hand, handling loops well may be a good idea if I get serious
about targeting asm.js, because it may not optimize said loops for me.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Thu, 10 Oct 2013 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2013/cleverness-of-compilers-2-how/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title><em>On the</em> Cleverness <em>of</em> Compilers</title>
    <link>https://alexey.radul.name/ideas/2013/cleverness-of-compilers/index.html</link>
    <description><![CDATA[<p>The “Sufficiently Clever Compiler” has become something of a trope in
the Lisp community: the mythical beast that promises language and
interface designers near-unlimited freedom, and leaves their output in
a performance lurch by its non-appearance. A few years ago, I was
young enough to join a research project to build one of these things.
Neglecting a raft of asterisks, footnotes, and caveats, we ended up
making something whose essence is pretty impressive: you pay for
abstraction boundaries in compile-time resources, but they end up free
at runtime. One
<a href="https://github.com/axch/dysvunctional-language">prototype</a> was just
open-sourced recently, so that makes now a good time to talk about it.</p>
<p>Permit me to begin my musings on this subject with an example. The
next element on this page is a Javascript Canvas object, whereon, upon
your clicking the button labeled “Go!”, your browser will draw a
progressively-refined rendition of the <a href="https://en.wikipedia.org/wiki/Mandelbrot_set">Mandelbrot
set</a>, anti-aliased for
your viewing pleasure. It will also display how long each iteration
took to compute, both in real time and in count of floating point
arithmetic operations.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<center>
<canvas id="it" width="500" height="500">
</canvas>
<br />
<button onclick="doit()">Go!</button><img id="spinner" src="spinner.gif"> <br />
<table id="timings">
<tr>
<th>
Resolution
</th>
<th>
Mflops
</th>
<th>
Time (ms)
</th>
</table>
</center>
<style>
#it {border: 1px solid grey; margin: 0 auto;}
#spinner {border: none; margin: 0px; vertical-align:middle; display: none;}
</style>
<script src="mandel.js" type="text/javascript"></script>
<script src="mandel-driver.js" type="text/javascript"></script>
<p>I obviously don’t know how fast this demo runs on your computer, but
on mine, in Firefox 23.0, with <a href="http://asmjs.org/">asm.js</a> turned on,
I get a little under 1 Mflop/ms. Given that my CPU is 1.6 GHz, that’s
an average of a little over half a floating-point operation per clock
cycle. Not bad for a single-threaded Javascript program, eh?<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>What do you think the inner loop of that program looks like? You
could be forgiven for thinking that it looks something like this:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">fol_program</span>(stdlib<span class="op">,</span> foreign<span class="op">,</span> heap) {</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;use asm&quot;</span><span class="op">;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">var</span> heap_view <span class="op">=</span> <span class="kw">new</span> stdlib<span class="op">.</span><span class="fu">Float32Array</span>(heap)<span class="op">;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">var</span> sqrt <span class="op">=</span> stdlib<span class="op">.</span><span class="at">Math</span><span class="op">.</span><span class="at">sqrt</span><span class="op">;</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">function</span> <span class="fu">op_231</span>(count<span class="op">,</span> c_x<span class="op">,</span> c_y<span class="op">,</span> z_x<span class="op">,</span> z_y) {</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    count <span class="op">=</span> <span class="op">+</span>count<span class="op">;</span> c_x <span class="op">=</span> <span class="op">+</span>c_x<span class="op">;</span> c_y <span class="op">=</span> <span class="op">+</span>c_y<span class="op">;</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    z_x <span class="op">=</span> <span class="op">+</span>z_x<span class="op">;</span> z_y <span class="op">=</span> <span class="op">+</span>z_y<span class="op">;</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (count <span class="op">&lt;=</span> <span class="fl">0.0</span>) {</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>      heap_view[<span class="dv">0</span>] <span class="op">=</span> z_x<span class="op">;</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>      heap_view[<span class="dv">1</span>] <span class="op">=</span> z_y<span class="op">;</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span><span class="op">;</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>      <span class="fu">op_231</span>(count <span class="op">-</span> <span class="fl">1.0</span><span class="op">,</span> c_x<span class="op">,</span> c_y<span class="op">,</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>             ((z_x<span class="op">*</span>z_x <span class="op">-</span> z_y<span class="op">*</span>z_y) <span class="op">+</span> c_x)<span class="op">,</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>             ((z_x<span class="op">*</span>z_y <span class="op">+</span> z_y<span class="op">*</span>z_x) <span class="op">+</span> c_y))<span class="op">;</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span><span class="op">;</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  <span class="kw">function</span> <span class="fu">__main__</span>(x<span class="op">,</span> y) {</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="op">+</span>x<span class="op">;</span> y <span class="op">=</span> <span class="op">+</span>y<span class="op">;</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">var</span> z_x <span class="op">=</span> <span class="fl">0.0</span><span class="op">;</span> <span class="kw">var</span> z_y <span class="op">=</span> <span class="fl">0.0</span><span class="op">;</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">op_231</span>(<span class="fl">400.0</span><span class="op">,</span> x<span class="op">,</span> y<span class="op">,</span> <span class="fl">0.0</span><span class="op">,</span> <span class="fl">0.0</span>)<span class="op">;</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    z_x <span class="op">=</span> <span class="op">+</span>heap_view[<span class="dv">0</span>]<span class="op">;</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    z_y <span class="op">=</span> <span class="op">+</span>heap_view[<span class="dv">1</span>]<span class="op">;</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="op">+</span><span class="fu">sqrt</span>(z_x<span class="op">*</span>z_x <span class="op">+</span> z_y<span class="op">*</span>z_y) <span class="op">&lt;</span> <span class="fl">2.0</span>)<span class="op">|</span><span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> __main__<span class="op">;</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>If you thought that, you would not be entirely wrong, because that is,
in fact, the Mandelbrot set membership test code that your browser
actually executes around a million times when you click on the “Go!”
button.</p>
<p>However, the way I want to actually write that program looks like
this:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode scheme"><code class="sourceCode scheme"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">;;; Complex arithmetic library</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> </span>(c:+ z1 z2)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  (<span class="kw">cons</span> (<span class="op">+</span> (<span class="kw">car</span> z1) (<span class="kw">car</span> z2))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        (<span class="op">+</span> (<span class="kw">cdr</span> z1) (<span class="kw">cdr</span> z2))))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> </span>(c:* z1 z2)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  (<span class="kw">cons</span> (<span class="op">-</span> (<span class="op">*</span> (<span class="kw">car</span> z1) (<span class="kw">car</span> z2))</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>           (<span class="op">*</span> (<span class="kw">cdr</span> z1) (<span class="kw">cdr</span> z2)))</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        (<span class="op">+</span> (<span class="op">*</span> (<span class="kw">car</span> z1) (<span class="kw">cdr</span> z2))</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>           (<span class="op">*</span> (<span class="kw">cdr</span> z1) (<span class="kw">car</span> z2)))))</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> c:0 </span>(<span class="kw">cons</span> (real <span class="dv">0</span>) (real <span class="dv">0</span>)))</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> </span>(<span class="kw">magnitude</span> z)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>  (<span class="kw">sqrt</span> (<span class="op">+</span> (<span class="op">*</span> (<span class="kw">car</span> z) (<span class="kw">car</span> z))</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>           (<span class="op">*</span> (<span class="kw">cdr</span> z) (<span class="kw">cdr</span> z)))))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co">;;; Iteration library</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> </span>(iterate count f x)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>  (<span class="kw">if</span> (<span class="op">&lt;=</span> count <span class="dv">0</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>      x</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>      (iterate (<span class="op">-</span> count <span class="dv">1</span>) f (f x))))</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co">;;; Mandelbrot set membership test</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> </span>((step c) z)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>  (c:+ (c:* z z) c))</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>(<span class="ex">define</span><span class="fu"> </span>(mandelbrot? c)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>  (<span class="op">&lt;</span> (<span class="kw">magnitude</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>      (iterate (real <span class="dv">400</span>) (step c) c:0))</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>     <span class="dv">2</span>))</span></code></pre></div>
<p>Doubtless the first difference that jumps out at you is that one of
these programs is written using a beautiful, familiar surface syntax
that makes you feel right at home, while the other uses an ugly,
foreign notation that makes your eyes glaze over. That’s not the
point, though.</p>
<p>The important difference is that the program the browser runs (in
Javascript) is much less modular than the program I want to write (in
Scheme, as it happens). In the Javascript program, all the concepts
relating to Mandelbrot set membership testing are mushed together: the
concepts of complex arithmetic are mixed up with the concept of
iterating a function, and mixed up with the definition of the function
to iterate. None of them are available as named entities; if you
wanted to solve the problem a different way, you would have to rewrite
the entire thing from scratch.</p>
<blockquote class="pullquote-display">
<p>
The very advantage of modularity is the
source of its performance cost: modularity is the ability to do things
other than what you did.
</p>
</blockquote>
<p>In the Scheme program, however, they are separate definitions, given
their own separate names. Complex arithmetic is defined in one place,
independent of how you choose to use it. The step function defining
the Mandelbrot recurrence is defined in another place, available to be
used any which way you want. The idea of iterating a function for a
while is defined in another place, independent of the function to
iterate. A change of algorithm becomes a matter of rearranging the
available pieces, rather than of rewriting the whole program.</p>
<p>And there we have a clue as to why, in modern computing, modular code
is not high-performance code. The very advantage of modularity is the
source of its performance cost: modularity is the ability to do things
other than what you did. Modularity is the ability to reuse the same
idea in many different ways and in many different places.</p>
<p>The cost that modularity carries is generic linkages between the
“idea” and the “places”. That linkage carries its own cost at
runtime, but more importantly, it impedes other optimizations. In our
example, <code>iterate</code> accepts an arbitrary function <code>f</code>. That means the
call <code>(f x)</code> in iterate has to be set up to call an arbitrary
function. It also means that its argument and its return value have
to be communicated using completely generic mechanisms.</p>
<p>Symmetrically, the function produced by <code>(step c)</code>, being a
first-class object in Scheme, can be used anywhere. That means it has
to accept its argument and produce its return value using completely
generic mechanisms, that match the generic call sites. It also means
that we cannot optimize it by changing its interface, because we do
not know the places where the complementary changes would need to be
made. So <code>(step c)</code> must accept its complex number argument and
return its complex number result each as a single value, presumably
machine-word-sized; so those complex numbers have to be allocated and
dereferenced; and the cost of making those data structures all the
time dwarfs the cost of the arithmetic we are trying to do.</p>
<p>“But wait!” I can hear you say to yourself, “I know what function
<code>iterate</code> is iterating, and I know where <code>(step c)</code> is called. Why is
my compiler so dumb as not to notice that <code>mandelbrot?</code> calls for an
iteration of exactly <code>(step c)</code>, and produce code for it that takes
this into account? Then it would be clear that the complex number
produced by one call to <code>(step c)</code> will just be consumed by the next,
and the program can avoid allocating a silly two-element structure for
it by just passing the two parts in variables.” (Which is, of course,
how the Javascript program behaves.)</p>
<blockquote class="pullquote-display">
<p>
Specialization is the path from modularity to performance.
</p>
</blockquote>
<p>That’s right. The thing you did there was a little bit of (combined
data- and control-) flow analysis, computing that <code>iterate</code> gets
called with <code>(step c)</code> as an argument and therefore <code>(step c)</code> appears
as the <code>f</code> in the call <code>(f x)</code> inside <code>iterate</code>. You also decided
that, at least for this problem, it is probably worthwhile to produce
a version of <code>iterate</code> that is specialized to <code>(step c)</code> as an
argument (with due provision for operating on different values of
<code>c</code>). And that’s my point. I conjecture that, at this moment in
history, the one remaining piece of compiler technology that human
programmers instinctively expect before admitting that a compiler is
Sufficiently Clever is the knack for automatically specializing the
right program elements to the context of their use.</p>
<p>Specialization is the path from modularity to performance.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> A modular
component is one that can be used in different ways in different
places. A specialized component is one that is tailored to the way
and place it is used, taking advantage of local context to improve
efficiency; often to the point that separation between component and
context is completely obscured. To the extent that specialization can
be automated, doing so would let us have our cake and eat it
too—write high-performance programs in a modular way.</p>
<p>The problem with specialization is that it comes with its own cost: if
a component is used in many places, specializing it to each context
requires copying it. Since the inside of a component is, of course, a
set of places where other components are used, the number of contexts
grows with each specialization, and, unchecked, this process can
easily get out of hand. That’s why mainstream compilers are pretty
conservative about it.</p>
<p>That being said, there are problem domains where really aggressive
specialization should work wonders. The activity called “scientific
computing” comes to mind: the main item of interest in these programs
is some complicated, CPU- and memory-intensive (as opposed to
I/O-intensive) computation involving floating-point numbers. All the
fine data structures, first-class functions, and abstractions with
which one might wish to write such programs are just scaffolding whose
sole purpose is to get the right floating-point numbers to meet the
right arithmetic operations. So it is tempting to try and see whether
one could write scientific computing programs in a nice, high-level,
modular way, and have the compiler solve the scaffolding at compile
time (by specialization followed by standard optimizations) and
produce efficient numerical code.</p>
<p>That was the theory behind that research project I alluded to in the
introduction. Modern general-purpose compilers do not generally
specialize aggressively enough to “solve the scaffolding” of any
nontrivial program. In particular, <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic
differentiation</a>
is an especially impressive form of scaffolding, that would be
very nice to be able to use, but that tends to trigger “this is too
costly to specialize” heuristics. But perhaps there is a niche for
tools that specialize too much for general-purpose use, but enough to
make (at least some) numerical kernels go really fast despite being
very modular.</p>
<p>I wrote one of those. Under the supervisorship of <a href="http://www-bcl.cs.may.ie/~barak/">Professor
Pearlmutter</a> and with much
assistance from our collaborator <a href="https://engineering.purdue.edu/~qobi/">Professor
Siskind</a>, the research group
studied various issues with semantics and implementation of automatic
differentiation (which are &lt;%= link_to ‘nontrivial’,
‘/ideas/2013-08-15-introduction-to-automatic-differentiation.html’
%&gt;), forays into compiler technology, etc. It so transpired that I
ended up being the primary author of the <a href="https://github.com/axch/dysvunctional-language">DysVunctional
Language</a> prototype,
which specializes away modularity boundaries (including a user-space
automatic differentiation library) until the cows come home.</p>
<p>Now, I do not advocate seriously using DVL (yet), as it is unfinished
(and not very actively worked on right now, for lack of funds). For one, it is
meant for numerical kernels, but it does not have a sensible foreign
interface. For another, it requires programmer
annotations to indicate what not to specialize on (in the complete
absence of which, its flow analysis will amount to running your
program slowly at compile time, and the code generator will amount to
writing an executable that just prints out the answer). It is also
research-prototype quality software in general—for instance, a type
error in the source program will give you a prompt debugging the
compiler.</p>
<p>I am, nevertheless, quite proud of DVL. I flatter myself that its
source is not too terribly hard to read, so it can serve as an example
for how to do that kind of thing in a different context. And the
thing it does, if you get your program to compile, feels pretty
magical: as much abstraction and modularity as you could want gets
stripped away, revealing the bare numerical computation for the
computer to just execute. Not just on podunk 30-line Mandelbrot set
programs, either—check out the <a href="https://github.com/axch/dysvunctional-language/blob/master/dvl/examples/celestial/">celestial
mechanics</a>
example.</p>
<p>Now, DVL is not Sufficiently Clever for general-purpose programming,
because it doesn’t know what not to specialize. That’s an
<a href="https://scholar.google.com/scholar?q=supercompilation">open</a>
<a href="https://scholar.google.com/scholar?q=partial+evaluation">problem</a>, but
it feels to me like the right place to dig.</p>
<h2 id="notes">Notes</h2>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      C: "{\\mathbb{C}}"
    },
    equationNumbers: { autoNumber: "AMS" },
    noErrors: { disabled: true },
  }
});
</script>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Here is how I count floating-point operations.
The Mandelbrot set, by definition, is those complex numbers <span class="math inline">\(c\)</span> for which
iterating
<span class="math display">\[z_{n+1} = z_n^2 + c\]</span>
from <span class="math inline">\(z_0 = 0\)</span> does not escape to infinity. It’s not too hard to
check that, if <span class="math inline">\(|z_n|\)</span> ever exceeds 2, then that iteration will escape
to infinity, regardless of <span class="math inline">\(c\)</span>. This suggests an algorithm for
approximating membership in the Mandelbrot set: for any <span class="math inline">\(c\)</span> of interest,
run the iteration for a fixed number of steps. If the result’s magnitude
exceeds 2, <span class="math inline">\(c\)</span> is not in the Mandelbrot set; if it does not, we can
approximately say that <span class="math inline">\(c\)</span> is in the set.</p>
<p>The program in the demo follows this algorithm. Every iteration
takes 10 arithmetic operations (six for the complex multiply, two for
the complex add, and two for bookkeeping). I chose to go for 400
iterations per point. Every pixel tests 4 points, for the
anti-aliasing. That makes
<span class="math display">\[ 4 \frac{\textrm{points}}{\textrm{pixel}}
   \cdot 400 \frac{\textrm{iters}}{\textrm{point}}
   \cdot 10 \frac{\textrm{flops}}{\textrm{iter}}
   = 16,\!000 \frac{\textrm{flops}}{\textrm{pixel}}, \]</span>
which is where the counts in the table come from.</p>
<p>Aside: It is surely computationally more efficient to check
intermediate steps in the iteration for escaping rather than always
going a fixed number of steps, but that makes counting floating point
operations harder, because you do not know how many get skipped.
That’s why I chose not to do that for this exercise.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>As of this writing, the best program
for computing the Mandelbrot set on the <a href="https://benchmarksgame.alioth.debian.org/u32/performance.php?test=mandelbrot">Computer Language Benchmarks
Game</a>
(which happened to be written in Fortran 90 and compiled with the Intel Fortran compiler)
was only six times faster than the demo you see here. I do not know
how much of that speedup was due to doing less actual
computation—unlike this demo, that benchmark does truncate its
iterations if they escape early, but the exact savings from this are
hard to measure.<!--
I didn't want to make a footnote to a footnote, so I include my
calculation of that 6 times number in a comment:
- The graphic in the benchmark is 16,000 x 16,000 pixels at 1 point
  per pixel, whereas in the demo it's 500 x 500 pixels at 4 points per
  pixel (for the antialiasing).
- The benchmark iterates the Mandelbrot recursion 50 times, whereas
  the demo does 400.
- The program's best time is 14 seconds, whereas the demo gets 4.
- The benchmark's test processor is 2.4 GHz, whereas mine is 1.6 GHz.
- But, the benchmark permits stopping early is an intermediate point
  is known to escape, and the best program also uses a trick to try
  and save time on doing this test.
If you multiply all that out, you get an advantage of 6x, which mixes in
superiority of compilation (which is what we care about) with algorithmic
improvements and differences in the suitability of the hardware for this
task (e.g., I do not know how many floating point multipliers there are
either in my chip or the processor the benchmark was run on).
--><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>One quip among compiler writers that expresses this
sentiment is “inlining is the mother of all optimizations”. Inlining
is the act of replacing a reference to some variable with that
variable’s definition; putting the definition “in line”, as it were.
This is particularly helpful if the variable in question refers to a
function.</p>
<p>Inlining can cause acceleration in its own right (for example, by
eliminating the runtime cost of referring to the variable that was
inlined), but the reason it is considered so important is because
inlining a variable that is used in more than one place accomplishes
specialization. Inlining a definition also constructs a proof that
the specialized definition is used in exactly the one place where it
is used, but bringing the definition to the use and deleting the
reference is not the only way to do that. The idea of specialization
is also applicable more widely than inlining, because it is possible
(and desirable) to be able to specialize to clusters of similar
contexts, without further specializing to each of these contexts
individually.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Thu, 22 Aug 2013 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2013/cleverness-of-compilers/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title><em>Introduction to</em> Automatic Differentiation</title>
    <link>https://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/index.html</link>
    <description><![CDATA[<p>Automatic differentiation may be one of the best scientific computing
techniques you’ve never heard of. If you work with computers and real
numbers at the same time, I think you stand to benefit from at least a
basic understanding of AD, which I hope this article will provide; and
even if you are a veteran automatic differentiator, perhaps you might
enjoy my take on it.</p>
<h2 id="what-it-is">What it is</h2>
<p><a href="https://en.wikipedia.org/wiki/Automatic_differentiation">Wikipedia</a> said it very well:</p>
<blockquote>
<p>Automatic differentiation (AD), also called algorithmic
differentiation or computational differentiation, is a set of
techniques to numerically evaluate the derivative of a function
specified by a computer program … derivatives of arbitrary order
can be computed automatically, accurately to working precision, and
using at most a small constant factor more arithmetic operations
than the original program.</p>
</blockquote>
<p>This bears repeating: <strong>any derivative or gradient</strong>, of any function
you can program, or <strong>of any program</strong> that computes a function, <a href="#approximate-the-derivative-dont-differentiate-the-approximation">[*]</a> with <strong>machine
accuracy</strong> and <strong>ideal asymptotic efficiency</strong>. This is good for</p>
<ul>
<li>real-parameter optimization (many good methods are gradient-based)</li>
<li>sensitivity analysis (local sensitivity = <span class="math inline">\(\partial\)</span>(result)/<span class="math inline">\(\partial\)</span>(input))</li>
<li>physical modeling (forces are derivatives of potentials; equations
of motion are derivatives of Lagrangians and Hamiltonians; etc)</li>
<li>probabilistic inference (e.g., <a href="https://arxiv.org/pdf/1206.1901">Hamiltonian Monte Carlo</a>)</li>
<li><a href="https://justindomke.wordpress.com/2009/02/17/automatic-differentiation-the-most-criminally-underused-tool-in-the-potential-machine-learning-toolbox/">machine learning</a></li>
<li>and who knows how many other scientific computing applications.</li>
</ul>
<p>My goal for this article is to provide a clear, concise introduction
to the technology of AD. In particular, I look to dispel some of the
<a href="#what-it-isnt-symbolic-or-numerical-differentiation">misconceptions</a> that seem to impede the adoption of AD, but
to also warn of the <a href="#subtleties">traps</a> that do await the unwary.</p>
<h2 id="contents">Contents</h2>
<ul>
<li><a href="#what-it-isnt-symbolic-or-numerical-differentiation">What automatic differentiation isn’t</a>
<ul>
<li><a href="#ad-is-not-numerical-differentiation">AD is not numerical differentiation</a></li>
<li><a href="#ad-is-not-symbolic-differentiation">AD is not symbolic differentiation</a></li>
</ul></li>
<li><a href="#forward-mode-computes-directional-derivatives">Forward mode computes directional derivatives</a></li>
<li><a href="#reverse-mode-computes-directional-gradients">Reverse mode computes directional gradients</a></li>
<li><a href="#subtleties">There are subtleties</a>:
<ul>
<li><a href="#overhead-of-nonstandard-interpretation">Overhead of nonstandard interpretation</a></li>
<li><a href="#perturbation-confusion">Perturbation confusion</a></li>
<li><a href="#derivatives-of-higher-order-functions">Derivatives of higher-order functions</a></li>
<li><a href="#approximate-the-derivative-dont-differentiate-the-approximation">Approximate the derivative, don’t differentiate the approximation</a></li>
<li><a href="#leaky-abstractions">Leaky abstractions</a></li>
</ul></li>
<li><a href="#ad-tool-space">There are AD tools out there</a></li>
<li><a href="#further-reading">You can learn more about it</a></li>
<li><a href="#notes">Notes</a></li>
</ul>
<h2 id="what-it-isnt-symbolic-or-numerical-differentiation">What it isn’t<em>: Symbolic or Numerical Differentiation</em></h2>
<p>Everyone who hears the name for the first time always thinks that
automatic differentiation is either symbolic differentiation or
numerical differentiation, but it’s much better.</p>
<h3 id="ad-is-not-numerical-differentiation">AD <em>is not</em> Numerical Differentiation</h3>
<p>Numerical differentiation is the technique one would obviously think
of from the standard definition of a derivative. Since
<span class="math display">\[\frac{df(x)}{dx} = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h},\]</span> you
can clearly approximate the left hand side by evaluating the right
hand side at a small but nonzero <span class="math inline">\(h\)</span>. This has the advantage of being
blindingly easy to code, but the disadvantages of costing <span class="math inline">\(O(n)\)</span>
evaluations of <span class="math inline">\(f\)</span> for gradients in <span class="math inline">\(n\)</span> dimensions, and of catching
you between the rock of truncation error and the hard place
of roundoff error.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Techniques have of course been
developed to mitigate these problems,<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> but these techniques
increase rapidly in programming complexity.</p>
<h3 id="ad-is-not-symbolic-differentiation">AD <em>is not</em> Symbolic Differentiation</h3>
<p>Symbolic differentiation is the technique you were taught in Calc 101:
<span class="math display">\[\begin{eqnarray*}
\frac{d(u(x) + v(x))}{dx} &amp; = &amp; \frac{du(x)}{dx} + \frac{dv(x)}{dx}, \\
\frac{d(u(x) v(x))}{dx} &amp; = &amp; u(x) \frac{dv(x)}{dx} + \frac{du(x)}{dx} v(x), \\
&amp; \textrm{etc.} &amp;
\end{eqnarray*}\]</span>
This technique is perfectly mechanical, given access to the expression
that defines the function of interest, so there is no reason it could
not be carried out by computer (and, in fact, computer algebra systems
like Maxima and Mathematica do implement it).</p>
<p>The advantage of symbolic differentiation is that you already know it,
and, if the original <span class="math inline">\(f\)</span> is simple enough, it can produce a legible
symbolic description of the derivative, which can be very valuable for
further thinking. However, from the perspective of just computing
answers, symbolic differentiation suffers from two drawbacks. The
first is that your Calc 101 course didn’t teach you what to do with
<span class="math display">\[\frac{d(\textrm{make-point}(x,y))}{dx},\]</span>
or other constructs that show up in computer programs. What’s the
derivative of a data structure anyway (even one as simple as a 2D
point)? This question does have a good answer, but it nevertheless makes
symbolically differentiating computer programs considerably more
subtle than it might at first appear.</p>
<p>The second drawback of symbolic differentiation is evident in the rule
for multiplication:
<span class="math display">\[\frac{d(u(x) v(x))}{dx} = u(x) \frac{dv(x)}{dx} + \frac{du(x)}{dx} v(x).\]</span>
Notice that if <span class="math inline">\(f\)</span> is a product, then <span class="math inline">\(f\)</span> and <span class="math inline">\(df\)</span> have some computations in
common (namely, <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>). Notice also that, on the right hand side,
<span class="math inline">\(u(x)\)</span> and <span class="math inline">\(du(x)/dx\)</span> appear separately. If you just proceed to
symbolically differentiate <span class="math inline">\(u\)</span> and plug its derivative into the
appropriate place, you will have duplicated any computation that
appears in common between <span class="math inline">\(u\)</span> and <span class="math inline">\(du\)</span>; and deeply nested duplications are
bad. Careless symbolic differentiation can easily produce
exponentially large symbolic expressions which take exponentially long
to evaluate.</p>
<p>In principle, the exponentially large expression should have lots of
duplication, so it ought to be possible to simplify it to something
fast afterwards—if you can store it in memory long enough. So to
prevent intermediate expression bulge, you would want to interleave
the differentiating with the simplifying. You can think of (forward
mode) automatic differentiation as a way to organize symbolic
differentiation to retain the sharing of intermediate results between
the main computation and the derivative; which coincidentally also
extends nicely to non-numeric programming constructs.</p>
<h2 id="forward-mode-computes-directional-derivatives">Forward Mode <em>Computes</em> Directional Derivatives</h2>
<p>One of the wrinkles with using AD is that it comes in two basic
flavors (and therefore, advanced usage admits many combinations and
variations). The easier basic flavor to describe and understand (and
implement and use) is called <em>forward mode</em>, which computes
directional derivatives.</p>
<p>To do forward mode AD on a program, do a nonstandard interpretation of
the program, as follows. Define
“dual numbers” as formal truncated Taylor series of the form <span class="math inline">\(x + \eps x&#39;\)</span>.
Define arithmetic on dual numbers by <span class="math inline">\(\eps^2 = 0\)</span>, and by interpreting
any non-dual number <span class="math inline">\(y\)</span> as <span class="math inline">\(y+\eps0\)</span>. So:
<span class="math display">\[\begin{eqnarray*}
  (x+\eps x&#39;) + (y+\eps y&#39;) &amp; = &amp; (x+y)+\eps (x&#39;+y&#39;) \\
  (x+\eps x&#39;) (y+\eps y&#39;) &amp; = &amp; (xy)+\eps (xy&#39;+x&#39;y) \quad \textrm{No quadratic term}\\
  &amp; \textrm{etc.} &amp;
\end{eqnarray*}\]</span></p>
<p>The coefficients of <span class="math inline">\(\eps\)</span> should look familiar: they are just the
corresponding symbolic derivative rules. We are setting up a regime where
<span class="math display">\[\begin{eqnarray}
  f(x+\eps x&#39;) &amp; = &amp; f(x)+\eps f&#39;(x)x&#39;. \label{forw-invariant}
\end{eqnarray}\]</span>
In other words, these dual numbers are just data structures for carrying the
derivative around together with the undifferentiated answer (called
the <em>primal</em>). The chain rule works—two applications of <span class="math inline">\(\eqref{forw-invariant}\)</span> give
<span class="math display">\[\begin{eqnarray*}
 f(g(x+\eps x&#39;)) &amp; = &amp; f(g(x)+\eps g&#39;(x)x&#39;) \\
 &amp; = &amp; f(g(x))+\eps f&#39;(g(x))g&#39;(x)x&#39;,
\end{eqnarray*}\]</span>
where the coefficient of <span class="math inline">\(\eps\)</span> on the right hand side is exactly
the derivative of the composition of <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>. That means that since
we implement the primitive operations to respect the invariant <span class="math inline">\(\eqref{forw-invariant}\)</span>,
all compositions of them will too. This, in turn, means that we can
extract the derivative of a function of interest by evaluating it in
this nonstandard way on an initial input with a 1 coefficient for <span class="math inline">\(\eps\)</span>:
<span class="math display">\[ \left. \frac{df(x)}{dx}\right|_x = \textrm{epsilon-coefficient}(\textrm{dual-version}(f)(x+\eps 1)). \]</span>
This also generalizes naturally to taking directional derivatives of
<span class="math inline">\(f : \R^n \to \R^m\)</span>.</p>
<p>That’s all there is to forward mode, conceptually. There are of
course some <a href="#subtleties">subtleties</a>, but first let’s look at what we have: We have
a completely mechanical way to evaluate derivatives at a point.
This mechanism works for any
function that can be composed out of the primitive operations that we
have extended to dual numbers. It turns every arithmetic
operation into a fixed number of new arithmetic operations, so the
arithmetic cost goes up by a small constant factor. We also do not
introduce any gross numerical sins, so we can reasonably expect the
accuracy of the derivative to be no worse than that of the primal
computation.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>This technique also naturally extends to arbitrary program
constructs—dual numbers are just data, so things like data
structures can just contain them. As long as no arithmetic is done on
the dual number, it will just sit around and happily be a dual number;
and if it is ever taken out of the data structure and operated on
again, then the differentiation will continue. The semantic
difficulties around what the derivative of a data structure is only
arise at the boundary of AD, if the function of interest <span class="math inline">\(f\)</span> produces
a data structure rather than a number, but do not embarrass the
internal invariants. If you look at it from the point of view of the
dual number, AD amounts to partially evaluating <span class="math inline">\(f\)</span> with respect to
that input to derive a symbolic expression, symbolically
differentiating that expression, and collapsing the result (in a
particular way) to make sure it isn’t too big; all interleaved to
prevent intermediate expression bulges.</p>
<p>Another way to look at forward mode of a function <span class="math inline">\(f: \R^n \to \R^m\)</span> is
to consider the <a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian</a>
(matrix of partial derivatives) <span class="math inline">\(J_xf\)</span>
at some point <span class="math inline">\(x\)</span>. The function <span class="math inline">\(f\)</span> perforce consists of a sequence
of primitive operations applied to various data in its internal
storage. If we call this sequence <span class="math inline">\(f_k \circ \ldots \circ f_3 \circ f_2 \circ f_1\)</span> (for <span class="math inline">\(k\)</span>
primitive operations, with data flowing from right to left), then the
full Jacobian of <span class="math inline">\(f\)</span> decomposes as
<span class="math display">\[ Jf = Jf_k \cdot \ldots \cdot Jf_3 \cdot Jf_2 \cdot Jf_1, \]</span>
where the points at which each intermediate Jacobian is taken are
determined by the computation of <span class="math inline">\(f\)</span> thus far.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> Now, the nice thing about this
decomposition is that even though the full Jacobian <span class="math inline">\(Jf\)</span> may be a
horrible huge dense matrix, each of the pieces <span class="math inline">\(Jf_i\)</span> is necessarily
very sparse, because each primitive <span class="math inline">\(f_i\)</span> only operates on a tiny
part of the data (in fact, for <span class="math inline">\(f_i\)</span> unary or binary, <span class="math inline">\(Jf_i - I\)</span>
will have just one or two non-zero entries, respectively). So we can
look at the result of forward-mode automatic differentiation as an implicit
representation of the point-indexed family of matrices <span class="math inline">\(J_xf\)</span>,<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> in the
form of a program that computes matrix-vector products <span class="math inline">\(J_xf \cdot v\)</span>
given <span class="math inline">\(x\)</span> and <span class="math inline">\(v\)</span>.</p>
<h2 id="reverse-mode-computes-directional-gradients">Reverse Mode <em>Computes</em> Directional Gradients</h2>
<p>As mentioned in the previous section, forward mode AD applied to a
function <span class="math inline">\(f\)</span> amounts to an implicit representation of the Jacobian <span class="math inline">\(Jf\)</span> that takes
advantage of the factorization of <span class="math inline">\(Jf\)</span> into extremely sparse factors
<span class="math inline">\(Jf_i\)</span>,
<span class="math display">\[ Jf = Jf_k \cdot \ldots \cdot Jf_3 \cdot Jf_2 \cdot Jf_1. \]</span>
Forward mode uses this decomposition to compute directional derivatives, which are vector
products <span class="math inline">\(J_xf \cdot v\)</span>. In modern practice, however, <em>gradients</em> of
functions <span class="math inline">\(f : \R^n \to \R\)</span> are generally much more valuable: they enable
multidimensional optimization methods, multiparameter sensitivity analyses, simulations of systems
following potentials, etc. The concept for a gradient of <span class="math inline">\(f : \R^n \to \R\)</span>
generalizes trivially to directional gradients of <span class="math inline">\(g : \R^n \to \R^m\)</span>; for
<span class="math inline">\(x\)</span> in <span class="math inline">\(\R^n\)</span> and <span class="math inline">\(u\)</span> in <span class="math inline">\(\R^m\)</span>, the directional gradient of <span class="math inline">\(g\)</span> at <span class="math inline">\(x\)</span> with respect
to <span class="math inline">\(u\)</span> is just the ordinary gradient at <span class="math inline">\(x\)</span> of <span class="math inline">\(g(x) \cdot u\)</span>.</p>
<p>The directional gradient is a symmetric concept to directional
derivative: where the directional derivative in the direction <span class="math inline">\(v\)</span> is
<span class="math inline">\(J_xf \cdot v\)</span>, the directional gradient is <span class="math inline">\(u^T \cdot J_xf\)</span>. One might therefore
hope that the decomposition
<span class="math display">\[ Jf = Jf_k \cdot \ldots \cdot Jf_3 \cdot Jf_2 \cdot Jf_1 \]</span>
would make a good implicit representation of <span class="math inline">\(Jf\)</span> for directional
gradients as well. Indeed it does, but with a twist. The twist is
that for directional derivatives, the information from <span class="math inline">\(x\)</span> about the
point at which to take each Jacobian flows through this product in the
same direction as the information from <span class="math inline">\(v\)</span> about what to multiply by
(to wit, right to left).
For directional gradients, however, they flow in opposite directions,
necessitating computing and storing some explicit representation of
the decomposition, based on the point <span class="math inline">\(x\)</span>, first, before being able to
multiply through by <span class="math inline">\(u^T\)</span> to compute the desired gradient. This can be
done—the method is called <em>reverse mode</em>—but it introduces both
code complexity and runtime cost in the form of managing this storage
(traditionally called the “tape”). In particular, the space
requirements of raw reverse mode are proportional to the <em>runtime</em> of
<span class="math inline">\(f\)</span>. There are <a href="https://www.google.com/search?q=checkpoint+reverse+mode">various
techniques</a>
for mitigating this problem, but it remains without an ideal solution.</p>
<h2 id="subtleties">Subtleties</h2>
<h3 id="overhead-of-nonstandard-interpretation">Overhead <em>of Nonstandard</em> Interpretation</h3>
<p>You will note that I have been careful to describe the performance of
AD in terms of arithmetic operations rather than runtime. While the
technique ensures that the amount of arithmetic goes up by no more
than a small constant,<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> managing this arithmetic can introduce a
lot of overhead if done carelessly. For instance, I introduced
forward mode by defining arithmetic on dual numbers. If you implement
AD by actually allocating data structures for holding dual numbers,
you will incur a great deal of overhead indeed: every arithmetic
operation will now involve accessing and allocating memory, which on modern computers is
much more expensive than arithmetic. An implementation by operator
overloading may also introduce method dispatches with their attendant
costs. If this is compared to raw numerical computation of the
primal, the slowdown can easily be an order of magnitude or two. How
best to manage this problem is the subject of ongoing work, and shapes
the <a href="#ad-tool-space">design space</a> of available <a href="http://www.autodiff.org/?module=Tools">AD tools</a>.</p>
<h3 id="perturbation-confusion">Perturbation confusion</h3>
<p>The other major implementation issue for AD is the possibility of a
class of bugs that are collectively called perturbation confusion.
The essential problem is that if two ongoing differentiations affect
the same piece of code, the coefficients of the two formal epsilons
they introduce (called perturbations) need to be kept distinct.
However, it is very easy to make bugs in the AD implementation that
confuse them in various ways. This problem especially bedevils the
performance-minded implementations, because they try to avoid allocating
and checking runtime storage
for explicit objects to represent the different perturbations. Such
situations arise naturally when nesting AD, that is, taking derivatives
of functions that internally take derivatives (or use AD-generated
derivatives).</p>
<h3 id="derivatives-of-higher-order-functions">Derivatives <em>of</em> higher-order functions</h3>
<p>We have so far been discussing directional derivatives and directional
gradients of functions <span class="math inline">\(f : \R^n \to \R^m\)</span>. If <span class="math inline">\(f\)</span> should internally use
data structures or higher-order functions or other programming-language
machinery, the nonstandard interpretation view
of AD turns out to just scale naturally. In particular, forward mode
by operator overloading requires exactly zero additional work, because
the dual numbers get carried around in just the right way. Reverse
mode takes a little more care, but can also be done.</p>
<p>Given that one knows how to use AD to get derivatives of any
<span class="math display">\[f : \R^n \to \R^m\]</span>
it becomes tempting to generalize the interface by defining a vector
space structure on all types in one’s programming language, and
getting directional derivatives of
<span class="math display">\[f : \R^n \to \alpha\]</span>
and directional gradients of
<span class="math display">\[g : \alpha \to \R^m\]</span>
for all types <span class="math inline">\(\alpha\)</span>. This is especially tempting since the
internal workings of AD point the way to such a vector space structure
already. Doing this increases the flexibility of AD
appreciably, but turns out to entail more subtleties still; because
appropriate representations need to be defined for tangents (or cotangents for reverse mode)
that are separated from their primals, and because assumptions on
which one might have rested a strategy for avoiding perturbation
confusion may need to be revised or enforced (especially if <span class="math inline">\(\alpha\)</span> is a function type).</p>
<h3 id="approximate-the-derivative-dont-differentiate-the-approximation">Approximate <em>the</em> derivative<em>, don’t</em> differentiate <em>the</em> approximation</h3>
<p>AD as a concept has one serious weakness that I know of (as opposed to
subtle points that implementers need to get right but for which the
answers are in principle known), and that is that it does not commute
with approximation. That is, AD will give you derivatives of the
procedure you actually programmed, which may not be good
approximations of the derivatives of the ideal function that your
procedure was approximating.</p>
<p>An example will elucidate: let your mind wander for a moment, back to a time when the <span class="math inline">\(e^x\)</span>
function was computed by an explicit subroutine, say <span class="math inline">\(\exp\)</span>, instead of
being microcoded in the chip (or even implemented directly in
hardware). This subroutine can only compare, add, and multiply, so it
must, perforce, compute not the true <span class="math inline">\(e^x\)</span> function but some
piecewise-rational approximation to it. Now imagine automatically
differentiating some computation that calls this <span class="math inline">\(\exp\)</span> routine.
For lack of any annotation to the contrary, AD will descend into
the implementation of <span class="math inline">\(\exp\)</span>. What will happen to it? The
piecewise structure of <span class="math inline">\(\exp\)</span> will remain, but the rational function in each
piece will get differentiated, producing a different rational function (of
degree 1 less). This remains an approximation of the derivative of
<span class="math inline">\(e^x\)</span>, of course, but we have access to a much better approximation,
namely <span class="math inline">\(\exp\)</span> itself! So AD increased the approximation error in
our computation more than was strictly necessary.</p>
<p>Now, this particular example is a non-problem today, because <span class="math inline">\(\exp\)</span> is
always taken as a primitive and (an approximation to) its derivative
is specified as part of the input to the AD system. There are,
however, other such examples, where some mathematical function <span class="math inline">\(f\)</span> that
can only be computed approximately has a well-defined mathematical
derivative (often expressed in terms of <span class="math inline">\(f\)</span> itself), and where
differentiating an approximation to <span class="math inline">\(f\)</span> produces much worse answers than
explicitly approximating the (known) derivative of <span class="math inline">\(f\)</span>. For instance,
situations like this arise when <span class="math inline">\(f\)</span> is something like a numerical
integration routine being differentiated with respect to either the
bounds of integration (the right answer is obvious) or some parameter
of the integrand (less obvious). Unfortunately, I know of no AD
system that allows the user to add their own higher-order functions to the set of
primitives whose derivatives the AD system knows, so this subtlety
must stand as a caution to users of AD for now.</p>
<h3 id="leaky-abstractions">Leaky Abstractions</h3>
<p>Differentiation is, like I would expect any nonstandard interpretation
to be, a pretty invasive procedure to execute upon a function. As
such, care must be taken in the implementation of any AD system if its
use is not to leak across abstraction boundaries.</p>
<p>The most egregious form of such leakage is what one might call a <em>caller
derives</em> discipline: when a routine (e.g., a maximum-finder) that accepts a
function and wants to know its derivatives simply demands to be called
with an additional parameter which computes the requested derivative.
Such an interface has the advantage of being simple to code (for the
implementer of the maximum-finder) and of not pinning the user to any
particular methodology for producing the needed derivatives (in fact,
such derivative functions are often calculated and coded by hand).
The downsides are that routines like this become ever more difficult
to</p>
<ul>
<li>compose (what set of derivatives, of what orders and with respect to
what inputs, does a minimax routine need? How about a minimax that
uses second order derivatives for quadratic methods for the min and
the max?),</li>
<li>interchange (because the entire call stack from client
to method needs to be adjusted if a different set of derivative
information should be required),</li>
<li>and use (because every time you wish to apply it to a different
function, you have to supply a new set of derivatives)</li>
<li>correctly (the coordination of having to supply the proper
derivative of the proper function is an additional source of
programming error).</li>
</ul>
<p>In contrast, a completely <em>callee derives</em> discipline allows the
external interfaces of all methods for the same task (such as
maximum-finding) to be the same (at least on this point), and invokes
AD automatically whenever some method requires the derivative of an
input function. The advantages of callee derives are improved
software engineering: more complex methods are easier to define,
implement, and use; experimentation with different approaches to a
problem becomes easier; methods that adaptively choose whether to use
derivatives or not become possible. The downside is that some AD
system is required, and mechanisms for the user to supply their own
derivative computations (should they be better than the automatic
ones) become more complex (when they are available at all).</p>
<p>It also appears that some AD systems operating on statically typed
base languages occupy an intermediate ground: derivatives of functions of
interest can be computed automatically by the callee, but whether (and
how much) it does this is apparent from said callee’s type signature.
The pros and cons of this situation are, in my view, a mix of those of
the two extremes between which it lies.</p>
<h2 id="ad-tool-space">AD Tool Space</h2>
<p>Now that you know what automatic differentiation is all about, how can
you go about using it? There are a fair number of AD tools out there.
Evaluating <a href="https://en.wikipedia.org/wiki/Automatic_differentiation#Software">all of them</a>
is beyond the scope of this article, but I want
to paint a general picture in broad strokes, and arm you with the
considerations wherewith to evaluate available tools on your own.</p>
<p>What does one want of an AD library or tool? There are of course the
things one wants of any library or tool—maturity, Freedom,
applicability to one’s programming language and ecosystem, etc. But
in the case of AD, the subtleties discussed in the previous section
translate into particular desiderata. You want your tool to get
all the subtle points right; stated positively, these desiderata are:</p>
<ul>
<li>Availability of <a href="#reverse-mode-computes-directional-gradients">reverse mode</a>, as this is much harder to implement
(correctly) than forward mode.</li>
<li><a href="#overhead-of-nonstandard-interpretation">Good performance</a> of the generated derivatives.</li>
<li><a href="#perturbation-confusion">Correct nesting</a>. Empirically, reverse over reverse is particularly
tough to implement (though there may be ways to avoid needing it).</li>
<li>Handling of <a href="#derivatives-of-higher-order-functions">language features</a> (especially higher-order functions),
at least when used internally by the program being differentiated.</li>
<li>Hooks for <a href="#approximate-the-derivative-dont-differentiate-the-approximation">custom derivatives</a> (for those cases when you know you can do
better than the AD system).</li>
<li>Enabling <a href="#leaky-abstractions">proper abstractions</a>.</li>
</ul>
<p>I know of three classes of mechanisms for implementing the nonstandard
interpretation necessary for AD; they vary in which of the above
desiderata they make easy or difficult.</p>
<h3 id="operator-overloading">Operator overloading</h3>
<p>In this approach, you overload all the
arithmetic operations in your language of choice to react to dual
numbers appropriately, and you observe that now every function stands
ready to compute its values and/or its derivatives, depending on what
input you give it.</p>
<ul>
<li><p>Good performance is tough, because operator overloading puts a great
deal of pressure on the compiler of the base language to get rid of
all the extra dispatches, allocations, and memory references that it
introduces. Many of them are not up to the challenge.</p></li>
<li><p>Care must still be taken to avoid perturbation confusion in the
presence of nested AD. Explicitly carrying around tags for the
perturbations is simple, but adds yet more pressure on the
underlying compiler.</p></li>
<li><p>Language features like data structures and higher-order functions
are typically handled for free, at least if they only appear
internally to the function being differentiated (rather than across
its interface). On the other hand, the language has to support
operator overloading for this to work in the first place.</p></li>
<li><p>I have not seen systems like this that handle custom derivatives
gracefully, though it should be possible to expose the API of the
overloading data structures so that the user can teach their own
functions to dispatch on them.</p></li>
<li><p>Callee derives is essentially the default with this method; though the
underlying language’s type system may need to be appeased. On the
other hand, said type system could also be good for reducing
perturbation confusion and for generating good code (by enabling the
underlying compiler to do type-driven optimizations).</p></li>
</ul>
<p>Such systems are numerous and include, among others</p>
<ul>
<li><a href="https://github.com/qobi/R6RS-AD">r6rs-ad</a> for R6RS Scheme,</li>
<li>the AD inside <a href="https://groups.csail.mit.edu/mac/users/gjs/6946/refman.txt">Scmutils</a>,</li>
<li>the <a href="https://github.com/ekmett/ad">ad/fad/rad</a> series of Haskell libraries, and</li>
<li><a href="http://www.fadbad.com/fadbad.html">FADBAD++</a> for C++ with templates.</li>
</ul>
<h3 id="extralinguistic-source-to-source-transform">Extralinguistic <em>Source-to-Source</em> transform</h3>
<p>This approach is
exemplified by <a href="https://www-sop.inria.fr/tropics/tapenade.html">Tapenade</a>,
<a href="https://www.mcs.anl.gov/research/projects/adifor/">ADIFOR</a>,
and <a href="https://www.mcs.anl.gov/research/projects/adic/">ADIC</a>.
What you do is you write a
program that reads source code files of the language you are defining
AD for, and some meta-information about what function(s) to
differentiate, carries out some appropriate analyses and
transformations, and emits source files in that same language that
compute the desired derivatives.</p>
<ul>
<li><p>Good performance is arguably more natural with this approach than
with operator overloading, because you can, in principle, emit
arbitrarily efficient code by optimizing the representations and
operations used in your output program. Of course, this means you
are essentially writing your own compiler, albeit one specialized to
AD.</p></li>
<li><p>Perturbation confusion tends, empirically, to be a big problem for
these kinds of systems, presumably because they are aggressive about
getting rid of the meta-information that would help to disambiguate
perturbations.</p></li>
<li><p>Supporting language features like data structures and higher-order
functions becomes that much more work—not only must you
essentially reimplement them, you also have to figure out how they
must transform under AD. On the other hand, the source language
doesn’t need to support operator overloading because you aren’t
using it.</p></li>
<li><p>Custom derivatives are frequently supported by source-to-source
transformation tools. I suspect that this is because the tools
already follow the program’s existing modularity boundaries, so
swapping in a custom derivative for one the tool would otherwise
produce itself is not difficult. However, these tools do not
usually support higher-order functions, so supplying a custom
derivative for an integration routine (that may rely upon
derivatives of the integrand!) remains an open problem.</p></li>
<li><p>Source-to-source tools naturally lean towards caller-derives,
because callee-derives necessarily involves inspection of which
functions end up flowing into places that need their derivatives;
and modification of those control flows to include said derivative
functions.</p></li>
</ul>
<p>Source-to-source tools essentially amount to reimplementations of the
language on which they operate. If that language is in any way
complex or unclear, the tool’s semantics are likely to diverge from
those of the canonical implementation; and in any case this approach
also imposes a nontrivial toolchain cost on its users.</p>
<h3 id="new-programming-language-with-ad-primitives">New Programming Language <em>with</em> AD <em>Primitives</em></h3>
<p>This is the approach taken by
<a href="http://www.bcl.hamilton.ie/~qobi/stalingrad/">Stalingrad</a>, the
research system that got me interested in all this, and, in a sense,
my own <a href="https://github.com/axch/dysvunctional-language">DysVunctional Language</a>.</p>
<ul>
<li><p>Good performance requires little or no additional effort for AD,
above and beyond the work of making a performant programming
language in the first place.</p></li>
<li><p>Correct nesting still requires work. However, it may be possible to
make the underlying compiler aggressive enough to eliminate the
performance overhead of tagging the perturbations explicitly in the
AD implementation (DVL attempts this).</p></li>
<li><p>Supporting advanced language features again requires no additional
work for AD, beyond the work of implementing them in the first
place. And the AD implementation’s semantics for those features
will not diverge from the standard semantics, since they are
integrated from the start.</p></li>
<li><p>Custom derivatives remain a challenge; perhaps even a greater one
than with other approaches. The whole point of defining a new
language is that AD will operate smoothly on all of it; how then to
introduce artificial barriers which AD does not cross, but instead
uses user-supplied derivatives? Especially if the derivatives in
question are themselves to be written in the same language (and need to
be amenable to further differentiation, for nesting)?</p></li>
<li><p>The callee-derives style of interface, however, becomes very natural
indeed.</p></li>
</ul>
<p>The additional disadvantage, of course, is that this approach imposes
the technical cost of dealing with your new language’s foreign
interface, and the social cost of creating a new language that needs
to be learned. These costs can be mitigated to some extent by making
your new language similar to an existing language, but that pulls you
towards the reimplementation problems inherent in the source-to-source
transform approach.
<a href="http://www.bcl.hamilton.ie/~qobi/fortran/">Farfallen</a> is a research
system of this shape that I worked on, whose language is similar to
Fortran77.</p>
<h2 id="further-reading">Further Reading</h2>
<p><a href="http://autodiff.org">http://autodiff.org</a> is the automatic differentiation community portal.
Among other things, it lists several
<a href="http://www.autodiff.org/?module=Introduction&amp;submenu=Selected%20Books">textbooks</a>,
and has an enormous
database of related <a href="http://www.autodiff.org/?module=Publications">academic publications</a>.</p>
<p>The formative literature for my own understanding of automatic
differentiation was the output of the collaboration between Professors
<a href="https://engineering.purdue.edu/~qobi/">Siskind</a> and
<a href="http://www-bcl.cs.may.ie/~barak/">Pearlmutter</a> and their research
groups; to which body of work I have even had the honor of contributing. For the
reader who may wish to repeat this educational path, an annotated
bibliography:</p>
<dl>
<dt>Jeffrey Mark Siskind and Barak A. Pearlmutter, <a href="https://docs.lib.purdue.edu/ecetr/368">Putting the Automatic Back into AD: Part I, What’s Wrong</a>, School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA, Jan 2008, TR-ECE-08-02.</dt>
<dd>
A screed on the state of high-performance automatic differentiation
tooling. By reversing the senses of all the assertions, this can
be read as a manifesto for what automatic differentiation tools
should offer.
</dd>
<dt>Barak A. Pearlmutter and Jeffrey Mark Siskind, <a href="https://docs.lib.purdue.edu/ecetr/369">Putting the Automatic Back into AD: Part II, Dynamic, Automatic, Nestable, and Fast</a>, School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA, Jan 2008, TR-ECE-08-03.</dt>
<dd>
<p>An argument, to the scientific computing community, that higher-order
functions are valuable in general, and that differentiation is a
higher-order function; describes AD in those terms. Argues that
therefore implementation technology developed for dealing with
higher-order functions would serve the community well. To that
point, presents some performance results for the Stalingrad
compiler.</p>
<p>Published, in somewhat condensed form, as Barak A. Pearlmutter and Jeffrey Mark Siskind. <a href="http://www-bcl.cs.may.ie/~barak/papers/sound-efficient-ad2008.pdf">Using programming
language theory to make AD sound and
efficient</a>. In
Proceedings of the 5th International Conference on Automatic
Differentiation, pages 79-90, Bonn, Germany, Aug 2008,
doi:10.1007/978-3-540-68942-3_8. Springer-Verlag.</p>
</dd>
<dt>Jeffrey Mark Siskind and Barak A. Pearlmutter. <a href="http://www-bcl.cs.may.ie/~barak/papers/ifl2005.pdf">Perturbation Confusion and Referential Transparency: Correct Functional Implementation of Forward-Mode AD</a>. Draft Proceedings of the 17th International Workshop on Implementation and Application of Functional Languages (IFL2005), Sep 19-21, 2005, Dublin Ireland. (Also <a href="http://www-bcl.cs.may.ie/~barak/papers/ifl2005.ps.gz">ps</a>, <a href="http://www-bcl.cs.may.ie/~barak/papers/ifl2005.djvu">djvu</a>)</dt>
<dd>
<p>A concise paper on the perturbation confusion problem, in the
context of forward mode AD embedded into a functional language.</p>
</dd>
<dt>Jeffrey Mark Siskind and Barak A. Pearlmutter. <a href="http://www-bcl.cs.may.ie/~barak/papers/HOSC-forward-nesting.pdf">Nesting forward-mode AD in a functional framework</a>. Higher Order and Symbolic Computation 21(4):361-76, 2008, doi:10.1007/s10990-008-9037-1. (Also <a href="http://www-bcl.cs.may.ie/~barak/papers/toplas-reverse.ps.gz">ps</a>)</dt>
<dd>
<p>A catalog of devices for avoiding perturbation confusion. The
devices are exhibited within complete implementations of a
first-class forward mode AD operator in a functional-programming
setting. I direct your attention to an argument, in footnote 6
near the end of the paper, to the effect that some kind of
referential non-transparency is required to implement a correctly
nestable derivative-taking operator. This is ironic, because the
resulting operator is itself referentially transparent.</p>
<p>A somewhat expanded version is also available as Siskind, J.M. and
Pearlmutter, B.A., <a href="https://docs.lib.purdue.edu/ecetr/377">Nesting Forward-Mode AD in a Functional
Framework</a>, Technical Report
TR-ECE-08-09, School of Electrical and Computer Engineering, Purdue
University, 2008.</p>
</dd>
<dt>Barak A. Pearlmutter and Jeffrey Mark Siskind. <a href="http://www-bcl.cs.may.ie/~barak/papers/toplas-reverse.pdf">Reverse-Mode AD in a functional framework: Lambda the ultimate backpropagator</a>. TOPLAS 30(2):1-36, Mar 2008, doi:10.1145/1330017.1330018. (Also <a href="http://www-bcl.cs.may.ie/~barak/papers/toplas-reverse.ps.gz">ps</a>)</dt>
<dd>
<p>The full story on how to implement a first-class reverse mode AD
operator in a higher-order language by (conceptually) reflective
runtime code transformation (including transforming code that
itself uses AD). This document is quite dense, but it does capture
both what reverse mode is and how it is actually implemented in
Stalingrad.</p>
</dd>
<dt>Barak A. Pearlmutter and Jeffrey Mark Siskind. <a href="http://www-bcl.cs.may.ie/~barak/papers/popl2007-multi-forward-AD.pdf">Lazy Multivariate Higher-Order Forward-Mode AD</a>. In Proceedings of the 2007 Symposium on Principles of Programming Languages (POPL 2007), Nice France, pages 155-60, doi:10.1145/1190215.1190242, Jan 2007. (Also <a href="http://www-bcl.cs.may.ie/~barak/papers/popl2007-multi-forward-AD.ps.gz">ps</a>, <a href="http://www-bcl.cs.may.ie/~barak/papers/popl2007-multi-forward-AD.djvu">djvu</a>)</dt>
<dd>
<p>The title basically says it all. This is a paper about how to do
the job at all, not how to do it super-efficiently.</p>
</dd>
<dt>Jeffrey Mark Siskind and Barak A. Pearlmutter, <a href="https://docs.lib.purdue.edu/ecetr/367">Using Polyvariant Union-Free Flow Analysis to Compile a Higher-Order Functional-Programming Language with a First-Class Derivative Operator to Efficient Fortran-like Code</a>, School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA, Jan 2008, TR-ECE-08-01.</dt>
<dd>
<p>An incredibly dense technical report on a compilation strategy
aggressive enough to make AD very efficient. This text was the
working blueprint
(in addition to inspiration from the Stalingrad source code) which
helped me get the implementation of DVL off the ground.
Specifically, my <a href="https://github.com/axch/dysvunctional-language/tree/master/vl">Vunctional
Language</a>
is a reasonably expository implementation of the flow analysis
contained in the report, and DVL builds on that to implement AD in
a different way.</p>
<p>While not for the faint of heart, this publication is an essential
piece of the reproducibility of Siskind and Pearlmutter’s other
results, especially as regards program performance.</p>
</dd>
<dt>Oleksandr Manzyuk, Barak A. Pearlmutter, Alexey Andreyevich Radul, David R. Rush, and Jeffrey Mark Siskind, <a href="https://arxiv.org/abs/1211.4892">Confusion of Tagged Perturbations in Forward Automatic Differentiation of Higher-Order Functions</a>, 2012, arxiv:1211.4892.</dt>
<dd>
<p>A spectacular bug that your implementation of AD may still have
even if you have zealously read everything else and followed all
the advice. Don’t read this unless you really want to implement
AD, or unless you need further convincing that the task is subtle.</p>
</dd>
<dt>Oleksandr Manzyuk, <a href="https://www.sciencedirect.com/science/article/pii/S1571066112000473">A Simply Typed λ-Calculus of Forward Automatic Differentiation</a>, Electronic Notes in Theoretical Computer Science, 2012, Elsevier.</dt>
<dd>
<p>Semantics for a first-class tangent bundle operator embedded in a
λ-calculus, paving the way to fully rigorous foundations for
AD.</p>
</dd>
<dt>Oleksandr Manzyuk, <a href="https://arxiv.org/abs/1202.0411">Tangent bundles in differential λ-categories</a>, 2012, arXiv:1202.0411.</dt>
<dd>
<p>Very heavy category-theoretic computations providing a
philosophical justification for the definition of forward mode AD
on functions that return arbitrary program objects (notably
including functions) as opposed to values that are easily
interpretable as vectors in <span class="math inline">\(\R^n\)</span>.</p>
</dd>
</dl>
<h2 id="notes">Notes</h2>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      R: "{\\mathbb{R}}",
      eps: "\\varepsilon"
    },
    equationNumbers: { autoNumber: "AMS" },
    noErrors: { disabled: true },
  }
});
</script>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Truncation error is the inaccuracy you get from <span class="math inline">\(h\)</span>
not actually being zero. Roundoff error is the inaccuracy you get from
valuable low-order bits of the final answer having to compete for
machine-word space with high-order bits of <span class="math inline">\(f(x+h)\)</span> and <span class="math inline">\(f(x)\)</span>, which
the computer has to store just until they cancel in the subtraction at
the end.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Better numerical differentiation lesson one:
evaluate <span class="math inline">\((f(x+h) - f(x-h))/2h\)</span> instead of <span class="math inline">\((f(x+h) - f(x))/h\)</span>. Your
truncation error moves from first-order to second-order in <span class="math inline">\(h\)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Disclaimer: I have not actually personally seen any
literature doing numerical error analysis of programs produced by
automatic differentiation. The technique does avoid the gross problem
of catastrophic cancellation when subtracting <span class="math inline">\(f(x)\)</span> from <span class="math inline">\(f(x+dx)\)</span>, but
may do poorly in more subtle situations. Consider, for example,
summing a set of numbers of varying magnitudes. Generally, the answer
is most accurate if one adds the small numbers together first, before
adding the result to the big ones, rather than adding small numbers to
large haphazardly. But what if the sizes of the perturbations in
one’s computation did not correlate with the sizes of the primals?
Then AD of a program that does the right thing with the primals will
do the wrong thing with the perturbations.</p>
<p>It would be better, in this case, to define the summation function as
a primitive (for AD’s purposes). The derivative is also summation,
but with the freedom to add the perturbations to each other in a
different order. Doing this scalably, however, is <a href="#approximate-the-derivative-dont-differentiate-the-approximation">an outstanding
issue</a> with AD.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Factoring the Jacobian in full: for the
decomposition of a function <span class="math inline">\(f\)</span> into primitives
<span class="math display">\[f = f_k \circ \ldots \circ f_i \circ \ldots \circ f_3 \circ f_2 \circ f_1,\]</span>
define the partial-result function <span class="math inline">\(f^i\)</span> as
<span class="math display">\[f^i = f_i \circ \ldots \circ f_3 \circ f_2 \circ f_1.\]</span>
Then
<span class="math display">\[ J_xf = J_{f^{k-1}(x)}f_k \cdot \ldots \cdot J_{f^{i-1}(x)}f_i \cdot \ldots \cdot J_{f^2(x)}f_3 \cdot J_{f^1(x)}f_2 \cdot J_xf_1.\]</span><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>For functions with data-dependent branches, the
decomposition of the Jacobian depends on the input <span class="math inline">\(x\)</span>, but that
doesn’t actually cause any trouble—all actual computations done by
AD are local at <span class="math inline">\(x\)</span>. If the input <span class="math inline">\(x\)</span> happens to be at a
boundary between regions of definition of <span class="math inline">\(f\)</span>, the answers may be
somewhat surprising, but the semantic difficulties of defining
derivatives of <span class="math inline">\(f\)</span> at such points are not unique to AD. Numerical
methods, in fact, start suffering from discontinuities even at some
distance, because of truncation error.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>In theory the arithmetic slowdown imposed by
automatic differentiation is no worse than 6x or 3x depending on whether
you take <span class="math inline">\(\tan(x)\)</span> as primitive or as the three primitives
<span class="math inline">\(\sin(x)/\cos(x)\)</span>. In practice it’s closer to +10% because the
expensive operations are exponentials, but they are their own
derivatives.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Thu, 15 Aug 2013 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>
<item>
    <title>Digital Foxes</title>
    <link>https://alexey.radul.name/ideas/2013/digital-foxes/index.html</link>
    <description><![CDATA[<p>The successful fox must know more than the sum of what the
hedgehogs know, for it must know the connections from one thing to
another. This fact is key to the design of computer systems for
solving certain kinds of problems.</p>
<p>I keep coming across domains with the following structure:</p>
<ol type="1">
<li><p>One wishes to solve a large, complex problem that fits into
some uniform formalism (for example, a detailed simulation of a car
as a system of 10,000 ODEs).</p></li>
<li><p>The formalism has a variety of methods developed for solving
problems stated in that formalism (e.g., various kinds of
integrators). These methods are hedgehogs in that they treat the
entire problem uniformly, possibly with state that’s global to the
whole solution (e.g., the step size).</p></li>
<li><p>The problem, being large, has many different parts, with different
solution characteristics (e.g., some variables change slowly and
smoothly, others oscillate rapidly).</p></li>
<li><p>The performance of each hedgehog method on the whole problem is
determined by its performance on the worst part of the problem
(e.g., the step size has to be small enough to trace the
highest-frequency oscillation in the model).</p></li>
<li><p>Different methods (and different settings of the global state) have
different weaknesses (e.g., high-order integrators are good for very
smooth systems, because they can achieve large step sizes).</p></li>
</ol>
<p>It is therefore desirable to solve such large problems like a fox: partition
the large problem into smaller subproblems, that are more uniform in
their solution characteristics, and solve each part with the most
appropriate method.</p>
<p>On the one hand, such foxy partitioning is eased because these
problems tend to be sparse, and have natural cliques, where variables
within a clique interact with each other much more strongly and
densely than with variables in a different clique.</p>
<p>On the other hand, partition is difficult, because the parts do
interact, so the solution methods need to communicate partial or
approximate solutions to each other during the solving process.
Cross-method communication is especially difficult because each method
is developed with its own invariants about what it communicates to
itself during its progress and how, and, being a hedgehog, does not
understand the communications of other hedgehog methods. Translation
is therefore necessary.</p>
<p>This pattern seems to appear quite often at the cutting edge of
computational science. Some formalisms where this phenomenon is
encountered, with example problems:</p>
<ul>
<li><p>Systems of ordinary differential equations or differential algebraic
equations (the car example, with parts of greatly differing
stiffness).</p></li>
<li><p>Coupled partial differential equations (e.g., detailed simulation of
water+temperature flow through a system of pipes, pools, and tanks:
those of the pipes that are narrow enough can be modeled with 1D
PDEs, those of the pools that are shallow enough can be modeled with
2D PDEs, and the tanks with 3D PDEs).</p></li>
<li><p>Model inference in machine learning (different model shapes, like
Gaussian mixtures and HMMs, can easily appear in the same compound
model).</p>
<ul>
<li>This is exacerbated by the rise of probabilistic programming
languages, which easily generate models with long chains of
deterministically-linked variables—the only place where
exhaustive search shines as an inference method.</li>
</ul></li>
<li><p>Constraint satisfaction</p></li>
</ul>
<p>One common theoretical thread is that all these problem domains are at
least NP-hard. This means that such problems are unsolvable in the
general case (unless P = NP), which is why every hedgehog has
weaknesses. Of course, this also means that any fox—any
combination of methods, or any system for selecting methods—must
also fail in some cases (unless P = NP). However, many particular
instances of interest are solvable; and the cutting edge occurs where
they are solvable only by compound methods, not by uniform application
of basic ones.</p>
<p>It is therefore desirable to develop foxy computer systems: ones that
allow easy (and, where possible, automatic) partitioning of a problem
into appropriate parts, selection of methods for solving the parts,
and composition thereof into a compound method for solving the whole
problem.</p>
<h2 id="how-to-breed-a-digital-fox">How <em>to</em> Breed <em>a</em> Digital Fox</h2>
<p>How might such a digital fox be developed? It seems to me that a
system like this must have the following pieces:</p>
<ul>
<li><p>Problem language: A language for specifying instances of the problem
class (e.g., the Modelica language for DAEs, various probabilistic
programming languages).</p>
<ul>
<li>This language must of necessity have means of abstraction, in
order for human authors of problem instances to be able to handle
the complexity of the problems they specify (e.g., one does not
write out 10,000 ODEs all in one big table). Are these
abstraction boundaries always going to be the same as the
desirable partitions of the problem into parts? If not, how should
the partitions be specified or hinted at?</li>
</ul></li>
<li><p>Hedgehogs: A library of primitive solution methods (e.g., 4th order Runge-Kutta, Gibbs sampling). I
do not know to what extent such libraries already exist as reusable
software in each domain. However, the definition of “a method” is
that practitioners, after suitable study, know how to implement an
instance of that method specialized to any particular problem they
wish to solve with it. The trick here is to extract that knowledge
into software definitions that specify enough of the method to
automatically construct efficient specializations, but not so much
as to over-restrict applicability.</p></li>
<li><p>Bridges: A library of bridges for communicating between solution methods. In
my mind, this is the key missing piece in all proposed foxes I have
seen so far—more <a href="#on-bridges">below</a>. In particular, I have seen no evidence
of the knowledge needed to make this library appearing in the
literature of any field, and I do not know for which domains this
knowledge exists even in the minds of practitioners.</p></li>
<li><p>Plan language: A language for specifying problem-specific compound methods. A
specification of a problem-specific compound method is a
decomposition of the problem into parts, a specification of what
methods are to be used to solve each part, and a specification of
the bridges for communicating between them. Compound methods can
have recursive structure in the sense that the method for solving a
part of the problem can itself be compound. This part is a little
tricky too, because one wants to be flexible about dynamic problem
structure (e.g., Chinese Restaurant Process introducing more
variables at solution runtime, or conditional triggers in an ODE
simulation changing the applicable equations).</p></li>
<li><p>Plan executor: A mechanism for efficiently executing problem-specific compound
solution methods. I presume this is going to be some combination of
query planner, compiler, and runtime system, possibly using
profiling information, etc. The better this mechanism is, the
easier becomes the task of writing the libraries of primitive
methods and bridges (the more the system can figure out itself, the
less needs to be specified), and the less cumbersome becomes the
language of compound methods.</p></li>
<li><p>Planner: A subsystem for evaluating, searching, suggesting, or
inferring compound methods automatically based on the specification
of the problem.</p></li>
</ul>
<p>While it is important that these six pieces are different, and
responsible for different jobs, I’m afraid that, with the notable
exception of the planner, they must all be invented essentially
together. What the hedgehogs are is presumably known at the start,
but how they should be specified or implemented depends critically on
the executor and the bridges. The bridges obviously depend critically
on the hedgehogs they are bridging. The design of the plan language
depends upon the composition constraints imposed by the hedgehogs and
the qualities of the bridges, as well as how much it needs to tell the
executor. The design of the plan language also back-influences the
hedgehogs and bridges. The problem language is intimately tied to the
plan language because they both talk about different perspectives on
the same thing. And conversely, the executor depends upon what it
will be executing. And all five of these essential pieces are
complicated, so should be designed iteratively. This is exactly the
kind of tight ball of design dependencies that should be neither
rushed nor partitioned across organizations, but should rest with a
close-knit team that has enough time to iterate on all the pieces
together until a good solution emerges.</p>
<p>You will note that I omitted the planner from the previous admonition.
I view the planner as optional because a digital fox without a planner would
already be very useful: rather than programming solutions to their
problems in C++, practitioners could program them in the plan
language; even without any further assistance, I expect the gains in
productivity to be tremendous.</p>
<p>In fact, I think the automatic method invention problem is something
of a red herring. It is attractive to people who propose to build
foxes because it looks like the kind of problem the fox is meant to
solve, so they are tempted to apply their favorite methods from the
discipline to try solving it. However, the method invention problem
is guaranteed to be intractable in general (unless P = NP) because the
underlying problems are intractable in general. What’s worse, the
hope of automating this step can lead to an obscure and hard-to-use
design for the plan language; which will turn the overall system into
a usability nightmare in the cases where the planner invents a bad
plan. I am therefore of the opinion that effort is better directed
at getting the other parts right; and a clear design for the rest
would enable a planner to be retrofit over the working fox later,
after the nature of the method invention problem in this domain is
better understood. Yes, doing so will probably require modifying the
executor to instrument execution and give feedback about the quality
of the plan, but that does not seem too steep a price to pay for not
having to think too hard about the planner when working out the other
five parts.</p>
<p>By the way, I see making a digital fox as a form of defeat. In some sense,
all the understanding of the domain is contained in the available
hedgehogs; and combining them into a fox introduces enormous
additional complexity for no “fundamental” gain in coverage.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> This
is not only complexity of the implementation, which is bad enough, but
also complexity of the interface: a fox can fail in any of the ways
its component methods can fail, and in additional ways introduced by
method selection and method combination. So it’s much better to solve
your problem with a better hedgehog if you can—making a fox is a
last resort. But new hedgehogs are hard to invent, and the time comes
when a field gives up and starts thinking about foxes. On the bright
side, a high-quality fox should be able to capture new hedgehogs as
they emerge from their dens.</p>
<h2 id="on-bridges"><em>On</em> Bridges</h2>
<p>The thing that makes some procedure a (hedgehog) solution method is
that it sets up and uses some invariants for representing partial or
approximate solutions to the problem, and iteratively expands or
improves them. In general, for any two different hedgehogs, the
invariants, and even the information communicated by them, will be
different. In order to stitch more than one method together, then, it
will be necessary to adjust the information one method generates for
use by the other; possibly both ways if the flow of information in the
solution is bidirectional.</p>
<p>This problem has an easy version and a hard version. The easy version
occurs when two methods really need the same information to flow
across the link, and that information just happens to need some kind
of format conversion because of accidents of internal representations
in the software implementing the methods. This version of the problem
can probably be eliminated by implementing all the methods on a common
substrate with common choices for representation of the same kinds of
information. The hard version is when the methods actually need
different information: for instance, an inference method that deals in
marginal distributions uses different information from one that deals
in samples. However, since both are ultimately about probability,
there is hope that it may be possible to make an explicit bridge to
translate, perhaps approximately, the information produced by one
method into the information consumed by the other.</p>
<p>I can envision two kinds of bridges: ones that operate across a link
in the structure of information flow, communicating across the link to
different methods; and ones that operate at a node, reconciling
overlapping information of different kinds. In the ODE domain, a
link-type bridge would be a way to use an equation some of whose
variables were being integrated exclusively by one integrator and some
by another to exchange appropriate information between them. A
node-type bridge, in contrast, would occur if we allowed the sets of
variables integrated by different methods to overlap, and would be a
way to reconcile the different values produced by the different
methods for the same variable, and communicate information between the
integrators through the results of reconciliation. I suppose hybrid
bridges that incorporated aspects of the link-type and the node-type
might also be possible.</p>
<p>My understanding is that bridge-building is not currently well
understood. I hear rumors that people who carry out large simulations
(ODEs, PDEs, etc) tend to implement such bridges, apparently in an
ad-hoc way for each problem. But at least the knowledge is there of
how to do it in principle. I don’t know whether such knowledge even
exists for machine learning problems—has anyone successfully
attacked a compound ML model using, say, MCMC to infer one part and a
variational method to infer another?</p>
<h2 id="extant-foxes"><em>Extant</em> Foxes</h2>
<p>When I started writing this essay, I didn’t think I knew of any
digital foxes, but in retrospect the good old relational database
management system fits the bill: SQL, together with the data schema,
indexes, and table statistics, form the problem language; there are
many different kinds of table scans, index searches, and result data
structures available for performing parts of a query; the query plan
language (which you can read with the SQL EXPLAIN statement) specifies
problem-specific compound methods; and the query planner is the
optional part that automatically searches plan space for good
candidates.</p>
<p>The operating system kernel seems to stretch the analogy a little, but
it does have the essential feature of managing multiple different
hedgehogs (processes), intermediating communication between them
(signals, pipes, sockets, files), and adjudicating use of resources
(CPU, screen, keyboard, network card). There is less of a definite
problem to solve, though, and method selection is pretty manual, so I
don’t know whether Linux is a fox in this sense.</p>
<p>For the mathematically inclined, a
<a href="https://en.wikipedia.org/wiki/Sheaf_%28mathematics%29">sheaf</a> is the
only example I know about of a mathematical structure which is a fox.
The gluing axiom is analogous to the all-important bridges.</p>
<h2 id="notes">Notes</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Actually, my main point, to the extent I have one, is that this is
not so. The fox must know how to connect the hedgehogs together and
make them cooperate; which perhaps necessitates a still-deeper
understanding of the domain than each hedgehog embodies. And, of
course, the whole point of a fox is that it does lead to better
coverage, by covering problems that have parts that stymie each
hedgehog.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></description>
    <pubDate>Mon, 05 Aug 2013 00:00:00 UT</pubDate>
    <guid>https://alexey.radul.name/ideas/2013/digital-foxes/index.html</guid>
    <dc:creator>Alexey Radul</dc:creator>
</item>

    </channel>
</rss>
